1	-1	0	0	5	b'What does "backprop" mean? I\'ve Googled it, but it\'s showing backpropagation.Is the "backprop" term basically the same as "backpropagation" or does it have a different meaning?'	25	0	0	0
2	-1	0	0	7	b'Does increasing the noise in data help to improve the learning ability of a network? Does it make any difference or does it depend on the problem being solved? How is it affect the generalization process overall?'	36	0	0	0
3	1	1	69	10	b'"Backprop" is the same as "backpropagation": it\'s just a shorter way to say it. It is sometimes abbreviated as "BP".'	19	0	0	0
4	-1	0	0	16	b"When you're writing your algorithm, how do you know how many neurons you need per single layer? Are there any methods for finding the optimal number of them, or is it a rule of thumb?"	34	0	0	0
5	-1	0	0	0	b"I have a LEGO Mindstorms EV3 and I'm wondering if there's any way I could start coding the bot in Python rather than the default drag-and-drop system. Is a Mindstorm considered AI?Is this possible?My goal is to write a basic walking program in Python. The bot is the EV3RSTORM. I searched and found this, but don't understand it. "	58	0	1	0
6	-1	0	0	3	b'The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge. (paraphrased from Wikipedia)Does this mean that humans are not intelligent? I think we all make mistakes that imply that we are not maximizing the expected value of a performance measure.'	60	0	1	0
7	-1	0	0	7	b'This quote by Stephen Hawking has been in headlines for quite some time: Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.Why does he say this? To put it simply in layman terms: what are the possible threats from AI? If we know that AI is so dangerous why are we still promoting it? Why is it not banned?What are the adverse consequences of the so called Technological Singularity? '	77	0	0	0
8	5	0	220	2	b'You can use python-ev3 which can be used to program Lego Mindstorms EV3 using Python on ev3dev.See: Setting Up a Python Development Environment with PyCharm'	24	0	1	0
9	2	1	402	6	b'Noise in the data, to a reasonable amount, may help the network to generalize better. Sometime, it has the opposite effect. It partly depends on the kind of noise ("true" vs. artificial).The AI FAQ on ANN gives a good overview. Except: Noise in the actual data is never a good thing, since it limits the accuracy of generalization that can be achieved no matter how extensive the training set is. On the other hand, injecting artificial noise (jitter) into the inputs during training is one of several ways to improve generalization for smooth functions when you have a small training set.In some field, such as computer vision, it\'s common to increase the size of the training set by copying some samples and adding some noises or other transformation.'	127	0	0	0
10	-1	0	0	15	b"I'm new to A.I. and I'd like to know in simple words, what is the fuzzy logic concept? How does it help, and when is it used?"	26	0	0	0
11	2	0	516	6	b'We typically think of machine learning models as modeling two different parts of the training data--the underlying generalizable truth (the signal), and the randomness specific to that dataset (the noise).Fitting both of those parts increases training set accuracy, but fitting the signal also increases test set accuracy (and real-world performance) while fitting the noise decreases both. So we use things like regularization and dropout and similar techniques in order to make it harder to fit the noise, and so more likely to fit the signal.Just increasing the amount of noise in the training data is one such approach, but seems unlikely to be as useful. Compare random jitter to adversarial boosting, for example; the first will slowly and indirectly improve robustness whereas the latter will dramatically and directly improve it.'	129	0	0	0
12	4	1	545	9	b'There is no direct way to find the optimal number of them: people empirically try and see (e.g., using cross-validation). The most common search techniques are random, manual, and grid searches. There exist more advanced techniques such as Gaussian processes, e.g. Optimizing Neural Network Hyperparameters with Gaussian Processes for Dialog Act Classification, IEEE SLT 2016.'	54	0	1	0
13	-1	0	0	3	b'In particular, an embedded computer (limited resources) analyzes live video stream from a traffic camera, trying to pick good frames that contain license plate numbers of passing cars. Once a plate is located, the frame is handed over to an OCR library to extract the registration and use it further.In my country two types of license plates are in common use - rectangular (the typical) and square - actually, somewhat rectangular but "higher than wider", with the registration split over two rows.(there are some more types, but let us disregard them; they are a small percent and usually belong to vehicles that lie outside our interest.)Due to the limited resources and need for rapid, realtime processing, the maximum size of the network (number of cells and connections) the system can handle is fixed.Would it be better to split this into two smaller networks, each recognizing one type of registration plates, or will the larger single network handle the two types better?'	160	0	0	0
14	5	1	616	3	b' Is a Mindstorm considered AI?This depends on what type of software you write in it... The algorithms you write could be seen as AI. You can absolutely use Python to progam it (or java or other languages). Check this link for a tutorial. '	44	0	1	0
15	-1	0	0	18	b'The Turing Test was the first test of artificial intelligence and is now a bit outdated. The Total Turing Test aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an artificial general intelligence (strong AI)?'	52	0	0	0
16	-1	0	0	4	b'What is the "early stopping" and what are the advantages using this method? How does it help exactly.'	17	0	0	0
17	-1	0	0	13	b"I've heard the idea of the technological singularity, what is it and how does it relate to Artificial Intelligence? Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off? How would we know when we reach this point?"	59	0	0	0
18	7	0	557	2	b" To put it simply in layman terms, what are the possible threats from AI? Currently, there are no threat. The threat comes if humans create a so-called ultraintelligent machine, a machine that can surpass all intellectual activities by any human. This would be the last invention man would need to do, since this machine is better in inventing machines than humans are (since that is an intellectual activity). However, this could cause the machine to invent machines that can destruct humans, and we can't stop them because they are so much smarter than we are.This is all hypothetical, no one has even a clue of what an ultraintelligent machine looks like.  If we know that AI is so dangerous why are we still promoting it? Why is it not banned?As I said before, the existence of a ultraintelligent machine is hypothetical. Artificial Intelligence has lots of useful applications (more than this answer can contain), and if we develop it, we get even more useful applications. We just have to be careful that the machines won't overtake us. "	179	0	0	0
19	7	0	560	2	b'Because he did not yet know how far away current AI is... Working in an media AI lab, I get this question a lot. But really... we are still a long way from this. The robots still do everything we detailledly describe them to do. Instead of seeing the robot as intelligent, I would look to the human programmer for where the creativity really happens.'	64	0	0	0
20	6	1	669	2	b"It rather depends on how one defines several of the terms used. For example:Whether the term `expected' is interpreted in a formal (i.e.statistical) sense. Whether it's assumed that humans have any kind of utilitarian`performance measure'.The motivation for this description of `agent' arose from a desire to have a quantitative model - it's not clear that such a model is a good fit for human cognition.However, there are alternative definitions of agents, for example the BDI model which are rather more open-ended and hence more obviously applicable to humans."	87	0	0	0
21	-1	0	0	0	b"I'm worry that my network become too complex. I don't want to end up with half of the network does nothing, but just take space and resources.So, what are the techniques of detecting and preventing overfitting to avoid such problem?"	39	0	0	0
22	7	0	661	4	b'It\'s not just Hawking, you hear variations on this refrain from a lot of people. And given that they\'re mostly very smart, well educated, well informed people (Elon Musk is another, for example), it probably shouldn\'t be dismissed out of hand.Anyway, the basic idea seems to be this: If we create "real" artificial intelligence, at some point, it will be able to improve itself, which improves it\'s ability to improve itself, which means it can improve it\'s ability to improve itself even more, and so on... a runaway cascade leading to "superhuman intelligence". That is to say, leading to something that more intelligent than we area.So what happens if there is an entity on this planet which is literally more intelligent than us (humans)? Would it be a threat to us? Well, it certainly seems reasonable to speculate that it could be so. OTOH, we have no particular reason, right now, to think that it will be so. So it seems that Hawking, Musk, etc. are just coming down on the more cautious / fearful side of things. Since we don\'t know if a superhuman AI will be dangerous or not, and given that it could be unstoppable if it were to become malicious (remember, it\'s smarter than we are!), it\'s a reasonable thing to take under consideration.Eliezer Yudkowsky has also written quite a bit on this subject, including come up with the famous "AI Box" experiment. I think anybody interested in this topic should read some of his material.'	249	0	1	0
23	7	0	730	3	b'As Andrew Ng said, worrying about such threat from AI is like worrying about of overpopulation on Mars. It is science fiction. That being said, given the rise of (much weaker) robots and other (semi-)autonomous agents, the fields of the law and ethics are increasingly incorporating them, e.g. see Roboethics.'	49	0	1	0
24	7	0	759	2	b"He says this because it can happen. If something becomes smarter than us, why would it continue to serve us? The worst case scenario is that it takes over all manufacturing processes and consumes all matter to convert it into material capable of computation, extending outward infinitely until all matter is consumed.We know that AI is dangerous but it doesn't matter because most people don't believe in it. It goes against every comfort religion has to offer. Man is the end-all-be-all of the universe and if that fact is disputed, people will feel out of place and purposeless.The fact is most people just don't acknowledge it's possible, or that it will happen in our lifetimes, even though many reputable AI experts put the occurrence of the singularity within two decades. If people truly acknowledged that AI that was smarter than them was possible, wouldn't they be living differently? Wouldn't they be looking to do things that they enjoy, knowing that whatever it is they do that they dread will be automated? Wouldn't everyone be calling for a universal basic income?The other reason we don't ban it is because its promise is so great. One researcher could be augmented by 1,000 digital research assistants. All manual labor could be automated. For the first time, technology offers us real freedom to do whatever we please.But even in this best case scenario where it doesn't overtake us, humans still have to adapt and alter their economic system to one where labor isn't necessary. Otherwise, those who aren't technically-trained will starve and revolt."	258	0	0	0
25	7	0	784	3	b"There are a number of long resources to answer this sort of question: consider Stuart Armstrong's book Smarter Than Us, Nick Bostrom's book Superintelligence, which grew out of this edge.org answer, Tim Urban's explanation, or Michael Cohen's explanation.But here's my (somewhat shorter) answer: intelligence is all about decision-making, and we don't have any reason to believe that humans are anywhere near close to being the best possible at decision-making. Once we are able to build an AI AI researcher (that is, a computer that knows how to make computers better at thinking), the economic and military relevance of humans will rapidly disappear as any decision that could be made by a human could be made better by a computer. (Why have human generals instead of robot generals, human engineers instead of robot engineers, and so on.)This isn't necessarily a catastrophe. If the Vulcans showed up tomorrow and brought better decision-making to Earth, we could avoid a lot of misery. The hard part is making sure that what we get are Vulcans who want us around and happy, instead of something that doesn't share our values."	184	0	4	0
26	-1	0	0	13	b"I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically. What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers? Are there examples where this is already happening to a degree today? Wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer? Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence."	95	0	0	0
27	15	0	548	5	b'The problem of the Turing Test is that it tests the machines ability to resemble humans. Not necessarily every form of AI has to resemble humans. This makes the Turing Test less reliable. However, it is still useful since it is an actual test. It is also noteworthy that there is a prize for passing or coming closest to passing the Turing Test, the Loebner Prize.The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge. (paraphrased from Wikipedia). This definition is used more often and does not depend on the ability to resemble humans. However, it is harder to test this. '	122	0	1	0
28	-1	0	0	6	b'Since human intelligence presumably is a function of a natural genetic algorithm in nature, is using a genetic algorithm in a computer an example of artificial intelligence? If not, how do they differ? Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into?'	55	0	0	0
31	10	0	972	4	b"It's analogous to analogue versus digital, or the many shades of gray in between black and white: when evaluating the truthiness of a result, in binary boolean it's either true or false (0 or 1), but when utilizing fuzzy logic, it's an estimated probability between 0 and 1 (such as 0.75 being mostly probably true). It's useful for making calculated decisions when all information needed isn't necessarily available.Wikipedia has a fantastic page for this."	73	0	0	0
32	10	1	1003	19	b"As complexity rises, precise statements lose meaning and meaningful statements lose precision. (Albert Einstein).Fuzzy logic deals with reasoning that is approximate rather than fixed and exact. This may make the reasoning more meaningful for a human:Fuzzy logic is an extension of Boolean logic by Lotfi Zadeh in 1965 based on themathematical theory of fuzzy sets, which is a generalization of the classical set theory.By introducing the notion of degree in the verification of a condition, thus enabling acondition to be in a state other than true or false, fuzzy logic provides a very valuableflexibility for reasoning, which makes it possible to take into account inaccuracies anduncertainties.One advantage of fuzzy logic in order to formalize human reasoning is that the rulesare set in natural language. For example, here are some rules of conduct that a driverfollows, assuming that he does not want to lose his driver\xe2\x80\x99s licence:Intuitively, it thus seems that the input variables like in this example are approximatelyappreciated by the brain, such as the degree of verification of a condition in fuzzylogic.I've written a short introduction to fuzzy logic that goes into a bit more details but should be very accessible."	191	0	0	0
33	17	0	679	2	b'The concept of "the singularity" is when machines outsmart the humans. Although Stephen Hawking opinion is that this situation is inevitable, but I think it\'ll be very difficult to reach that point, because every A.I. algorithm needs to be programmed by humans, therefore it would be always more limited than its creator.We would probably know that point, when humanity will lost control over Artificial Intelligence where super-smart AI would be in competition with humans and maybe creating more sophisticated intelligent beings, but currently it\'s more like science fiction (aka Terminator\'s Skynet).The risk could involve killing people (like self-flying war drones making their own decision), destroying countries or even the whole planet (like A.I. connected to the nuclear weapons (aka WarGames movie), but it doesn\'t prove the point that the machines would be smarter than humans.'	134	0	0	0
35	-1	0	0	23	b'These two terms seem to be related, especially in their application in computer science and software engineering. Is one a subset of another? Is one a tool used to build a system for the other? What are their differences and why are they significant?'	43	0	0	0
36	-1	0	0	16	b'What aspects of quantum computers, if any, can help to further develop Artificial Intelligence?'	13	0	0	0
37	-1	0	0	4	b'I believe a Markov chain is a sequence of events where each subsequent event depends probabilistically on the current event. What are examples of the application of a Markov chain and can it be used to create artificial intelligence? Would a genetic algorithm be an example of a Markov chain since each generation depends upon the state of the prior generation?'	60	0	0	0
38	28	0	322	2	b'This is kind of an opinion question, and it\'s probably more a question of philosophy than anything. But in terms of how things are commonly defined, I\'ll say "yes, genetic algorithms are part of AI". That is, if you pick up a comprehensive book on artificial intelligence, there will probably be a chapter on genetic algorithms" (or more broadly, "evolutionary algorithms"). One area that has been extensively studied in the past, is the idea of using genetic algorithms to train neural networks. I don\'t know if people are still actively researching this topic or not, but it at least illustrates that GA\'s are part of the overall rubric of AI in one regard.'	112	0	0	0
39	15	0	918	7	b"The rhetorical point of the Turing Test is that it places the 'test' for 'humanity' in observable outcomes, instead of in internal components. If you would behave the same in interacting with an AI as you would with a person, how could you know the difference between them?But that doesn't mean it's reliable, because intelligence has many different components and there are many sorts of intellectual tasks. The Turing Test, in some respects, is about the reaction of people to behavior, which is not at all reliable--remember that many people thought ELIZA, a very simple chatbot, was an excellent listener and got deeply emotionally involved very quickly. It calls to mind the Ikea commercial about throwing out a lamp, where the emotional attachment comes from the human viewer (and the music), rather than from the lamp.Turing tests for specific economic activities are much more practically interesting--if one can write an AI that replaces an Uber driver, for example, what that will imply is much clearer than if someone can create a conversational chatbot."	172	0	0	0
40	-1	0	0	5	b'What purpose does the "dropout" method serve and how does it improve the overall performance of the neural network?'	18	0	0	0
41	-1	0	0	4	b'Can an AI program have an IQ?In other words, can the IQ of an AI program be measured?Like how humans can do an IQ test.'	24	0	0	0
42	-1	0	0	4	b'Why anybody would want to use the "hidden layers"? How they enhance the learning ability of the network in comparison to the network which doesn\'t have them (linear models)?'	28	0	0	0
43	10	0	1325	10	b'Fuzzy logic is based on regular boolean logic. Boolean logic means you are working with truth values of either true or false (or 1 or 0 if you prefer). Fuzzy logic is the same apart from you can have truth values which are in-between true and false, that is to say you are working with any number between 0 (inclusive) and 1 (inclusive). The fact that you can have a \'partially true and partially false\' truth value is where the word "fuzzy" comes from. Natural languages often use fuzzy logic like "that balloon is red" meaning that balloon could be any colour which is similar enough to red, or "the shower is warm". Here is a rough diagram for how "the temperature of the shower is warm" could be represented in terms of fuzzy logic (the y axis being the truth value and the x axis being the temperature):Fuzzy logic can be applied to boolean operations such as and, or, and not. These would work like:You can then use the three "basic fuzzy logic operations" to build all other "fuzzy logic operations", just like you can use the three "basic boolean operations" to build all other "boolean logic operations".Sources:Fuzzy logic wikipedia, Boolean algebra wikipedia,Explanation of fuzzy logic on YoutubeNote: if anyone could suggest some more reliable sources in the comments, I will happily add them to the list (I understand that the current aren\'t too reliable).'	235	5	0	0
44	40	1	225	5	b"Dropout means that every individual data point is only used to fit a random subset of the neurons. This is done to make the neural network more like an ensemble model.That is, just as a random forest is averaging together the results of many individual decision trees, you can see a network network trained using dropout as averaging together the results of many individual neural networks (with 'results' understood to mean activations at every layer, rather than just the output layer)."	80	0	0	0
45	17	1	1151	9	b"The technological singularity is a theoretical point in time at which a self-improving artificial general intelligence becomes able to understand and manipulate concepts outside of the human brain's range, that is, the moment when it can understand things humans, by biological design, can't.The fuzziness about the singularity comes from the fact that, from the singularity onwards, history is effectively unpredictable. Humankind would be unable to predict any future events, or explain any present events, as science itself becomes incapable of describing machine-triggered events. Essentially, machines would think of us the same way we think of ants. Thus, we can make no predictions past the singularity. Furthermore, as a logical consequence, we'd be unable to define the point at which the singularity may occur at all, or even recognize it when it happens.However, in order for the singularity to take place, AGI needs to be developed, and whether that is possible is quite a hot debate right now. Moreover, an algorithm that creates superhuman intelligence out of bits and bytes would have to be designed. By definition, a human programmer wouldn't be able to do such a thing, as his/her brain would need to be able to comprehend concepts beyond its range. There is also the argument that an intelligence explosion (the mechanism by which a technological singularity would theoretically be formed) would be impossible due to the difficulty of the design challenge of making itself more intelligent, getting larger proportionally to its intelligence, and that the difficulty of the design itself may overtake the intelligence required to solve said challenge (last point credit to god of llamas in the comments).Also, there are related theories involving machines taking over humankind and all of that sci-fi narrative. However, that's unlikely to happen, if Asimov's laws are followed appropriately. Even if Asimov's laws were not enough, a series of constraints would still be necessary in order to avoid the misuse of AGI by misintentioned individuals, and Asimov's laws are the nearest we have to that."	331	0	1	0
46	-1	0	0	6	b'When did research into Artificial Intelligence first begin? Was it called Artificial Intelligence then or was there another name?'	18	0	0	0
47	28	0	704	2	b"The notion of genetics used in Genetic Algorithms (GAs) is a very stripped down version relative to genetics in nature, essentially consisting of a population of 'genes' (representing solutions to some predefined problem) subject to `survival of the fittest' during iterated application of recombination and mutation.Nowadays, the term 'Computational Intelligence' (CI) tends to be used to describe computational techniques intended to produce `the appearance of intelligence by any computational means', rather than specifically attempting to mimic the mechanisms that are believed to give rise to human (or animal) intelligence.That said, the distinction between CI and AI is not so hard and fast, and arguably arose during the `AI Winter' when the term AI was out of fashion."	116	0	0	0
48	42	0	384	2	b'Hidden layers by themselves aren\'t useful. If you had hidden layers that were linear, the end result would still be a linear function of the inputs, and so you could collapse an arbitrary number of linear layers down to a single layer.This is why we use nonlinear activation functions, like RELU. This allows us to add a level of nonlinear complexity with each hidden layer, and with arbitrarily many hidden layers we can construct arbitrarily complicated nonlinear functions.Because we can (at least in theory) capture any degree of complexity, we think of neural networks as "universal learners," in that a large enough network could mimic any function.'	106	0	0	0
49	35	0	659	5	b'Machine learning is a subset of artificial intelligence. Roughly speaking, it corresponds to its learning side. There is no "official" definitions, boundaries are a bit fuzzy.'	25	0	0	0
50	-1	0	0	0	b'How would you estimate the generalisation error? What are the methods of achieving this?'	13	0	0	0
51	42	1	453	3	b'"Hidden" layers really aren\'t all that special... a hidden layer is really no more than any layer that isn\'t input or output. So even a very simple 3 layer NN has 1 hidden layer. So I think the question isn\'t really "how do hidden layers help?" as much as "why are deeper networks better?". And the answer to that latter question is an area of active research. Even top experts like Geoffrey Hinton and Andrew Ng will freely admit that we don\'t really understand why deep neural networks work. That is, we don\'t understand them in complete detail anyway.That said, the theory, as I understand it goes something like this... successive layers of the network learn successively more sophisticated features, which build on the features from preceding layers. So, for example, an NN used for facial recognition might work like this: the first layer detects edges and nothing else. The next layer up recognizes geometric shapes (boxes, circles, etc). The next layer up recognizes primitive features of a face, like eyes, noses, jaw, etc. The next layer up then recognizes composites based on combinations of "eye" features, "nose" features, and so on. So, in theory, deeper networks (more hidden layers) are better in that they develop a more granular / detailed representation of "thing" being recognized. '	216	0	0	0
52	-1	0	0	2	b"I've implemented the reinforcement learning alogrithm for an agent to play snappy bird (a shameless cheap ripoff of flappy bird) utilizing a q-table for storing the history for future lookups. It works and eventually achieves perfect convergence after enough training.Is it possible to implement a neural network to do function approximation in order to accomplish the purpose of the q-table? Obviously storage is a concern with the q-table, but it doesn't seem to ever train with the neural net alone. Perhaps training the nnet on an existing q-table would work, but I would like to not use a q-table at all if possible."	102	0	0	0
53	35	0	877	16	b'Machine learning has been defined by many people in different ways. One definition says, that machine learning (ML) is the field of study that gives computers the ability to learn without being explicitly programmed.Given the above definition, we might say that machine learning is geared towards problems, for which we have (lots of) data (experience), from which a program can learn and can get better at a task.Artificial intelligence has many more aspects, where machines do not get better at tasks by learning from data, but may exhibit intelligence through rules (e.g. expert systems like Mycin), logic or algorithms, e.g. finding paths.The TOC of Artificial Intelligence: A Modern Approach shows more research fields of AI, like Constraint Satisfaction Problems, Probabilistic Reasoning or Philosophical Foundations.'	123	0	2	0
54	-1	0	0	3	b'I read that in the spring of 2016 a computer Go program was finally able to beat a professional human for the first time. Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of even more processing power being applied to the problem? What are some of the methods used to program the successful Go playing program, and are those methods considered to be artificial intelligence?'	79	0	0	0
55	35	0	916	2	b' Machine learning is a science that involves development of self-learning algorithms. These algorithms are more generic in nature that it can be applied to various domain related problems.  Artificial Intelligence is a science to develop a system or software to mimic human to respond and behave in a circumference. As field with extremely broad scope, AI has defined its goal into multiple chunks. Later each chuck has become a separate field of study to solve its problem.Sakthi Dasan Sekar'	80	0	1	0
56	35	0	925	6	b"Many terms have 'mostly' the same meanings, and so the differences are just in emphasis, perspective, or historical descent. People disagree as to which label refers to the superset or the subset; there are people who will call AI a branch of ML and people who will call ML a branch of AI.I typically hear Machine Learning used as a form of 'applied statistics' where we specify a learning problem in enough detail that we can just feed training data into it and get a useful model out the other side.I typically hear Artificial Intelligence as a catch-all term to refer to any sort of intelligence embedded in the environment or in code. This is a very expansive definition, and others use narrower ones (such as focusing on artificial general intelligence, which is not domain-specific). (Taken to an extreme, my version includes thermostats.)This is also a good time to point out other StackExchange sites, Cross Validated and Data Science, which have quite a bit of overlap with this sit."	168	0	2	0
57	17	0	1775	2	b'The "singularity," viewed narrowly, refers to a point at which economic growth is so fast that we can\'t make useful predictions about what the future past that point will look like.It\'s often used interchangeably with "intelligence explosion," which is when we get so-called Strong AI, which is AI that is intelligent enough to understand and improve itself. It seems reasonable to expect that the intelligence explosion would immediately lead to an economic singularity, but the reverse is not necessarily true.'	79	0	0	0
58	-1	0	0	5	b'Who first coined the term Artificial Intelligence, is there a published research paper which is the first to use that term?'	20	0	0	0
59	46	0	711	0	b' The creator of Artificial Intelligence studies begins with the work of pioneer computer scientist Alan Turing (1912-1954) who in the 1930\'s evolved a concept of a "Turing Machine."eNotes In layman\'s terms, it began in the 1930s, and they were most likely called "Machines" not AI.'	45	0	1	0
60	-1	0	0	5	b'I have a background in Computer Engineering and have been working on developing better algorithms to mimic human thought. (One of my favorites is Analogical Modeling as applied to language processing and decision making.) However, the more I research, the more I realize just how complicated AI is.I have tried to tackle many problems in this field, but sometimes I find that I am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable (ie. the halting problem). So, to help in furthering AI, I want to better understand the current obstacles that are hindering our progress in this field.For example, time and space complexity of some machine learning algorithms is super-polynomial which means that even with fast computers, it can take a while for the program to complete. Even still, some algorithms may be fast on a desktop or other computer while dealing with a small data set, but when increasing the size of the data, the algorithm becomes intractable.What are other issues currently facing AI development?'	175	0	0	0
61	36	0	1323	2	b"Quantum computers can help further develop A.I. algorithms and solve the problems to the extent of our creativity and ability to define the problem. For example breaking cryptography can take seconds, where it can takes thousands of years for standard computers. The same with artificial intelligence, it can predict all the combinations for the given problem defined by algorithm. This is due to superposition of multiple states of quantum bits.Currently, quantum computers are still in the early stages of development and can perform complex calculation. There are already technologies like D-Wave systems which are used by Google and NASA for complex data analysis, using Multi-Qubit type quantum computers for solving NSE fluid dynamics problems of interest or global surveillance for military purposes, and many more which we're not aware.Currently there are only a few quantum computers available to the public, like IBM Quantum Experience (the world\xe2\x80\x99s first quantum computing platform delivered via the IBM Cloud), but it's programming on quantum logic gates levels, so we're many years behind creating artificial intelligence available to public. There are some quantum computing languages such as QCL, Q or Quipper, but I'm not aware any libraries which can provide artificial intelligence frameworks. It doesn't mean it's not there, and I'm sure huge companies and governments organisations are using it for their agenda to outcome the competition (like financial market analysis, etc.)."	226	0	1	0
62	28	0	1587	2	b"Human intelligence is not an example of natural genetic algorithms.Genetic algorithms have collections of solutions that are collided with each other to make new solutions, eventually returning the best solution. Human intelligence is a network of neurons doing information processing, and almost all of it doesn't behave the same way.But that something doesn't behave in the same way that human intelligence does doesn't mean that it's not an AI algorithm; I would include 'genetic algorithms' as a numerical optimization technique, and since optimization and intelligence are deeply linked any numerical optimization technique could be seen as an AI technique."	98	0	0	0
63	-1	0	0	6	b"I've read that the most of the problems can be solved with 1-2 hidden layers.How do you know you need more than 2? For what kind of problems you would need them (as example)?"	33	0	0	0
64	-1	0	0	2	b"What were the first areas of research into Artificial Intelligence and what were some early successes? More recently we've had:Beating a human at the game of chessConvincing a human that a person was conversing with them (passing the Turing test)Beating a human at Jeopardy game showBeating a human at the game of go.Were there milestones that were considered major in the field before the 1990s?"	64	0	0	0
65	41	1	1314	7	b"Short answer: No.Longer answer: It depends on what IQ exactly is, and when the question is asked compared to ongoing development. The topic you're referring to is actually more commonly described as AGI, or Artificial General Intelligence, as opposed to AI, which could be any narrow problem solving capability represented in software/hardware.Intelligence quotient is a rough estimate of how well humans are able to generally answer questions they have not previously encountered, but as a predictor it is somewhat flawed, and has many criticisms and detractors.Currently (2016), no known programs have the ability to generalize, or apply learning from one domain to solving problems in an arbitrarily different domain through an abstract understanding. (However there are programs which can effectively analyze, or break down some information domains into simpler representations.) This seems likely to change as time goes on and both hardware and software techniques are developed toward this goal. Experts widely disagree as to the likely timing and approach of these developments, as well as to the most probable outcomes.It's also worth noting that there seems to be a large deficit of understanding as to what exactly consciousness is, and disagreement over whether there is ever likely to be anything in the field of artificial intelligence that compares to it."	210	0	0	0
66	58	0	364	5	b'John McCarthy (1927 - 2011) was an American computer scientist. A pioneer in the foundations of artificial intelligence research, he coined the term "artificial intelligence". He was one of the creators of the (original) Lisp programming language, which was quite involved in early AI research in the 1960\'s and 1970\'s.He coined the term in 1955, and organized the first Artificial Intelligence conference in 1956, while working as a math teacher at Dartmouth. He founded the AI labs at MIT and Stanford.He\'s responsible for developing several other important concepts in today\'s mainstream computer science. Namely, he developed garbage collection (used by a Lisp interpreter) and designed the first time-sharing systems.On a side note, McCarthy predicted that creating a truly intelligent machine would require "1.8 Einsteins and one-tenth the resources of the Manhattan Project."'	131	0	0	0
67	-1	0	0	1	b'Why somebody would use SAT solvers (Boolean satisfiability problem) to solve their real world problems?Are there any examples of the real uses of this model?'	24	0	0	0
68	-1	0	0	4	b'What designs for genetic algorithms are there, if they are classified differently and/or have different names, that leverage models for epigenetics in evolution? What are the pros/cons of the designs? Are there vast insufficiencies or wide-open questions about their usefulness? '	40	0	0	0
69	54	0	825	5	b'It doesn\'t make much sense to have a single threshold with "unintelligent" below it and "intelligent" above it.I think it makes more sense to have a gradation of intelligence by cognitive task. Inverting a matrix is a \'cognitive task,\' and one where working memory pays off immensely; computers have been much better at that cognitive task than humans for a long time.What the AlphaGo victory represents has several components. One is that we have algorithms that are competitive with the best board-game playing humans at doing tactical and strategic thinking in the well-described world of Go. Another is that the deeper structure of the human visual system seems to have been duplicated, and so we have algorithms that can recognize patterns as well as humans--with very limited resolution. (AlphaGo is seeing one pixel per stone, whereas we have very, very high-resolution eyes and the visual cortex to match.)Different people have different intuitions, but it seems to me that visual intelligence is a huge component of human intelligence in general. If we know most of the secrets of human visual intelligence, that means there might be many tasks that computers could now perform as well as humans (if provided the correct training data).'	201	0	0	0
70	-1	0	0	10	b"Can a Convolutional Neural Network be used for pattern recognition in a problem domain where there are no pre-existing images, say by representing abstract data graphically? Would that always be less efficient?This developer says current development could go further but not if there's a limit outside image recognition. "	48	0	0	0
71	46	0	1473	6	b"The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 30s, 40s and early 50s (e.g. formal logic, automata, robots). Although the Turing test was proposed in 1950s by Alan Turing, the work culminated back in the 1940s in the invention of the programmable digital computers, an abstract essence of mathematical reasoning. These ideas were inspired by a handful of scientists from a variety of fields who began seriously considering the possibility of building an electronic brain. The field of artificial intelligence research was founded as an academic discipline in 1956.However the concept of artificial beings is not new and it's as old as Greek myths of Hephaestus and Pygmalion which incorporated the idea of intelligent robots (such as Talos) and artificial beings (such as Galatea and Pandora).See the following articles at Wikipedia for further details:Artificial intelligence (AI) History of artificial intelligenceTimeline of artificial intelligence"	153	0	0	0
72	67	1	440	2	b'Instead of talking about just SAT solvers, let me talk about optimization in general. Many economic problems can be cast as optimization problems: for example, FedEx may have a list of packages and the destinations for those packages, and must decide which packages to put on which trucks, and what order to deliver those packages in.If you write out a mathematical description of this problem, there are a truly stunning number of possible solutions, and a well-defined way to evaluate which of two solutions is better. A solver is an algorithm that will evaluate a solution, come up with another solution, and then evaluate that one, and so on.In small cases and simple problems, the solver can also terminate with a proof that it is actually the best solution possible. But typically instead the solver just reports "this is the best solution that I\'ve seen," and that\'s used. An improvement in the solver means you can reliably get lower-cost solutions than you were seeing before.For the SAT problem specifically, the Wikipedia page on SAT gives some examples: Since the SAT problem is NP-complete, only algorithms with exponential worst-case complexity are known for it. In spite of this, efficient and scalable algorithms for SAT were developed over the last decade[when?] and have contributed to dramatic advances in our ability to automatically solve problem instances involving tens of thousands of variables and millions of constraints (i.e. clauses).[1] Examples of such problems in electronic design automation (EDA) include formal equivalence checking, model checking, formal verification of pipelined microprocessors,[12] automatic test pattern generation, routing of FPGAs,[14] planning, and scheduling problems, and so on. A SAT-solving engine is now considered to be an essential component in the EDA toolbox.'	283	0	0	0
73	37	0	1970	7	b'A Markov model includes the probability of transitioning to each state considering the current state. "Each state" may be just one point - whether it rained on specific day, for instance - or it might look like multiple things - like a pair of words. You\'ve probably seen automatically generated weird text that almost makes sense, like Garkov (the output of a Markov model based on the Garfield comic strips). That Coding Horror article also mentions the applications of Markov techniques to Google\'s PageRank.Markov models are really only powerful when they have a lot of input to work with. If a machine looked through a lot of English text, it would get a pretty good idea of what words generally come after other words. Or after looking through someone\'s location history, it could figure out where that person is likely to go next from a certain place. Constantly updating the "input corpus" as more data is received would let the machine tune the probabilities of all the state transitions.Genetic algorithms are fairly different things. They create functions by shuffling around parts of functions and seeing how good each function is at a certain task. A child algorithm will depend on its parents, but Markov models are interested mostly in predicting what thing will come next in a sequence, not creating a new chunk of code. You might be able to use a Markov model to spit out a candidate function, though, depending on how simple the "alphabet" is. You could even then give more weight to the transitions in successful algorithms.'	260	0	0	0
74	-1	0	0	21	b"I've heard the terms strong-AI and weak-AI used. Are these well defined terms or subjective ones? How are they generally defined?"	20	0	0	0
75	-1	0	0	-3	b'As AI gains capabilities, and becomes more prevalent in society, our legal system will encounter questions it has not encountered before. For example, if a self-driving car is involved in an accident while being controlled by the AI, who is at fault? The "driver" (who\'s really just a passenger), the programmer(s) who made the AI, or the AI itself?So, what\'s on the cutting edge in terms of these kinds of issues at the intersection of law and artificial intelligence?'	78	0	0	0
76	54	0	1354	7	b'There are at least two questions in your question:  What are some of the methods used to program the successful go playing program?and Are those methods considered to be artificial intelligence?The first question is deep and technical, the second broad and philosophical.The methods have been described in: Mastering the Game of Go with Deep Neural Networks and Tree Search.The problem of Go or perfect information games in general is that: exhaustive search is infeasible.So the methods will concentrate on shrinking the search space in an efficient way.Methods and structures described in the paper include:learning from expert human players in a supervised fashionlearning by playing against itself (reinforcement learning)Monte-Carlo tree search (MCTS) combined with policy and value networksThe second question has no definite answer, as you will have at least two angles on AI: strong and weak. All real-world systems labeled "artificial intelligence" of any sort are weak AI at most.So yes, it is artificial intelligence, but it is non-sentient.'	159	0	0	0
77	-1	0	0	12	b'I know that language of Lisp was used early on when working on artificial intelligence problems. Is it still being used today for significant work? If not, is there a new language that has taken its place as the most common one being used for work in AI today?'	48	0	0	0
79	75	0	178	1	b'One person working in this space is Dr. Woody Barfield. He just wrote a book titled "Cyberhumans: Our Future With Machines" that focuses largely on the legal/policy issues around AI (and related topics). In addition to the book, he is continuing with other research in this area.'	46	0	1	0
80	-1	0	0	5	b'What are the specific requirements of the Turing Test?What requirements if any must the evaluator fulfill in order to be qualified to give the test?Must there always be two participants to the conversation (one human and one computer) or can there be moreAre placebo tests (where there is not actually a computer involbed) allowed or encouraged?Can there be multiple evaluators? If so does the decision need to be unanimous among all evaluators in order for the machine to have passed the test?'	81	0	0	0
81	-1	0	0	2	b'I believe that statistical AI uses inductive thought processes. For example, deducing a trend from a pattern. What are some examples of successfully applying statistical AI to real world problems.'	29	0	0	0
82	-1	0	0	1	b'How do the basic components optimality theory apply to artificial intelligence?How is optimality theory related to neural network research?'	18	0	0	0
83	1	0	4525	1	b'Yes, as Franck has rightly put, "backprop" means backpropogation, which is frequently used in the domain of neural networks for error optimization.For a detailed explanation, I would point out this tutorial on the concept of backpropogation by a very good book of Michael Nielsen. '	44	0	1	0
84	-1	0	0	4	b'Some programs do exhaustive searches for a solution while others do heuristic searches. For example, in chess, the search for the best next move tends to be more exhaustive in nature whereas in go, the search for the best next move tends to be more heuristic in nature due to the much larger search space.Is the technique of brute force exhaustive searching for a good answer considered to be AI or is it generally required that heuristic algorithms be used before being deemed AI? If so, is the chess playing computer beating a human professional seen as a meaningful milestone?'	99	0	0	0
85	80	1	784	7	b'The "Turing Test" is generally taken to mean an updated version of the Imitation Game Alan Turing proposed in his 1951 paper of the same name. An early version had a human (male or female) and a computer, and a judge had to decide which is which, and what gender they were if human. If they were correct less than 50% then the computer was considered "intelligent."The current generally accepted version requires only one contestant, and a judge to decide whether it is human or machine. So yes, sometimes this will be a placebo, effectively, if we consider a human to be a placebo.Your first and fourth questions are related - and there are no strict guidelines. If the computer can fool a greater number of judges then it will of course be considered a better AI.The University of Toronto has a validity section in this paper on Turing, which includes a link to Jason Hutchens\' commentary on why the Turing test may not be relevant (humans may also fail it) and the Loebner Prize, a formal instantiation of a Turing Test .'	182	0	3	0
86	-1	0	0	11	b'How is a neural network having the "deep" adjective actually distinguished from other similar networks?'	14	0	0	0
87	81	0	609	6	b'There are several examples. For example, one instance of using Statistical AI from my workplace is:Analyzing the behaviour of the customer and their food-ordering trends, and then trying to upsell by reccommending them the dishes which they might like to order. This can be done through the apriori and FP-growth algorithms. We then, automated the algorithm, and then the algorithm improves itself through a Ordered/Not-Ordered metric.Self-driving cars. They use reinforcement and supervised learning algorithms for learning the route and the gradient/texture of the surface.'	83	0	0	0
88	-1	0	0	3	b'What is the effectiveness of pre-training of unsupervised deep learning?Does unsupervised deep learning actually work?'	14	0	0	0
89	84	0	388	3	b'If a computer is just brute-forcing the solution, it\'s not learning anything or using any kind of intelligence at all, and therefore it shouldn\'t be called "artificial intelligence." It has to make decisions based on what\'s happened before in similar instances. For something to be intelligent, it needs a way to keep track of what it\'s learned. A chess program might have a really awesome measurement algorithm to use on every possible board state, but if it\'s always trying each state and never storing what it learns about different approaches, it\'s not intelligent.'	92	0	0	0
91	-1	0	0	9	b"Are search engines considered AI because of the way they analyze what you search for and remember it? Or how they send you ads of what you've searched for recently? Is this considered AI or just smart?"	36	0	0	0
92	-1	0	0	29	b'The following page/study demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images, e.g.How this is possible? Can you please explain ideally in plain English?'	31	0	2	0
93	86	1	410	15	b'Short answer: The difference is mostly in the number of layers. For a long time, it was believed that "1-2 hidden layers are enough for most tasks" and it was impractical to use more than that, because training neural networks can be very computationally demanding.Nowadays, computers are capable of much more, so people have started to use networks with more layers and found that they work very well for some tasks.The word "deep" is there simply to distinguish these networks from the traditional, "more shallow" ones.'	85	0	0	0
94	-1	0	0	3	b'In a feedforward neural network the inputs are fed directly to the outputs via a series of weights.What purpose do the weights serve and how are they significant in this neural network?'	31	0	0	0
95	86	0	557	7	b'Deep Learning is just (feed forward)neural networks with many layers.However, deep belief networks, Deep Boltzman networks, etc are not considered(debatable) as Deep Learning, as their topology is different (they ave undirected networks in their topology).Helpful Reference'	35	0	1	0
96	-1	0	0	4	b"I'm pretty sure this a noob-y question, but what is Deep Network? As of now it is the most popular tag on AI. Is there a reason for this? Please note, I am not asking how to distinguish a deep network from a neural network, I am simply asking for the definition of deep network."	54	0	0	0
97	91	1	647	11	b'I believe it would be more correct to say that (some) search engines use AI. Broadly saying "search engines are AI" is not really correct. At core, most search engines are nothing more than an inverted text index using something like tf\xe2\x80\x93idf scoring. That\'s a very mechanical / simple thing that nobody would really call AI. But more sophisticated search engines may use AI or AI techniques to do things like semantic analysis - so they can actually "answer questions" instead of just looking up words in an index. '	89	0	0	0
98	96	1	313	6	b'Deep Network is nothing but a neural network which has multiple layers. Multiple can be subjective.However, any network which has >=6 or 7 layers are considered deep. So, the above would form a very basic definition of a deep network. '	40	0	0	0
100	96	0	1434	4	b"Deep networks have two main differences with 'normal' networks.The first is that computational power and training datasets have grown immensely, meaning that it's practical to run larger networks and statistically valid (that is, we have enough training examples that we won't just run into over-fitting problems with larger networks).The second is that back propagation is limited the more layers you have; each layer represents a gradient of the error, and so by the time one is about six layers deep there isn't much error left to modify the neuron weights. But one might reasonably expect earlier neurons to be more important than later neurons, since they represent 'concepts' that are closer to the raw inputs.New training techniques sidestep this problem, typically by doing unsupervised learning on the raw inputs, creating higher-level 'concepts' that are then useful as inputs for supervised learning.(For example, consider the problem of determining whether or not an image contains a cat from the pixels. The early layers of the network should be doing things like detecting edges, which one could expect to be shared among all images and mostly independent of what one is trying to do with the output layers, thus also hard to train through 'cat-not cat' signals many layers up."	206	0	0	0
101	84	0	2581	3	b'If one thinks of intelligence as a continuous measure of optimization power (that is, how much better are outcomes for any unit of cognitive effort expended), then exhaustive search has non-zero intelligence (in that it does actually give better outcomes as more effort is expended) but very, very low intelligence (as the outcomes are better mostly by luck, and the amount of effort expended can be impossibly large).'	67	0	0	0
102	80	0	3358	1	b'There are really two questions here, that I can see. One is "what were the specific requirements of the original Turing test, as stated by Turing himself." The other is "what should the specific requirements of a modern Turing test be?" Things have advanced a lot since Turing\'s day, and I think it\'s reasonable for us to consider extending / modifying his test to reflect our current understanding.The answer to the first question is easy enough to look up, so I think the interesting one is the second one. What should a test to determine intelligence look like? With that in mind, I think the answer to all four questions posed by the OP is "it depends". I don\'t think there\'s universal consensus on how to structure a perfect Turing test, so a given experimenter is really free to set things up however he/she wants. This is all, of course, based on the assumption that the Turing test, or a Turing-test-like test is actually of value. That\'s not necessarily a given. Consider that, to some extent, what we\'re talking about is designing an AI with an exceptional ability for deceit! That is, assuming the questioner is allowed to simply ask "are you human", then we have to assume that the AI is supposed to lie if it wants to pass the test. So one might rightly ask, is designing a system to be really good at telling lies, a valuable approach to AI?'	242	0	0	0
103	-1	0	0	1	b'I believe that Classical AI uses deductive thought processes. For example, given as a set of constraints, deduce a conclusion. What are some examples of successfully applying Classical AI to real world problems.'	32	0	0	0
104	-1	0	0	8	b'In this video an expert says, "One way of thinking about what intelligence is [specifically with regard to artificial intelligence], is as an optimization process."Can intelligence always be thought of as an optimization process, and can artificial intelligence always be modeled as an optimization problem? What about pattern recognition? Or is he mischaracterizing?'	52	0	0	0
105	103	0	1462	5	b"The term classical AI refers to the concept of intelligence that was broadly accepted after the Dartmouth Conference and basically refers to a kind of intelligence that is strongly symbolic and oriented to logic and language processing. One basic point is the duality body vs. mind. It's in this period that the mind start to be compared with computer software.Two classical historic examples of this conception of intelligenceDeep Blue, whose aim in life was to be the master of chess, ruling over the (not-so) intelligent mankindEliza, a computer-based therapist that turned out to trigger a critic to the classical AITwo technical examples of classical AIExpert systems, which are computer programs that strongly rely on the type of constrains and conclusions that you refer to, *in order to accomplish feats of apparent intelligence](https://www.britannica.com/technology/expert-system)Fuzzy logic, which is an extension of multivalued logic, but with continuous values instead of discrete onesNote that in all cases the hardware (once compared with the body) does not play any role: Intelligence is abstract and independent from the material world."	172	0	1	0
106	54	0	6836	2	b' Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of ever more processing power being applied to the problem? Neither, really. It is a milestone and a significant advance in computers beating humans in games, but the techniques used are only relevant to that game, not for other purposes in AI. The solution lies in humans analysing the game and implementing algorithms for finding a good move. This is the main reason that a computer can beat the humans, together with the fact that it can calculate much faster and that it doesn\'t make really bad moves by not seeing something.Processing power helps, but the game-tree complexity for go is very large, estimated to be larger than 10200, whereas the game-tree complexity for chess is only 10120 (known as the Shannon number), so chess is less hard. This means that for neither chess nor go a database can be created with all possible positions. The fact that Deep Blue beat Kasparov in a six-game match in 1997 was quite a development, since this was one of the first "hard" games where a computer beat a top human. But it still isn\'t really Artificial Intelligence, more analysing the game. Implementing an opening and endgame book was a large part for chess, the middle game was done using analysis, I don\'t know the details. '	237	0	0	0
107	81	0	5223	2	b"There're many online services that uses statistical neural networks for recommendations. For example we have well known service here in Russia that could give it's users recommendations for movies and shows to watch and books to read. It's recommendation core is based on many things known about user: what movies/books he or she loves and what not, analyses his or her friends likes and do on. While you have only few items rayed it will give you very strange recommendations but then it becomes more correct and really could give you some true gems."	93	0	1	0
108	-1	0	0	3	b'What specific advantages of declarative languages make them more applicable to AI than imperative languages? What can declarative languages do easily that other languages styles find difficult for this kind of problem?'	31	0	0	0
109	-1	0	0	1	b'In years past, GOFAI (Good Old Fashioned AI) was heavily based on "rules" and symbolic computation based on rules. Unfortunately, that approach ran into stumbling blocks, and the world moved heavily towards statistical / probabilistic approaches leading to the current wave of interest in "machine learning".It seems though, that the symbolic / rule based approach probably still has application. So, could one "learn" rules using a probabilistic rule induction method, and then layer symbolic computation on top? If so, how could the whole process be made truly two-way, so that something "learned" from processing rules, can be fed back into how the system learns rules? '	105	0	0	0
111	-1	0	0	37	b'Obviously driverless cars aren\'t perfect, so imagine that the Google car (as an example) got into difficult situation.Here are a few examples of unfortunate situations caused by set of events:the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,killing animal on the street in favour of human being,changing lanes to crash into another car to avoid killing a dog,And here are few dilemmas:Does the algorithm recognize the difference between a human being and an animal?Does the size of the human being or animal matter?Does it count how many passengers it has vs. people in the front?Does it "know" when babies/children are on board?Does it take into the account the age (e.g. killing the older first)?How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?Related articles:Why Self-Driving Cars Must Be Programmed to KillHow to Help Self-Driving Cars Make Ethical Decisions'	205	0	0	0
112	-1	0	0	3	b"Which deep neural network is used in Google's driverless cars to analyse the surroundings? Is this information is open?"	18	0	0	0
113	-1	0	0	6	b'Two common activation functions used in deep learning are the hyperbolic tangent function and the sigmoid activation function. I understand that the hyperbolic tangent is just a rescaling and translation of the sigmoid function (i,e tanh(z) = 2*sigma(z) - 1). Is there a significant difference between these two activation functions, and in particular, when is one preferable to the other?I realize that in some cases (like when estimating probabilities) outputs in the range of [0,1] are more convenient than outputs that range from [-1,1]. I want to know if there are differences other than convenience which distinguish the two activation functions.'	100	0	0	0
114	36	1	10576	11	b'Quantum computers are super awesome at matrix multiplication, with some limitations. Quantum superposition allows each bit to be in a lot more states than just zero or one, and quantum gates can fiddle those bits in many different ways. Because of that, a quantum computer can process a lot of information at once for certain applications.One of those applications is the Fourier transform, which is useful in a lot of problems, like signal analysis and array processing. There\'s also Grover\'s quantum search algorithm, which finds the single value for which a given function returns something different. If an AI problem can be expressed in a mathematical form amenable to quantum computing, it can receive great speedups. Sufficient speedups could transform an AI idea from "theoretically interesting but insanely slow" to "quite practical once we get a good handle on quantum computing."'	140	0	5	0
115	77	0	8950	2	b'In my opinion python and java have taken over from LISP. Many people use them, there is a large amount of libraries available. And more importantly, they are easy to integrate in web technologies. '	34	0	0	0
116	108	0	3542	3	b'The advantage of a declarative language like Prolog is that it can be used to express facts and inference rules separately from control flow. This allows the developer to focus on the data and inference rules (the knowledge model), and allows the developer to extend the knowledge model more easily.I should add that in practice, this dichotomy between facts/rules on the one hand, and control flow on the other, is not strict. A knowledge engineer who writes a code base in Prolog does sometimes have to consider control flow. The "!" operator is used so that the developer can influence the evaluation of the rules.'	104	0	0	0
117	104	0	4923	7	b'A good answer to this question depends on what you want to use the labels for.When I think about "optimization," I think about a solution space and a cost function; that is, there are many possible answers that could be returned and we can know what the cost is of any particular answer.In this view, the answer is "yes"--pattern recognition is a case where each pattern is a possible answer, and the optimization method is trying to find the one where the cost is lowest (that is, where the answer matches what you want it to match).But most interesting optimization problems are characterized by exponential solution spaces and clean cost functions, and so can be thought of more as \'search\' problems, whereas most pattern recognition problems are characterized by simple solution spaces and complicated cost functions, and it might feel unnatural to put the two of them together.(In general, I do think that optimization and intelligence are deeply linked enough that optimization power is a good measure of intelligence, and certainly a better measure of the practical use of intelligence than pattern recognition.)'	182	0	0	0
118	-1	0	0	3	b'Fuzzy logic is the logic where every statement can have any real truth value between 0 and 1.How can fuzzy logic be used in creating AI? Is it useful for certain decision problems involving multiple inputs? Can you give an example of an AI that uses it?'	46	0	1	0
119	113	0	1527	3	b"I don't think it makes sense to decide activation functions based on desired properties of the output; you can easily insert a calibration step that maps the 'neural network score' to whatever units you actually want to use (dollars, probability, etc.).So I think preference between different activation functions mostly boils down to the different properties of those activation functions (like whether or not they're continuously differentiable). Because there's just a linear transformation between the two, I think that means there isn't a meaningful difference between them."	85	0	0	0
120	-1	0	0	4	b"In Minds, Machines and G\xc3\xb6del (1959), J. R. Lucas shows that any human mathematician can not be represented by an algorithmic automaton (a Turing Machine, but any computer is equivalent to it by the Church-Turing thesis), using G\xc3\xb6del's incompleteness theorem. As I understand it, he states that since the computer is an algorithm and hence a formal system, G\xc3\xb6del's incompleteness theorem applies. But a human mathematician also has to work in a formal axiom system to prove a theorem, so wouldn't it apply there as well? "	86	0	1	0
121	118	0	715	2	b'My impression is that fuzzy logic has mostly declined in relevance and probabilistic logic has taken over its niche. (See the comparison on Wikipedia.) The two are somewhat deeply related, and so it\'s mostly a change in perspective and language.That is, fuzzy logic mostly applies to labels which have uncertain ranges. An object that\'s cool but not too cool could be described as either cold or warm, and fuzzy logic handles this by assigning some fractional truth value to the \'cold\' and \'warm\' labels and no truth to the \'hot\' label.Probabilistic logic focuses more on the probability of some fact given some observations, and is deeply focused on the uncertainty of observations. When we look at an email, we track our belief that the email is "spam" and shouldn\'t be shown to the user with some number, and adjust that number as we see evidence for and against it being spam.'	150	0	0	0
122	118	1	726	6	b"A classical example of fuzzy logic in an AI is the expert system Mycin.Fuzzy logic can be used to deal with probabilities and uncertainties. If one looks at, for example, predicate logic, then every statement is either true or false. In reality, we don't have this mathematical certainty. For example, let's say a physician (or expert system) sees a symptom that can be attributed to a few different diseases (say A, B and C). The physician will now attribute a higher likelihood to the possibility of the patient having any of these three diseases. There is no definite true or false statement, but there is a change of weights. This can be reflected in fuzzy logic, but not so easily in symbolic logic."	122	0	0	0
123	-1	0	0	8	b"Back in college, I had a Complexity Theory teacher who stated that artificial intelligence was a contradiction in terms. If it could be calculated mechanically, he argued, it wasn't intelligence.This seems to be a variant of the Chinese Room argument. This argument is a metaphor, where a person is put in a room full of Chinese books. This person doesn't understand a word of Chinese, but is slipped messages in Chinese under the door. The person has to use the books, which contain transformation rules, to answer these messages. The person can apply the transformation rules, but does not understand what (s)he is communicating.Does the chinese room argument hold? Can we argue that artificial intelligence is merely clever algorithmics?"	118	0	0	0
124	120	0	956	2	b"After he lays out his argument, he deals with some counterarguments. The following looks like the weakest one to me: We can use the same analogy also against those who, finding a formula their first machine cannot produce as being true, concede that that machine is indeed inadequate, but thereupon seek to construct a second, more adequate, machine, in which the formula can be produced as being true. This they can indeed do: but then the second machine will have a G\xc3\xb6delian formula all of its own, constructed by applying G\xc3\xb6del's procedure to the formal system which represents its (the second machine's) own, enlarged, scheme of operations. And this formula the second machine will not be able to produce as being true, while a mind will be able to see that it is true. And if now a third machine is constructed, able to do what the second machine was unable to do, exactly the same will happen: there will be yet a third formula, the G\xc3\xb6delian formula for the formal system corresponding to the third machine's scheme of operations, which the third machine is unable to produce as being true, while a mind will still be able to see that it is true. And so it will go on.In short, by making the system more complex, it can see the inadequacy of a less complex system, but a yet more complex system can see its inadequacy. But from whence comes the claim that a mind could see the inadequacy in the nth machine? If, say, the G\xc3\xb6delian formula had as many components to it as a human brain had neurons, it seems suspect to claim that the human could evaluate that formula and identify that it is in fact a G\xc3\xb6delian formula, rather than a similar but not quite identical sentence. "	302	0	0	0
125	120	1	1124	2	b'Yes, it applies. If a statement cannot be derived in a finite number of steps, then it doesn\'t matter if the person trying to prove it is a human or a computer. The mathematician has one advantage over a standard theorem proving algorithm: the mathematician can "step out of the system" (as Douglas Hofstadter called in G&ouml;del, Escher, Bach), and start thinking about the system. From this point of view, the mathematician may find that the derivation is impossible.However, an AI for proving theorems could be programmed to recognize patterns in the derivation, just like our hypothetical mathematician, and start reasoning about the formal system to derive properties of the formal system itself.Both the AI and the mathematician would still be bound by the laws of mathematics, and not be able to prove a theorem if it was mathematically improvable.'	139	0	0	0
126	123	0	564	2	b'Depends on who you ask! John Searle, who proposed this argument, would say "yes", but others would say it is irrelevant. The Turing Test does not stipulate that a machine must actually "understand" what it is doing, as long as it seems that way to a human. You could argue that our "thinking" is only a more sophisticated form of clever algorithmics.'	61	0	0	0
127	123	0	641	4	b'There are two broad types of responses to philosophical queries like this.The first is to make analogies and refer to intuition; one could, for example, actually calculate the necessary size for such a Chinese room, and suggest that it exists outside the realm of intuition and thus any analogies using it are suspect.The second is to try to define the terms more precisely. If by "intelligence" we mean not "the magic thing that humans do" but "information processing," then we can say "yes, obviously the Chinese Room involves successful information processing."I tend to prefer the second because it forces conversations towards observable outcomes, and puts the difficulty of defining a term like "intelligence" on the person who wants to make claims about it. If "understanding" is allowed to have an amorphous definition, then any system could be said to have or not have understanding. But if "understand" is itself understood in terms of observable behavior, then it becomes increasingly difficult to construct an example of a system that "is not intelligent" and yet shares all the observable consequences of intelligence.'	179	0	0	0
128	123	0	698	5	b"It depends on the definition of (artificial) intelligence. The position that Searle originally tried to refute with the Chinese room experiment was the so-called position of strong AI: An appropriately programmed computer would have a mind in the exact same sense as humans have minds. Alan Turing tried to give an definition of artificial intelligence with the Turing Test, stating that a machine is intelligent if it can pass the test. The Turing Test is introduced here. I won't explain it in detail because it is not really relevant to the answer. If you define (artificial) intelligence as Turing did, then the Chinese room experiment is not valid.So the point of the Chinese room experiment is to show that an appropriately programmed computer is not the same as a human mind, and therefore that Turing's Test is not a good one. "	141	0	0	0
129	68	0	12238	4	b'Genetic algorithms are an analogy to biology, not a copy of it. The core piece of the analogy is that the "phenotype," or the observable portion of a solution, is constructed from the "genotype," or the internal portion of a solution. For example, a number (the phenotype) can be stored as a binary series of 0s and 1s (the genotype), and by changing individual bits we make potentially dramatic changes in the resulting number, and by combining two genotypes we can get a broad range of \'related\' numbers.Epigenetics are a wrinkle in the genotype -> phenotype mapping, making it a non-deterministic function, and so incorporating them would degrade the performance of a genetic algorithm by adding unnecessary noise.'	117	0	0	0
130	-1	0	0	1	b'What are the main differences between Deep Boltzmann Machines (DBM) recurrent neural network and Deep Belief Network (which is based on RBMs)?'	21	0	0	0
131	77	1	11764	6	b'The following thread has many answers regarding why LISP used to be thought of as the AI language: Why is Lisp used for AI and the following is an answer by Peter Norvig, who wrote a popular textbook on the subject and is currently Director of Research at Google: Is it true that Lisp is highly used programming language in AI?I am not overly familiar with the history, but I think LISP was oversold to industry as "the AI language". It is a good language for humans to think in and pioneered many important ideas which have since been incorporated into many modern languages (see the Wikipedia page), but it is no way the "best". It was likely also popular because it is very expressive: you can write short programs to represent complex ideas, a property it shares with other functional languages in use such as Scala. This also means that it is easy to write a program that is very hard to debug in LISP. Modern functional languages have been trying to do better in this regard through typing etc.The paradigm for AI that currently receives most attention is Machine Learning, i.e. learning hypothesis from data, as opposed to previous approaches like Expert Systems where experts wrote rules for the AI to follow. Python is currently the most widely used language for prototyping machine learning algorithms and has many libraries and an active community. Another important detail about modern AI is the volume of data it uses. Big Data analysis is done using cluster computing systems like Hadoop (with code written in Java) and Spark (with code written in Python or Scala). Often, the core time-intensive subroutines are written in C, but this is often done in the form of third-party libraries.Finally it must be said that the AI Winter of the 80s was not because we did not have the right language, but because we did not have the right algorithms and did not have enough computational power. This has changed as GPUs have gotten better. If you\'re trying to learn AI, spend your time studying algorithms and not languages.'	351	0	1	0
132	94	1	10509	4	b"You described a single layer feed forward network. They can have multiple layers. The significance of the weights is that they make a linear transformation from the output of the previous layer and hand it to the node they are going to. To say it more simplistically, they specify how important (and in what way: negative or positive) is the activation of node they are coming from to activating the node they are going to.In your example, since there is only one layer (a row of input nodes and a row of output nodes) it is easy to explain what each node represents. However in multilayer feed forward networks they can become abstract representations which makes it difficult to explain them and therefore explain what the weights that come to them or go out of them represent.Another way of thinking about it is that they describe hyperplanes in the space of the output of the previous node layer. If each output from the previous layer represents a point in space, a hyperplane decides which part of the space should give a positive value to the plane's corresponding node in the next layer and which part should give a negative input to it. It actually cuts that space into two halves. If you consider the input space of a multi-layer feed forward network, the weights of the first layer parametrize hyperplanes, however in the next layers they can represent non-linear surfaces in the input space."	243	0	0	0
134	111	1	4370	24	b'The answer to a lot of those questions depends on how the device is programmed. A computer capable of driving around and recognizing where the road goes is likely to have the ability to visually distinguish a human from an animal, whether that be based on outline, image, or size. With sufficiently sharp image recognition, it might be able to count the number and kind of people in another vehicle. It could even use existing data on the likelihood of injury to people in different kinds of vehicles.Ultimately, people disagree on the ethical choices involved. Perhaps there could be "ethics settings" for the user/owner to configure, like "consider life count only" vs. "younger lives are more valuable." I personally would think it\'s not terribly controversial that a machine should damage itself before harming a human, but people disagree on how important pet lives are. If explicit kill-this-first settings make people uneasy, the answers could be determined from a questionnaire given to the user.'	162	0	0	0
135	130	1	847	2	b'The graph that represents a Deep Boltzmann Machine can be any weighted undirected graph. However, the graph that represents a Deep Belief Network must be a connection of graphs that represent Restricted Boltzmann Machines. Those graphs are biparite, so there are two groups of vertices in those graphs so that every edge connects two vertices from different groups. Those groups are usually the visible and hidden components of the machine. Learning is hard and impractical and hard in a general Deep Boltzmann Machine, but easier and practical in a Restricted Boltzmann Machine and hence in a Deep Belief Network, which is a connection of some of these machines. '	108	0	0	0
136	-1	0	0	0	b"I'd like to learn more about the differences between related automata which can be based on hexagonal cells instead of squares (rule 34/2), like in CoDi model which uses spiking neural network (SNN). Is using a plane tiled with regular hexagons is more efficient and reliable than using square cells? What is the difference and how do I know which one I should use in which scenario?In other words, more efficient in terms of flexibility that it learn to solve much more difficult problems and can be used for more scenarios (for me hexagonal sounds like it has more possibilities, because it can send/share the signal with/to more tiles). Or maybe one is more modern than the other, or both they're on the same level? In general, I'd like to learn the differences between them to know when I should use one over another. "	144	0	0	0
137	123	0	2074	4	b"First of all, for a detailed view of the argument, check out the SEP entry on the Chinese Room.I consider the CRA as an indicator of you definition of intelligence. If the argument holds, yes, the person in the room understands Chinese. However, let's sum up the three replies discussed in the SEP entry:The man himself doesn't understand Chinese (he wouldn't be able to understand it when outside the room), but the system man+room understands it. Accepting that reply suggests that there can exist an intelligent system which parts aren't themselves intelligent (which can be argued of the human body itself).The system doesn't understand Chinese, as it cannot interact with the world in the same way a robot or a human could (i.e. it cannot learn, is limited in the set of questions it can answer)The system doesn't understand Chinese (depending on your definition of understanding), and you couldn't say a human performing the same feats as the Chinese room understands Chinese either.So whether the argument, or a variant of it holds, depends on your definitions of intelligent, understanding, on how you define the system, etc. The point being that the thought experiment is a nice way to differentiate between the definitions (and many, many debates have been held about them), in order to avoid talking past each other endlessly."	219	0	1	0
138	50	1	15325	3	b'Generalization error is the error obtained by applying a model to data it has not seem before. So, if you want to measure generalization error, you need to remove a subset from your data and don\'t train your model on it. After training, you verify your model accuracy (or other performance measure) on the subset you have removed, since your model hasn\'t seem it before. Hence, this subset is called a "test set". Additionally, another subset can also be used for parameter selection, which we call a "validation set". We can\'t use the training set for parameter tuning, since it does not measure generalization error, but we can\'t use the test set too, since our parameter tuning would overfit test data. That\'s why we need a third subset.Finally, in order to obtain more predictive performance measures, we can use many different train/test partitions and average the results. This is called "cross-validation".'	150	0	0	0
139	108	0	8392	3	b'There\'s no objective reason to state that declarative languages are better suited for AI development. However, there\'s indeed a bias towards them in practice. Although most functional languages are impure (that is, they allow side effects), and such can\'t count as "declarative", a few languages are purely functional (that is, they don\'t allow side effects), most prominently Haskell. Purity is key here. In Haskell, even I/O is pure.The key difference between imperative languages and (purely) functional languages is in the way they describe the program. An imperative program describes how to do stuff, that is, algorithms. It specifies the specific instructions that the machine must carry on in order to perform the computation. OTOH, purely functional languages describe what is to be computed, that is, the relationship between the input and the output. In mathematics, "function" is just a fancy name for a relationship between an input and an output.Again, in the mathematical sense, the only variability is that of the function\'s arguments. That is, the function\'s output depends solely on its input (arguments). This is known as referential transparency. Referential transparency states that:Where \xcf\x95 is the set of all functions, and \xce\xb4(f) is f\'s domain. For the typical imperative language\'s definition of "function", the above doesn\'t hold. For instance, C\'s getchar() does not always return the same value.Let\'s say we want to calculate the set of the ten least prime numbers whose least significant digit is 3. First, in mathematical notation:Where G(n, s) is the set of the lesser nth elements from s. In mathematics, you don\'t worry about how is the set S supposed to be computed, but rather about S\'s definition itself.Now, in Python (in imperative style):In Python, we care about (and are responsible for) the algorithm being used to compute the set. We specify, pretty much in recipe-style, how to build the set from scratch. If there\'s an algorithm that may be better suited for checking whether a number is prime or odd, but we don\'t use it, it\'s our fault, not Python\'s.Finally, Haskell steps in:Haskell\'s version is a lot more like the mathematical model than Python is. We define the isPrime function in terms of the constraints that a prime number must obey, not by describing a step-by-step algorithm to do such a check. Moreover, s (the set we have been defining so far) is defined in terms of the constraints its members must obey, rather than in terms of an algorithm to compute s itself. The compiler, more often than not The Glorious Glasgow Haskell Compilation System (a.k.a GHC), is the one responsible for generating an algorithm. GHC\'s optimizer is known to be one of the strongest in the world, not because its the best compiler of \'em all, but because Haskell\'s nature allows for this.Haskell (and other functional languages), in summary, have several features that make it look, taste, and behave like pure math:Referential transparency and purity.Haskell is lazy.A very strong type system, with such exotic (but very useful!) stuff as recursive (and algebraic) data types.So, the bottom line is that AI researchers often prefer functional languages over imperative languages (or, more assertively, pure over impure languages), because they are attempting to define artificial intelligence itself, by means of functions (relationship between an input and an output). At the end, we do this because we have no real algorithm for human-level intelligence to raise from a handful of transistors. Also, there\'s been a historical bias towards these kind of languages, starting with John McCarthy, Lisp\'s creator and a pioneer in early AI research.'	589	21	2	0
140	-1	0	0	6	b"An ultraintelligent machine is a machine that can surpass all intellectual activities by any human, and such machine is often used in science fiction as a machine that brings mankind to an end. Any machine is executed using an algorithm. By the Church-Turing thesis, any algorithm that can be executed by a modern computer can be executed by a Turing Machine. However, a human can easily simulate a Turing Machine. Doesn't this mean that a machine can't surpass all intellectual activities, since we can also execute the algorithm?This argument is most likely flawed, since my intuition tells me that ultraintelligent machines are possible. However, it is not clear to me where the flaw is. Note that this is my own argument. "	121	0	0	0
141	74	1	14748	19	b'The terms strong and weak don\'t actually refer to processing, or optimization power, or any interpretation leading to "strong AI" being stronger than "weak AI". It holds conveniently in practice, but the terms come from elsewhere. In 1980, John Searle coined the following statements:AI hypothesis, strong form: an AI system can think and have a mind (in the philosophical definition of the term);AI hypothesis, weak form: an AI system can only act like it thinks and has a mind.So strong AI is a shortcut for an AI systems that verifies the strong AI hypothesis. Similarly, for the weak form. The terms have then evolved: strong AI refers to AI that performs as well as humans (who have minds), weak AI refers to AI that doesn\'t.The problem with these definitions is that they\'re fuzzy. For example, AlphaGo is an example of weak AI, but is "strong" by Go-playing standards. A hypothetical AI replicating a human baby would be a strong AI, while being "weak" at most tasks.Other terms exist: Artificial General Intelligence (AGI), which has cross-domain capability (like humans), can learn from a wide range of experiences (like humans), among other features. Artificial Narrow Intelligence refers to systems bound to a certain range of tasks (where they may nevertheless have superhuman ability), lacking capacity to significantly improve themselves.Beyond AGI, we find Artificial Superintelligence (ASI), based on the idea that a system with the capabilities of an AGI, without the physical limitations of humans would learn and improve far beyond human level.'	249	0	0	0
142	16	1	18291	5	b'In some iterative learning methods the more iterations you apply the more specific your model becomes about the training set. If there are too much iterations, your model will become too specifically trained for the training samples and will score less on other samples that are not seen during the training phase. This is call over-fitting, though over-fitting is not specific to iterative learning methods.One solution to prevent over-fitting in these iterative learning algorithms is early stopping. Normally a control group of samples called validation samples (validation set) are used to validate the model and notify when it starts to over-fit. The validation set is not used by the training algorithm however its corresponding outputs are known and after each iteration its samples are employed to measure how good the model currently works. As soon as the performance on the validation set stops to grow and starts to drop we stop iterating the training algorithm. This is called early stopping which can help to maximize the generalization power of our learned model.Note that if we use the training set itself for validation the performance will always increase because that is what the learning algorithm is designed to do. However the learning algorithm does not know how specifically it should learn the training set and that is why we need methods like early stopping.'	222	0	0	0
143	28	1	17906	3	b'An ability that is commonly attributed to intelligence is problem solving. Another one is learning (improving itself from experience).Artificial intelligence can be defined as "replicating intelligence, or parts of it, at least in appearance, inside a computer" (dodging the definition of intelligence itself).Genetic algorithms are computational problem solving tools that find and improve solutions (they learn).Thus, genetic algorithms are a kind of artificial intelligence.Regarding scale, I don\'t see it as an important factor for defining G.A. as A.I or not. The same way we can simply classify different living forms as more or less intelligent instead of just saying intelligent or not intelligent.Finally, let\'s just make an important distinction: our brains are the product of natural selection, but the brains themselves don\'t use the same principle in order to achieve intelligence.'	130	0	0	0
144	140	1	1699	2	b"I believe this argument is based on the fact that intelligence is a single dimension when it really isn't. Are machines and humans really on the same level if a machine can solve a complex problem in a millionth of the time a human can? It also assumes that the Turing machine is still the best computational model for the time period that you are in, which is not necessarily true for the future, it is just true until this point in time. "	83	0	0	0
145	-1	0	0	3	b"From Wikipedia: AIXI ['ai\xcc\xafk\xcd\xa1si\xcb\x90] is a theoretical mathematical formalism for artificial general intelligence. It combines Solomonoff induction with sequential decision theory. AIXI was first proposed by Marcus Hutter in 2000[1] and the results below are proved in Hutter's 2005 book Universal Artificial Intelligence.[2]Albeit non-computable, approximations are possible, such as AIXItl. Finding approximations to AIXI could be an objective way for solving AI.My question is: is AIXI really a big deal in artificial general intelligence research? Can it be thought as a central concept for the field? If so, why don't we have more publications on this subject (or maybe we have and I'm not aware of them)?"	106	0	0	0
146	-1	0	0	1	b'In what ways can connectionist artificial intelligence (neural networks) be integrated with Good Old-Fashioned A.I. (GOFAI)? For instance, how could deep neural networks be integrated with knowledge bases or logical inference? One such example seems to be the OpenCog + Destin integration.'	41	0	1	0
147	-1	0	0	3	b'It is proved that a recurrent neural net with rational weights can be a super-Turing machine. Can we achieve this in practice ?'	22	0	0	0
148	-1	0	0	10	b'Given the proven halting problem for Turing machines, can we infer limits on the ability of strong Artificial Intelligence?'	18	0	0	0
149	140	0	2416	1	b"A quantum computer has a huge amount of internal state that even the machine can't get at directly. (You can only sample the matrix state.) The amount of that state goes up exponentially with each quantum bit involved in the system. Some operations get insane speedups from quantum computing: you just put the quantum wire through a quantum gate and you've updated the entire matrix at once.Simulating a quantum computer with a classical one would take exponentially longer for each qubit. With several dozen qubits, the machine's computing power for some tasks couldn't even be approached by a normal computer, much less a human mind.Relevant: my answer on To what extent can quantum computers help to develop Artificial Intelligence?Note that with quantum computers, you've gone beyond the normal zeroes and ones. You then need a quantum Turing machine, which is a generalization of the classical one."	145	0	1	0
151	-1	0	0	1	b'By default using DeepDream technique you can creating a dreamlike image out of two different images.Is it possible to easily enhance this technique to generate one image out from three?'	29	0	0	0
152	-1	0	0	6	b'Consider these neural style algorithms which produce some art work:Neural Doodleneural-styleWhy is generating such images so slow and why does it take huge amounts of memory? Isn\'t there any method of optimizing the algorithm?What is the mechanism or technical limitation behind this? Why we can\'t have a realtime processing?Here are few user comments (How ANYONE can create Deep Style images): Anything above 640x480 and we\'re talking days of heavy crunching and an insane amount of ram. I tried doing a 1024pixel image and it still crashed with 14gigs memory, and 26gigs swap. So most of the VM space is just the swapfile. Plus it takes several hours potentially days cpu rendering this. I tried 1024x768 and with 16gig ram and 20+ gig swap it was still dying from lack of memory. Having a memory issue, though. I\'m using the "g2.8xlarge" instance type.'	141	0	0	0
153	-1	0	0	7	b'Can autoencoders be used for supervised learning without adding an output layer? Can we simply feed it with a concatenated input-output vector for training, and reconstruct the output part from the input part when doing inference? The output part would be treated as missing values during inference and some imputation would be applied.'	52	0	0	0
154	-1	0	0	7	b"I'm aware that neural networks are probably not designed to do that, however asking hypothetically, is it possible to train the deep neural network (or similar) to solve math equations?So given the 3 inputs: 1st number, operator sign represented by the number (1 - +, 2 - -, 3 - /, 4 - *, and so on), and the 2nd number, then after training the network should give me the valid results.Example 1 (2+2):Input 1: 2; Input 2: 1 (+); Input 3: 2; Expected output: 4Input 1: 10; Input 2: 2 (-); Input 3: 10; Expected output: 0Input 1: 5; Input 2: 4 (*); Input 3: 5; Expected output: 25and soThe above can be extended to more sophisticated examples.Is that possible? If so, what kind of network can learn/achieve that?"	129	0	0	0
155	154	0	1223	2	b"Not really.Neural networks are good for determining non-linear relationships between inputs when there are hidden variables. In the examples above the relationships are linear, and there are no hidden variables. But even if they were non-linear, a traditional ANN design would not be well suited to accomplish this.By carefully constructing the layers and tightly supervising the training, you could get a network to consistently produce the output 4.01, say, for the inputs: 2, 1 (+), and 2, but this is not only wrong, it's an inherently unreliable application of the technology."	90	0	0	0
156	-1	0	0	12	b'From Wikipedia: A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another.Mirror neurons are related to imitation learning, a very useful feature that is missing in current real-world A.I. implementations. Instead of learning from input-output examples (supervised learning) or from rewards (reinforcement learning), an agent with mirror neurons would be able to learn by simply observing other agents, translating their movements to its own coordinate system. What do we have on this subject regarding computational models?'	89	0	0	0
157	-1	0	0	5	b"If I have a paragraph I want to summarize, for example: Ponzo and Fila went to the mall during the day. They walked for a long while, stopping at shops. They went to many shops. At first, they didn't buy anything. After going to a number of shops, they eventually bought a shirt, and a pair of pants.Better summarized as: They shopped at the mall today and bought some clothes.What is the best AI strategy to automate this process, if there is one? If there isn't, is it because it would be dependent on first having an external information resource that would inform any algorithm? Or is it because the problem is inherently contextual?"	113	0	0	0
158	154	1	4400	6	b"Yes, it has been done!However, the applications aren't to replace calculators or anything like that. The lab I'm associated with develops neural network models of equational reasoning to better understand how humans might solve these problems. This is a part of the field known as Mathematical Cognition. Unfortunately, our website isn't terribly informative, but here's a link to an example of such work.Apart from that, recent work on extending neural networks to include external memory stores (e.g. Neural Turing Machines) tend to use solving math problems as a good proof of concept. This is because many arithmetic problems involve long procedures with stored intermediate results. See the sections of this paper on long binary addition and multiplication."	116	0	3	0
159	-1	0	0	1	b'What happens if you apply the same deep dream technique which produces "dream" visuals, but to media streams such as audio files?Does changing image functions into audio and enhancing the logic would work, or it won\'t work or doesn\'t make any sense?My goal is to create "dream" like audio based on the two samples.'	53	0	0	0
160	-1	0	0	0	b'With which neural network it is possible to scale the learning between the independent networks?For example given the stream of images one network is trained to recognise cats, another dogs, and so on, all of them are talking to the main visual network which is responsible for making some decision and pass the analysis to the main "brain" network. Then another network of neural network are given the audio so each network can recognise specific pattern, then they talk to the common audio specific network which talks to the main "brain" network (mentioned before). In other words, something like a robot.Which type of network would be the most suitable and scalable for such configuration, so you can easily extend it for additional separate network modules? Does it matter which type of deep network (or not deep) I should choose, or not?'	140	0	0	0
161	111	0	16440	10	b'This is the well known Trolley Problem. As Ben N said, people disagree on the right course of action for trolley problem scenarios, but it should be noted that with self-driving cars, reliability is so high that these scenarios are really unlikely. So, not much effort will be put into the problems you are describing, at least in the short term.'	60	0	1	0
162	111	0	17220	4	b"For a driverless car that is designed by a single entity, the best way for it to make decisions about whom to kill is by estimating and minimizing the probable liability.It doesn't need to absolutely correctly identify all the potential victims in the area to have a defense for its decision, only to identify them as well as a human could be expected to.It doesn't even need to know the age and physical condition of everyone in the car, as it can ask for that information and if refused, has the defense that the passengers chose not to provide it, and therefore took responsibility for depriving it of the ability to make a better decision.It only has to have a viable model for minimizing exposure of the entity to lawsuits, which can then be improved over time to make it more profitable."	141	0	0	0
163	13	0	29022	1	b'Well I do not know what type of features you are giving to your neural network. However in general I would go with a single neural network. It seems that you have no limitation in resources for training your network and the only problem is resources while you apply your network. The thing is that probably the two problems have things in common (e.g. both types of plates are rectangular). This means that if you use two networks, each has to solve the same sub-problem (the common part) again. If you use only one network the common part of the problem takes fewer cells/weights to be solved and the remaining weights/cells can be employed for better recognition.At the end if I was in your place I would try both of them. I think that is the only way to be really sure what is the best solution. When speaking theoretically it is possible that we do not include some factors.'	159	0	0	0
164	77	0	27073	3	b'LISP is still used significantly, but less and less. There is still momentum due to so many people using it in the past, who are still active in the industry or research (anecdote: the last VCR was produced by a Japanese maker in July 2016, yes). The language is however used (to my knowledge) for the kind of AI that does not leverage Machine Learning, typically as the reference books from Russell and Norvig. These applications are still very useful, but Machine Learning gets all the steam these days.Another reason for the decline is that LISP practitioners have partially moved to Clojure and other recent languages.If you are learning about AI technologies, LISP (or Scheme or Prolog) is good choice to understand what is going on with "AI" at large. But if you wish or have to be very pragmatic, Python or R are the community choicesNote: The above lacks concrete example and reference. I am aware of some work in universities, and some companies inspired by or directly using LISP.To add on @Harsh\'s answer, LISP (and Scheme, and Prolog) has qualities that made it look like it was better suited for creating intelligent mechanisms---making AI as perceived in the 60s.One of the qualities was that the language design leads the developer to think in a quite elegant way, to decompose a big problem into small problems, etc. Quite "clever", or "intelligent" if you will. Compared to some other languages, there is almost no choice but to develop that way. LISP is a list processing language, and "purely functional".One problem, though, can be seen in work related to LISP. A notable one in the AI domain is the work on the Situation Calculus, where (in short) one describes objects and rules in a "world", and can let it evolve to compute situations---states of the world. So it is a model for reasoning on situations. The main problem is called the frame problem, meaning this calculus cannot tell what does not change---just what changes. Anything that is not defined in the world cannot be processed (note the difference here with ML). First implementations used LISPs, because that was the AI language then. And there were bound by the frame problem. But, as @Harsh mentioned, it is not LISP\'s fault: Any language would face the same framing issue (a conceptual problem of the Situation Calculus).So the language really does not matter from the AI / AGI / ASI perspective. The concepts (algorithms, etc.) are really what matters.Even in Machine Learning, the language is just a practical choice. Python and R are popular today, primarily due to their library ecosystem and the focus of key companies. But try to use Python or R to run a model for a RaspberryPI-based application, and you will face some severe limitations (but still possible, I am doing it :-)). So the language choice burns down to pragmatism.'	480	0	0	0
165	140	0	14555	1	b'The flaw in your argument is that "surpass" doesn\'t just mean that you should be able to run all algorithms, it includes a notion of complexity, i.e. how many time steps you will take to simulate an algorithm. How do you simulate an algorithm with a Turing machine? A Turing machine consists of a finite state machine and an infinite tape. A TM does run an algorithm, determined by its initial state and the state transition matrix, but what I think you are talking about is Universal Turing Machines (UTM) that can read "code" (which is usually a description of another Turing machine) written on a "code segment" of the tape and then simulate that machine on input data written on the "data segment" of the tape.Turing machines can differ in the number of states in their finite state machines (and also in the alphabet they write on the tape but any finite alphabet is easily encoded in binary so this should not be the big reason for differences among Turing machines). So, you can have UTMs with bigger state machines and UTMs with smaller state machines. The bigger UTM could possibly surpass the smaller one if they use the same encoding for the "code" part of the tape.You can also play around with the code used to describe the TM being simulated. This code could be C++ for example, or could be a Neural network with the synapse strength written down as a matrix. Which description is better for computation depends on the problem.An example comparison among UTMs with different state machines: consider different compilers for the same language, say C++. Both of them will first compile C++ to assembly and then run another UTM which reads and executes assembly (your physical CPU). So, a better compiler will run the same code faster.Back to humans vs computers, humans are neural networks that run algorithms like those you would write in C++. This involves a costly and inefficient conversion of the algorithm into hand movements. A computer uses a compiler to convert C++ to assembly that it can run natively, so its able to do a much more efficient implementation of C++ code. Alternately, humans have a ton of neurons, and the neural code, i.e. synapse strength, is hard to read, so current computers cannot run that code yet.'	388	0	0	0
166	77	0	30232	4	b"I definitely continue to often use Lisp when working on AI models.You asked if it is being used for substantial work. That's too subjective for me to answer regarding my own work, but I queried one my AI models whether or not it considered itself substantial, and it replied with an affirmative response. Of course, it's response is naturally biased as well.Overall, a significant amount of AI research and development is conducted in Lisp. Furthermore, even for non-AI problems, Lisp is sometimes used. To demonstrate the power of Lisp, I engineered the first neural network simulation system written entirely in Lisp over a quarter century ago."	105	0	0	0
167	-1	0	0	4	b"In DeepDream wikipedia page it's suggested that a dreamlike images created by a convolutional neural network may be related to how visual cortex works in humans when they're tripping. The imagery to LSD- and psilocybin-induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex.How this is even possible?How exactly convolutional neural networks have anything to do with human visual cortex?"	68	0	0	0
169	-1	0	0	1	b'This 2014 article saying that a Chinese team of physicists have trained a quantum computer to recognise handwritten characters.Why did they have to use a quantum computer to do that?Is it just for fun and demonstration, or is it that recognising the handwritten characters is so difficult that standard (non-quantum) computers or algorithms cannot do that?If standard computers can achieve the same thing, what are the benefits of using quantum computers to do that then over standard methods?'	77	0	0	0
170	148	1	18017	5	b'Does the halting problem imply any limits on human cognition?Yes, absolutely--that there are pieces of code a human could look at and not be sure whether or not it will halt in finite time. (Certainly there are pieces of code that a human can look at and say "yes" or "no" definitely, but we\'re talking about the ones that are actually quite difficult to analyze.)The halting problem means that there are types of code analysis that no computer could do, because it\'s mathematically impossible. But the realm of possibility is still large enough to allow strong artificial intelligence (in the sense of code that can understand itself well enough to improve itself).'	111	0	0	0
171	74	0	34852	5	b'In contrast to the philosophical definitions, which rely on terms like "mind" and "think," there are also definitions that hinge on observables.That is, a Strong AI is an AI that understands itself well enough to self-improve. Even if it is philosophically not equivalent to a human, or unable to perform all cognitive tasks that a human can, this AI can still generate a tremendous amount of optimization power / good decision-making, and its creation would be of historic importance (to put it lightly).A Weak AI, in contrast, is an AI with no or limited ability to self-modify. A chessbot that runs on your laptop might have superhuman ability to play chess, but it can only play chess, and while it might tune its weights or its architecture and slowly improve, it cannot modify itself in a deep enough way to generalize to other tasks.Another way to think about this is that a Strong AI is an AI researcher in its own right, and a Weak AI is what AI researchers produce.'	170	0	0	0
172	-1	0	0	0	b'Is it possible that at some time in the future, AIs will be able to initiatively develop themselves, rather than passively being developed by humanity?'	24	0	0	0
173	148	0	25544	6	b"The halting problem is an example of a general phenomenon known as Undecidability, which shows that there are problems no Turing machine can solve in finite time. Let's consider the generalization that it is undecidable whether a Turing Machine satisfies some non-trivial property P, called Rice's theorem.First note that the halting problem applies only if the Turing machine takes arbitrarily long input. If the input is bounded, it is possible to enumerate all possible cases and the problem is no longer undecidable. It might still be inefficient to calculate it, but then we are turning to the complexity theory, which should be a separate question.Rice's theorem implies that an intelligence (a human) cannot be able to determine whether another intelligence (such as an AGI) possesses a certain property, such as being friendly. This does not mean that we cannot design a Friendly AGI, but it does mean that we cannot check whether an arbitrary AGI is friendly. So, while we can possibly create an AI which is guaranteed to be friendly, we also need to ensure that IT cannot create another AI which is unfriendly."	184	0	0	0
174	167	1	9100	7	b'The similarity of artificial neural networks and the human visual cortex goes very deep, and in many ways the human visual cortex was the inspiration for the techniques we use for the design and implementation of ANNs designed for image recognition. So in that direction, the similarity seems obvious to me.The reverse direction, though, is a question about how the human mind works under the influence of LSD, which you\'ll probably get a better answer asking about in the biology or cognitive science stack exchange sites.Some brief details to add to the answer, though: the human visual cortex is arranged in layers that correspond to increasing layers of abstraction. In the eyes themselves, photons are detected by light-sensitive cells and added together to make what are essentially the color elements of pixels. Those are then routed to another layer which does something like edge detection, and then the next layer does something like shape detection, and so on up to higher level concepts like "a cat\'s face.\' If LSD lowers the activation threshold for those neurons, or makes them more excitable, then more things will be interpreted as having the higher level concept (and so a patch of rough texture may have a face jump out of it, for example).The way that CNN "deep dreaming\' works is that the base image is amplified. That is, to make a particular patch look more like a dog, the shapes are nudged to be more dog-like, and the shapes nudge the edges, and the edges nudge the pixels.'	254	0	2	0
176	169	1	8952	5	b'Handwritten digit recognition is a standard benchmark in Machine Learning in the form of the MNIST dataset. For example, scikit-learn, a python package for Machine Learning uses it as a tutorial example. The paper you cite uses this standard task as a proof of concept, to show that their system works.'	50	0	1	0
177	26	0	46188	4	b'As for your comment about a computer program showing lower emotional intelligence, you may find Eliza (which you can try here) interesting. It is a classical in the history of AI, and pretends to mimic an analyst (psychology).However, I think your question fits nowadays more in the field of Human-Robot Interaction, which relies largely on vision for recognition of gestures and follow movements, as well as soft, natural movements as a response. Note that the movements of the face and hands belong to the most complex tasks, involving many muscles at a time. I strongly recommend the film Plug&amp;Pray to have an idea of what people are researching in this area.On the purely human end of the scale, I sometimes wonder about our (my) emotional intelligence myself. Would I want to implement such an intelligence in an artificial agent at all?I remember why I thought of Eliza: not because of its emotional intelligence, but because it was apparently taken seriously by a couple of humans. Could this be taken as a sort of (approved) Turing test? What does it say about the humans it met?'	184	0	4	0
178	172	0	4734	3	b'This is known as the Intelligence Explosion hypothesis or Recursive self-improvement'	10	0	0	0
179	-1	0	0	5	b"I have been wondering since a while ago about the multiple intelligences and how they could fit in the field of Artificial Intelligence as a whole.We hear from time to time about Leonardo being a genius or Bach's musical intelligence. These persons are commonly said to be (have been) more intelligent. But the multiple intelligences speak about cooking or dancing or chatting as well, i.e. coping with everyday tasks (at least that's my interpretation).Are there some approaches on incorporating multiple intelligences into AI?Related question - How could emotional intelligence be implemented?"	90	0	1	0
180	-1	0	0	1	b'Which is the preferred algorithm to build word vector for a given language?'	12	0	0	0
182	-1	0	0	9	b'How to decide the optimum number of layers to be created while implementing a Neural Network (Feedforward, back propagation or RNN)?'	20	0	0	0
183	182	1	258	5	b'There is a technique called Pruning in neural networks, which is used just for this same purpose.The pruning is done on the number of hidden layers. The process is very similar to the pruning process of decision trees. The pruning process is done as follows:Train a large, densely connected, network with a standard trainingalgorithmExamine the trained network to assess the relative importance of theweightsRemove the least important weight(s)retrain the pruned networkRepeat steps 2-4 until satisfiedHowever, there are several optimized methods for pruning neural nets, and it is also a very active area of research.'	93	0	1	0
184	-1	0	0	1	b'I am interested in the emergence of properties in agents, and, more generally in robotics.I was wondering if there is work on the emergence of time-related concepts, on the low-level representation of notions like before and after. I know, for example, that there is work on the emergence of spatial representation (similar to knn), or even communication* but time seems to be a tricky concept. This has everything to do with the platform, i.e. the way that the representation would be coded in. We tend to favour ways that have some meaning or somehow mimic natural, well, yes, human structures, like the brain. I am not a neuroscientist and do not know that the sense of time looks like in humans, or if it is even present in other living beings.Is there some work on the (emergence of the) representation of time in artificial agents?*I remember watching a really cool... Actually creepy video from these robots but cannot find it anymore. Does anyone have the link at hand?'	167	0	1	0
185	172	0	9388	2	b'Humans might create somewhere in the future a so-called ultraintelligent machine, a machine that can surpass all intellectual activities by any human. This would be the last invention man would need to do, since this machine is better in inventing machines than humans are (since that is an intellectual activity). Also, since humans can create machines as good as the ultraintelligent machine, this machine can create better machines, which in turn can create better machines, etcetera. This is known as the Intelligence explosion, and it is also called recursive self-improvement (as has been pointed out by @Harsh).The existence, let alone the development, if an ultraintelligent machine is still hypothetical. We are nowhere close to creating an ultraintelligent machine.'	117	0	0	0
186	-1	0	0	2	b'Have there been proposed extensions to go beyond a Turing machine that solve the halting problem and if so, would those proposed extensions have value to advance strong Artificial Intelligence? For example, does quantum computing go beyond the definition of a Turing machine and resolve the halting problem, and does that help in creating strong AI?'	55	0	0	0
187	37	0	52172	1	b' (this was intended as a comment, but turned out long and longer)A couple of points to elaborate on Ben\'s answer:It is possible to generate different models (out of existing data!) and then look for the model that best fit new data (e.g. with knn). Example: States = {sleep, eat, walk, work}Model 1: Most probable sequence on weekdays, say: sleep \xe2\x86\x92 sleep \xe2\x86\x92 eat \xe2\x86\x92 walk \xe2\x86\x92 work \xe2\x86\x92 work \xe2\x86\x92 eat \xe2\x86\x92 walk \xe2\x86\x92 sleep \xe2\x86\x92 sleepModel 2: Most probable sequence on weekends, some: sleep \xe2\x86\x92 sleep \xe2\x86\x92 eat \xe2\x86\x92 walk \xe2\x86\x92 eat \xe2\x86\x92 walk \xe2\x86\x92 sleep \xe2\x86\x92 sleepNew data arrives: Which sequence is more probable that it came from? Check model 1, check model 2. Which fits better? \xe2\x86\x92 AssignNote that the previous example is oversimplified. Also note that a unit time is needed there (other than letters / words, for instance).You can nest Markov models. That means that you generate a model (a set of probabilities for all the states) in a "lower scale" and then use it in a more abstract model. For example, you can nest your day-scale model to a month or year (to include holidays, for instance).Also see this link for a nice introduction and some posts in crossvalidated.As for the question if artificial intelligence can be created by using this kind of methods, my personal (easy) answer would be no, because they only relate data and probabilities and thus belong more to the statistics and machine learning branch. A longer answer needs to take into account the weak vs. strong AI question.'	259	0	3	0
188	184	0	3407	1	b"To my knowledge, this is very much an open research issue.Here is a paper by Prof Leslie Smith, an acknowledged expert on neuromorphic perceptual coding, which explains the importance of the notion of perceptual time for Artificial General Intelligence and sketches an architecture from which a notion of 'now' might emerge: Perceptual Time."	52	0	1	0
189	26	1	54878	10	b'Architectures for recognising and generating emotion are typically somewhat complex and don\'t generally have short descriptions, so it\'s probably better to reference the literature rather than give a misleading soundbite:Some of the early work in `Affective Computing\' was done by Rosamund Picard.There is a research group at MIT specializing in this area.Some of the more developed architectural ideas are due to Marvin Minsky.A pre-publication draft of his book, `The Emotion Machine\' is available via Wikipedia.Emotional intelligence would certainly seem to be a necessary component of passing the Turing test - indeed, in the original Turing test essay in Computing Machinery and Intelligence implied some degree of "Theory of Mind" about Mr Pickwick\'s preferences:"Yet Christmas is a Winter\xe2\x80\x99s day, and I do not think Mr Pickwick would mind the comparison."'	128	0	2	0
190	186	0	4556	3	b"It depends a bit on what you mean by 'quantum computer'. The 'conventional' notion is that quantum computation buys a (in some cases, exponential) speedup - it doesn't change what can be computed, just how quickly.In contrast, advocates of hypercomputation claim that quantum effects may make it possible to do infinite computations in finite time. Note, however, that this is not a mainstream belief - the reknowned logician Martin Davis has written an article claiming that hypercomputation is a myth.Roger Penrose has also claimed that quantum vibrations in neural microtubules may be responsible for consciousness."	94	0	1	0
191	-1	0	0	4	b"What was the first AI that was able to carry on a conversation, with real responses, such as in the famous 'I am not a robot. I am a unicorn' case?A 'real response' constitutes a sort-of personalized answer to a specific input by a user."	44	0	0	0
193	146	0	40080	1	b"A neural net with even a single hidden layer is capable of Universal function approximation - it can approximate any continuous function 'as closely as you like'.Hence, one option would be to look for GOFAI applications that would benefit from this property - for example, in state-space search approaches where the utility of a state is not readily defined in advance, and could instead be learned."	65	0	0	0
196	191	0	3125	5	b'In 1986, the first PC therapist program was written by Joseph Weintraub. This program won the first Loebner Prize in 1991, and then again in 1992, 1993 and 1995. In 1981 or 1982, Jabberwacky was founded, which is the foundation of the current Cleverbot. Jabberwacky appeared on the internet in 1997, reaching the third place for the Loebner Prize in 2003, the second place in 2004, and won in 2005 and 2006. In 2008, Cleverbot was launched as an variant of Jabberwacky. I\'m not sure these are really the earliest, but that also depends on what you want earliest (programming started, first conversation, first decent conversation, etc.). Also, it depends on what you call a "real response".'	116	0	0	0
197	-1	0	0	2	b'This question stems from quite a few "informal" sources. Movies like 2001, A Space Odyssey and Ex Machina; books like Destination Void (Frank Herbert), and others suggest that general intelligence wants to survive, and even learn the importance for it.There may be several arguments for survival. What would be the most prominent?'	51	0	0	0
198	-1	0	0	12	b'Identifying sarcasm is considered as one of the most difficult open-ended problems in the domain of ML and NLP.So, was there any considerable research done in that front? If yes, then what is the accuracy like? Please also explain the NLP model briefly.'	42	0	0	0
199	41	0	61072	2	b"It all depends of what your A.I. can do. Even humans cannot do everything.If your AI program is so smart, ask it to take the general IQ tests for humans. Because the real IQ tests are made of several questions from different areas, so in that way you can measure IQ of your AI.This is because the IQ means the tests which are designed to assess human intelligence. An intelligence quotient (IQ) is a total score derived from one of several standardized tests designed to assess human intelligence.wikiSo there is no any other way of measuring IQ without taking IQ test, otherwise it won't be IQ (very logical).If your program is not so smart, you should look for specific tests related to the expertise or problem being solved. Ideally let it compete with humans who has the same expertise in that area, but it's important make the test on the same ground/level.For example the intelligence of Deep Blue project was measured by playing chess with Kasparov. Then if world champion cannot win the game, who will?If you're writing program to play a game, make it play with compete with humans and measure the intelligence in terms of score.The equivalent of IQ for AI is a Turing Test (like MIST and other), see:Is the Turing Test, or any of its variants, a reliable test of artificial intelligence?"	225	0	2	0
200	111	0	51557	20	b'Personally, I think this might be an overhyped issue. Trolley problems only occur when the situation is optimized to prevent "3rd options".A car has brakes, does it not? "But what if the brakes don\'t work?" Well, then the car is not allowed to drive at all. Even in regular traffic, human operators are taught that your speed should be limited as such that you can stop within the area you can see. Solutions like these will reduce the possibility of a trolley problem.As for animals... if there is no explicit effort to deal with humans on the road I think animals will be treated the same. This sounds implausible - roadkill happens often and human "roadkill" is unwanted, but animals are a lot smaller and harder to see than humans, so I think detecting humans will be easier, preventing a lot of the accidents.In other cases (bugs, faults while driving, multiple failures stacked onto each other), perhaps accidents will occur, they\'ll be analysed, and vehicles will be updated to avoid causing similar situations. '	173	0	0	0
201	70	0	59969	5	b'Convolutional Nets (CNN) rely on mathematical convolution (e.g. 2D or 3D convolutions), which is commonly used for signal processing. Images are a type of signal, and convolution can equally be used on sound, vibrations, etc. So, in principle, CNNs can find applications to any signal, and probably more.In practice, there exists already work on NLP (as mentioned by Matthew Graves), where some people process text with CNNs rather than recursive networks. Some other works apply to sound processing (no reference here, but I have yet unpublished work ongoing).Original contents: In answer to the original title question, which has changed now. Perhaps need to delete this one.Research on adversarial networks (and related) show that even deep networks can easily be fooled, leading them to see a dog (or whatever object) in what appears to be random noise when a human look at it (the article has clear examples).Another issue is the generalization power of a neural network. Convolutional nets have amazed the world with their capability to generalize way better than other techniques. But if the network is only fed images of cats, it will recognize only cats (and probably see cats everywhere, as by adversarial network results). In other words, even CNs have a hard time generalizing too far beyond what they learned from.The recognition limit is hard to define precisely. I would simply say that the diversity of the learning data pushes the limit (I assume further detail should lead to more appropriate venue for discussion).'	246	0	1	0
202	-1	0	0	1	b"I'd like to know more about implementing emotional intelligence.Given I'm implementing a chat bot and I'd like to introduce the levels of curiosity to measure whether user text input is interesting or not.High level would mean bot is asking more questions and is following the topic, lower level of curiosity makes the bot not asking any questions and changing the topics.Less interesting content could mean the bot doesn't see any opportunity to learn something new or it doesn't understand the topic or doesn't want to talk about it, because of its low quality. How this possibly can be achieved? Are there any examples?"	102	0	1	0
205	-1	0	0	2	b'I would like to learn more whether it is possible and how to write a program which decompiles executable binary (an object file) to the C source. I\'m not asking exactly \'how\', but rather how this can be achieved.Given the following hello.c file (as example):Then after compilation (gcc hello.c) I\'ve got the binary file like:For the learning dataset I assume I\'ll have to have thousands of source code files along with its binary representation, so algorithm can learn about moving parts on certain changes.My concerns are:do my algorithm needs to be aware about the header file, or it\'s "smart" enough to figure it out,if it needs to know about the header, how do I tell my algorithm \'here is the header file\',what should be input/output mapping (whether some section to section or file to file),do I need to divide my source code into some sections,do I need to know exactly how decompilers work or AI can figure it out for me,or should I\'ve two networks, one for header, another for body it-self,or more separate networks, each one for each logical component (e.g. byte->C tag, etc.)How would you tackle this?'	188	16	0	0
206	202	1	3245	6	b"It's possible to implement a form of curiosity-driven behavior without requiring full 'emotional intelligence'. One elementary strategy would be to define some form of similarity measure on inputs.More generally, Jurgen Schmidhuber has pioneered work on 'Artificial Curiosity/Creativity' and 'Intrinsic Motivation' and has written a number of papers on the subject:Artificial Curiosity Intrinsic MotivationHere is a video of a nice associated presentation."	60	0	2	0
207	-1	0	0	3	b'Text summarization is a long-standing research problem that was "ignited" by Luhn in 1958. However, a half century later, we still came nowhere close to solving this problem (abstractive summarization). The reason for this might be because researchers are resorting to statistical (and sometimes linguistic) methods to find &amp; extract the most salient parts of the text.Is summarization problem solvable using AI (neural networks to be precise)? '	67	0	0	0
208	63	1	64612	6	b"Formally, a single hidden layer is sufficient to approximate a continuous function to any desired degree of accuracy, so in that sense, you never need more than 1.Finding the best topology for a given problem is an open research problem. As far as I know, there are few universal 'rules of thumb' for this.For a given problem, one option is to apply a neuroevolutionary approach such as NEAT, which attempts to find a topology that works well for the problem at hand."	81	0	0	0
210	-1	0	0	0	b"I'd like to know which common file format is more efficient in terms of simplicity and storage space for storing the state of artificial neural network.I'm not talking about memory storage, but file storage, so the data can be loaded later on.My first guess would be XML, but having millions of connections and weights would generate huge amount of data. Another thing would be to dump object instances into binary file using some export/serialize functions, but the disadvantage is that the file isn't common and it's language specific.Are there any common file format standards which can be used for exporting huge artificial neural network into the file to be loaded by another program? If so, which one."	116	0	0	0
211	-1	0	0	1	b'What AI techniques does IBM use for its Watson platform, specifically its natural language analysis?'	14	0	0	0
212	-1	0	0	0	b"I'm investigating the possibility of storing the semantic-lexical connections (such as the relationships to the other words such as phrases and other dependencies, its strength, part of speech, language, etc.) in order to provide analysis of the input text.I assume this has been already done. If so, to avoid reinventing the wheel, is there any efficient method to store and manage such data in some common format which has been already researched and tested?"	73	0	0	0
213	210	0	2593	0	b'In general purpose for this kind of applications, One can use databases such as sqlite, mysql, mssql etc. It simplifies read / write operations, allows for a common language to interact with different databases from different vendors and platforms. '	39	0	0	0
214	-1	0	0	1	b'Which objective and measurable tests have been developed to test the intelligence of AI? The classical test is the Turing Test, which has objective criteria and is measurable since it can be measured what percentage of the jury is fooled by the AI.I am looking for other, more modern tests. '	50	0	0	0
215	210	1	3585	1	b"One option is NeuroML, one of the goals of which is: To facilitate the exchange of complex neuronal models between researchers, allowing for greater transparency and accessibility of modelsIn general, the matrices associated with large neural network models are likely to be sparse. Hence a 'homebrew' alternative to the above would be to use something like the Matrix Market format."	59	0	1	0
216	68	0	75697	5	b"Over the last few years, evolutionary computation research has shown increasing interest in including some aspect of epigenetics. For example:A 2008 paper by Tanev and YutaWork from Lee Spector's genetic programming groupA recent paper by Ricalde and Banzhaf"	37	0	3	0
217	207	0	13714	1	b"The ability to re-frame summarization as a problem for ANN is rather dependent on what kind of output you're looking for: you mentioned 'salient parts of the text'.One possibly is to use a deep learning approach that first chunks together words that belong in the same phrase as a single 'feature'.Another possibility is to identify both key words and relations between them. Here is some previous work on using neural nets for relational learning."	73	0	0	0
218	-1	0	0	4	b"I'm interested in implementing a program for natural language processing (aka ELIZA).Assuming that I'm already storing semantic-lexical connections between the words and its strength.What are the methods of dealing with words which have very distinct meaning?Few examples:'Are we on the same page?'The 'page' in this context isn't a document page, but it's part of the phrase.'I'm living in Reading.'The 'Reading' is a city (noun), so it's not a verb. Otherwise it doesn't make any sense. Checking for the capital letter would work in that specific example, but it won't work for other (like 'make' can be either verb or noun).'I've read something on the Facebook wall, do you want to know what?'The 'Facebook wall' has nothing to do with wall at all.In general, how algorithm should distinguish the word meaning and recognise the word within the context?For example:Detecting the word for different type of speech, so it should recognise whether it's a verb or noun.Detecting whether the word is part of phrase.Detecting word for multiple meaning.What are the possible approaches to solve that problem in order to identify the correct sense of a word with the context?"	186	0	1	0
219	205	1	15340	2	b"In-between your input and desired output, there's obviously a huge space to search. The more relevant domain information you include as features, the higher chance that the Deep Learning (DL) algorithm can find the desired mapping.At this early stage in DL research, there aren't so many rules of thumb to tell you what features to explicitly encode - not least because it depends on the size of your training corpus. My suggestion would be: obtain (or generate) a large corpus of C code, train on that with the most naive feature representation that you think might work, then repeatedly gather data and add more feature preprocessing as necessary.This following paper describes a DL approach to what is almost the 'reverse problem' to yours - generating the source code for a program described in natural language.I found the strength of the results reported in this paper surprising, but it does give me some hope that what you are asking might be possible."	160	0	1	0
220	-1	0	0	1	b'Unsupervised learning does not involve target values, so basically targets are most likely the same as the inputs (in other words, involves no target values).So how does this model learn?'	29	0	0	0
221	-1	0	0	2	b'Currently, many different organizations do cutting-edge AI research, and some innovations are shared freely (at a time lag) while others are kept private. I\'m referring to this state of affairs as \'multipolar,\' where instead of there being one world leader that\'s far ahead of everyone else, there are many competitors who can be mentioned in the same breath. (There\'s not only one academic center of AI research worth mentioning, there might be particularly hot companies but there\'s not only one worth mentioning, and so on.)But we could imagine instead there being one institution that mattered when it comes to AI (be it a company, a university, a research group, or a non-profit). This is what I\'m referring to as "monolithic." Maybe they have access to tools and resources no one else has access to, maybe they attract the best and brightest in a way that gives them an unsurmountable competitive edge, maybe returns to research compound in a way that means early edges can\'t be overcome, maybe they have some sort of government coercion preventing competitors from popping up. (For other industries, network or first-mover effects might be other good examples of why you would expect that industry to be monolithic instead of multipolar.)It seems like we should be able to use insights from social sciences like economics or organizational design or history of science in order to figure out, if not which path seems more likely, how we would know which path seems more likely.(For example, we may be able to measure how much returns to research compound, in the sense of one organization coming up with an insight meaning that organization is likely to come up with the next relevant insight, and knowing this number makes it easier to figure out where the boundary between the two trajectories is located.)'	301	0	0	0
222	1	0	82787	1	b'\'Backprop\' is short for \'backpropagation of error\' in order to avoid confusion when using backpropagation term.Basically backpropagation refers to the method for computing the gradient of the case-wise error function with respect to the weights for a feedforward networkWerbos. And backprop refers to a training method that uses backpropagation to compute the gradient.So we can say that a backprop network is a feedforward network trained by backpropagation.The \'standard backprop\' term is a euphemism for the generalized delta rule which is most widely used supervised training method.Source: What is backprop? at FAQ of Usenet newsgroup comp.ai.neural-netsReferences:Werbos, P. J. (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University.Werbos, P. J. (1994). The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting,Wiley Interscience.Bertsekas, D. P. (1995), Nonlinear Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-14-0.Bertsekas, D. P. and Tsitsiklis, J. N. (1996), Neuro-Dynamic Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-10-8.Polyak, B.T. (1964), "Some methods of speeding up the convergence of iteration methods," Z. Vycisl. Mat. i Mat. Fiz., 4, 1-17.Polyak, B.T. (1987), Introduction to Optimization, NY: Optimization Software, Inc.Reed, R.D., and Marks, R.J, II (1999), Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, Cambridge, MA: The MIT Press, ISBN 0-262-18190-8.Rumelhart, D.E., Hinton, G.E., and Williams, R.J. (1986), "Learning internal representations by error propagation", in Rumelhart, D.E. and McClelland, J. L., eds. (1986), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1, 318-362, Cambridge, MA: The MIT Press.Werbos, P.J. (1974/1994), The Roots of Backpropagation, NY: John Wiley &amp; Sons. Includes Werbos\'s 1974 Harvard Ph.D. thesis, Beyond Regression.'	265	0	0	0
223	220	1	1032	5	b'Supervised learning is typically an attempt to learn a mathematical function, f(X)=y. For this, you need both the input vector X and the output vector y. The model outputs have whatever type / dimensionality / etc. that the target values have. basically targets are most likely the same as the inputs.This doesn\'t seem right to me.Unsupervised learning models instead learn a structure from the data. A clustering model, for example, is learning both how many clusters exist in the data (a number that\'s not the same type as the inputs) and where those clusters are located (which is also a different type from the inputs). The output of running this model on a new datapoint x is not the same type as x, but instead a classification label.Similarly, time series models learn parameters that symbolize how vectors in the input relate to each other, rather than raw inputs themselves.As for how they learn, the structures are mathematical objects whose fitness is determined by the input data. The simplest possible unstructured unsupervised learning problem is probably "what\'s the mean of the data?", and it should be clear how that\'s \'learned\' through processing the input. More sophisticated models are just adding more pieces to that calculation.'	203	0	0	0
224	-1	0	0	4	b'One of the most compelling applications for AI would be in augmenting human biological intelligence. What are some of the currently proposed methods for doing this aside from vague notions such as "nanobots swimming around our brains and bodies" or "electrodes connected to our skulls"?'	44	0	0	0
225	-1	0	0	-6	b'Given list of fixed numbers from a mathematical constant such as Pi, is it is possible to train AI to attempt to predict the next numbers?Which AI or neural network would be more suitable for this task? Especially the one which will work without memorizing the entire training set, but the one which will attempt to find some patterns or statistical association.'	61	0	0	0
226	224	0	1751	5	b"'Direct augmentation' of human intelligence, of the sort that you would see in science fiction, looks to be very hard. Most of our promising approaches deal with avoiding damage rather than adding capabilities--there's no drug that you can take now that will make you smarter to the degree that missing a night of sleep can make you dumber.The most informative area of current practice is probably game-playing, where 'centaurs,' or humans working with computers, outcompete human players or computer players.But a centaur player doesn't have a wire jutting out of their skull to jack into the computer; they're looking at a laptop screen. One of the reasons to be pessimistic about cyborg augmentation is because current I/O technology is already so good. Why install a new wire to put information into your visual cortex, when you come already equipped with two? If you could think code directly onto the screen, how much better would that be than typing code through a keyboard? Probably some, but I find it difficult to imagine that it'll be more than twice as good. So most human-computer intelligence augmentation will look like people using software, and software using human inputs, rather than humans and computers evolving together.Transcranial Direct Current Stimulation (TDCS) and similar approaches cause temporary changes in mental abilities by raising or lowering the activation potentials of neurons in particular regions of the brain. (I've done it myself, a few years ago, and what weak effects I noticed were probably negative. Not too much surprise for a DIY setup!)It looks like it has a number of useful implications. One article about TDCS that I found particularly striking was the journalist who tried it gushing about how their anxiety disappeared for a few days, presumably because the part of their brain behind the anxiety was dampened. One could imagine it being useful for the treatment of many different mental disorders.That said, I'm pessimistic that it will translate into superior peak performance, and I think that's the sort of thing that's more relevant for discussions of augmentation. (Is there TDCS that we could do that would make Terrence Tao better at doing mathematics?)Where improved AI methods will come into play is by improving our models of the brain, allowing us to better target interventions, much in the way that AI methods are improving our treatment of cancer (through superior diagnosis and targeting of radiotherapy, as two easy examples). These effects will all be indirect--for example, AI empowering an app or gadget that helps you sleep better won't directly augment your intelligence, but will cause population-level increases in effective intelligence through reducing sleep deprivation.I haven't talked yet about nootropics, chemicals that increase intelligence, but it's reasonable to expect that AI will improve drug discovery there like it improves drug discovery for anything else. But the same caveats apply--the effect of nootropics seem to be negatively correlated with intelligence (that is, the smarter someone already is, the harder it is to increase their intelligence further)."	496	0	1	0
227	-1	0	0	6	b'What are the main differences between two types of feedforward networks such as multilayer perceptrons (MLP) and radial basis function (RBF)?What are the fundamental differences between these two types?'	28	0	0	0
228	225	1	424	3	b'Pseudo-random number generators are specifically defined to defeat any form of prediction via \'black box\' observation. Certainly, some (e.g. linear congruential) have weaknesses, but you are unlikely to have any success in general in predicting the output of a modern RNG. For devices based on chaotic physical systems (e.g. most national lotteries), there is no realistic possibility of prediction."Patterns or statistical association" is a much weaker criterion than \'prediction\'. Some very recent work has applied topological data analysis to visualize patterns within the infamous Randu RNG.'	85	0	0	0
229	227	0	408	-4	b' Multilayer Perceptron networks (MLP) have been applied to distinct areas, performing tasks such as function fitting and pattern recognition problems, by using the supervised training with an algorithm known as \xe2\x80\x9cerror back propagation\xe2\x80\x9d.   Radial basis function (RBF) networks have the advantages of an easy design (just three layer architecture), good generalization, and high tolerance of input noises and ability of online learning. From the point of generalization, RBF networks can respond well well to patterns that were not used for training.1.1, 1.2 of 230.pdf'	86	0	1	0
230	225	0	982	-1	b'You would probably have to pack recursive structures into finite-dimensional real vectors and there have been such attempts. The finite precision limits goes as far as the recursion can go.The limitation of feedforward neural networks is restricted to finite input and output spaces, so recurrent may be more suitable for this task as in theory can process arbitrarily long strings of numbers, but it has much more practical difficulties than feedforward network.These kind of methods are open to debate.Source: SAS FAQReferences:Blair, 1997; Pollack, 1990; Chalmers, 1990; Chrisman, 1991; Plate, 1994; Hammerton, 1998; Hadley, 1999'	93	0	0	0
232	197	0	24528	2	b"The concept of 'survival instinct' probably falls in the category of what Marvin Minsky would call a 'suitcase word', i.e. it packages together a number of related phenomena into what at first appears to be a singular notion. So it's quite possible that we can construct mechanisms that have the appearance of some kind of 'hard-coded' survival instinct, without that ever featuring as an explicit rule(s) in the design.See the beautiful little book 'Vehicles' by the neuroanatomist Valentino Braitenberg for a compelling narrative of how such 'top down' concepts as 'survival instinct' might evolve 'from the bottom up'.Also, trying to ensure that intelligent artefacts place too high a priority on their survival might easily lead to a Killbot Hellscape."	118	0	0	0
233	-1	0	0	7	b'According to my knowledge most of the current artificial intelligence study uses of some kind of neural network or its variants. A good example would be DeepMind\'s alphago which I believe is a deep neural network, for vision CNN, text, music and other ordered features RNN\'s, etc. But for machine learning application we have neural networks, support vector machines, random forest, regression methods, etc. available for applications. So are neural networks and its variants the only way to reach "true" artificial intelligence? '	82	0	0	0
234	197	0	25280	3	b'Steve Omohudro wrote a paper called Basic AI Drives that steps through why we would expect an AI with narrow goals to find some basic, general concepts as instrumentally useful for their narrow goals. For example, an AI designed to maximize stock market returns but whose design is silent on the importance of continuing to survive would realize that its continued survival is a key component of maximizing stock market returns, and thus take actions to keep itself operational.In general, we should be skeptical of \'anthropomorphizing\' AI and other code, but it seems like there are reasons to expect this beyond "well, humans behave this way, so it must be how all intelligence behaves."'	113	0	0	0
235	147	0	67793	3	b"I presume the proof the OP is referring to can be found in this monograph by Hava Siegelmann?In his article 'The Myth of Hypercomputation', the eminent computer scientist Martin Davis explains (p8-9) that there is nothing 'super Turing' about this formulation.EDIT: It's looking like the claim about rational weights being super-Turing is made in this more recent paper by Siegelmann, which introduces an additional assumption of plasticity, i.e. that weights can be dynamically updated."	73	0	3	0
236	233	1	1524	8	b'If by true AI, you mean \'like human beings\', the answer is - no-one knows what the appropriate computational mechanisms (neural or otherwise) are or indeed whether we are capable of constructing them.What Artificial Neural Nets (ANNs) do is essentially \'nonlinear regression\' - perhaps this is not a sufficiently strong model to express humanlike behaviour. Despite the \'Universal function approximation\' property of ANNs, what if human intelligence depends on some as-yet-unguessed mechanism of the physical world?With respect to your question about "the only way":Even if (physical) neural mechanisms somehow actually were the only route to intelligence (e.g. via Penrose\'s quantum microtubules), how could that be proved? Even in the formal world of mathematics, there\'s a saying that "Proofs of non-existence are hard". It scarcely seems conceivable that, in the physical world, it would be possible to demonstrate that intelligence could not arise by any other mechanism.Moving back to computational systems, note that Stephen Wolfram made the interesting observation in his book \'A New Kind of Science\' that many of the apparently distinct mechanisms he observed seem to be capable of \'Universal Computation\', so in that sense there\'s nothing very particular about ANNs.'	191	0	1	0
237	-1	0	0	3	b"I'm interested in hardware implementation of ANNs (artificial neural networks). Are there any popular existing technology implementations in form of microchips which are purpose designed to run artificial neural networks? For example, a chip which is optimised for an application like image recognition or something similar?"	45	0	0	0
238	159	1	63877	2	b"As far as I can see, there's no reason why you couldn't (for example) take the convolutional inputs to deepdream from adjacent sample points, rather than adjacent spatial positions, as is the case with image input.Given the 'self similar' nature of deep dream images, listening to this fractal granular synthesis technique might be of interest/inspiration."	54	0	0	0
239	233	0	4858	3	b'It depends on what you consider "true artificial intelligence". But this probably means to be able to think like a human - and perhaps, do so in a more rational manner, as in the human brain emotion comes before ratio. It would seem that a neural network, or a genetic algorithm that evolves neural networks, is the closest way - mimicking humans. However, the traditional counter-argument to this is that we tried to do the same with flight. We tried to copy nature, mimick the birds - trying to fly by flapping wings. But eventually we made airplanes that did not rely on flapping their wings. In AI, there are far more variables than in aerodynamics. So it is quite likely that a human-like intelligence can be attained by other methods than neural networks.In the end, neural networks are one approach to machine learning. There are others, all governed by the rules for what can and cannot be learnt. (There is a field called Computational Learning Theory that covers this). Although it is possible to extend learning systems beyond what can be learnt according to COLT, this means that such a learning system - neural network or otherwise - is essentially flawed, and will draw wrong conclusions at one point or another.'	211	0	0	0
240	-1	0	0	8	b"I've noticed that a few questions on this site mention genetic algorithms and it made me realize that I don't really know much about those.I have heard the term before, but it's not something I've ever used, so I don't have much idea about how they work and what they are good for. All I know is that they involve some sort of evolution and randomly changing values.Can you give me a short explanation, preferably including some sort of practical example that illustrates the basic principles?"	85	0	0	0
241	-1	0	0	3	b'In detective novels, the point is often that the reader gets enough information to solve the crime themselves. This "puzzle" aspect of detective novels is part of the attraction.Often the difficulty for humans is to keep track of all the variables - events, items, motivations.An AI would have an easier time keeping track of all the details, but would rely on real-world knowledge to prevent making crazy mistakes. For example, if it was stated that a character took the train, the AI would need to know that this is a method of transportation - that it changes the location property of an agent over time.Has an AI ever been able to solve a detective mystery?'	114	0	0	0
242	240	0	322	5	b'A genetic algorithm is an algorithm that randomly generates a number of attempted solutions for a problem. This set of attempted solutions is called the "population". It then tries to see how well these solutions solve the problem, using a given fitness function. The attempted solutions with the best fitness value are used to generate a new population. This can be done by making small changes to the attempted solutions (mutation) or by combining existing attempted solutions (crossover).The idea is that, over time, an attempted solution emerges that has a high enough fitness value to solve the problem.The inspiration for this came from the theory of evolution; the fittest solutions survive and procreate.Example 1Suppose you were looking for the most efficient way to cut a number of shapes out of a piece of wood. You want to waste as little wood as possible.Your attempted solutions would be random arrangements of these shapes on your piece of wood. Fitness would be determined by how little wood would be left after cutting the shapes following this arrangement.The less wood is left, the better the attempted solution. Example 2Suppose you were trying to find a polynomial that passes through a number of points. Your attempted solutions would be random polynomials.To determine the fitness of these polynomials, you determine how well they fit the given points. (In this particular case, you would probably use the least squares method to determine how well the polynomial fit the points).Over a number of trials, you would get polynomials that fit the points better, until you had a polynomial that fit the points closely enough.'	266	0	0	0
243	240	1	1197	5	b'Evolutionary algorithms are a family of optimization algorithms based on the principle of Darwinian natural selection. As part of natural selection, a given environment has a population of individuals that compete for survival and reproduction. The ability of each individual to achieve these goals determines their chance to have children, in other words to pass on their genes to the next generation of individuals, who for genetic reasons will have an increased chance of doing well, even better, in realizing these two objectives. This principle of continuous improvement over the generations is taken by evolutionary algorithms to optimize solutions to a problem. In the initial generation, a population composed of different individuals is generated randomly or by other methods. An individual is a solution to the problem, more or less good: the quality of the individual in regards to the problem is called fitness, which reflects the adequacy of the solution to the problem to be solved. The higher the fitness of an individual, the higher it is likely to pass some or all of its genotype to the individuals of the next generation.An individual is coded as a genotype, which can have any shape, such as a** bit vector (genetic algorithms) or a vector of real (evolution strategies). Each genotype is transformed into a phenotype when assessing the individual, i.e. when its fitness is calculated. In some cases, the phenotype is identical to the genotype: it is called direct coding. Otherwise, the coding is called indirect. For example, suppose you want to optimize the size of a rectangular parallelepiped defined by its length, height and width. To simplify the example, assume that these three quantities are integers between 0 and 15. We can then describe each of them using a 4-bit binary number. An example of a potential solution may be to genotype 0001 0111 01010. The corresponding phenotype is a parallelepiped of length 1, height 7 and width 10.During the transition from the old to the new generation are called variation operators, whose purpose is to manipulate individuals. There are two distinct types of variation operators:the mutation operators, which are used to introduce variations within the same individual, as genetic mutations;the crossover operators, which are used to cross at least two different genotypes, as genetic crosses from breeding.Evolutionary algorithms have proven themselves in various fields such as operations research, robotics, biology, nuance, or cryptography. In addition, they can optimize multiple objectives simultaneously and can be used as black boxes because they do not assume any properties in the mathematical model to optimize. Their only real limitation is the computational complexity.'	431	0	0	0
244	240	0	1283	2	b'As observed in another answer, all you need to apply Genetic Algorithms (GAs) is to represent a potential solution to your problem in a form that is subject to crossover and mutation. Ideally, the fitness function will provide some kind of smooth feedback about the quality of a solution, rather than simply being a \'Needle in a Haystack\'.Here are some characteristics of problems that Genetic Algorithms (and indeed Metaheuristics in general) are good for:NP-complete - The number of possible solutions to the problem isexponential, but checking the fitness of a solution is relativelycheap (technically, with time polynomial in the input size). Black box - GAs work reasonably well even if you don\'t have a particularlyinformed model of the problem to be solved. This means that theseapproaches are also useful as a \'rapid prototyping\' approach tosolving problems.However, despite their widespread use for the purpose, note that GAs are actually not function optimizers - GA mechanisms tend not to explore \'outlying\' regions of the search space in the hope of finding some distant high quality solution, but rather to cluster around more easily attainable peaks in the \'fitness landscape\'.More detail on the applicability of GAs is given in a famous early paper "What makes a problem hard for a Genetic Algorithm?"'	208	0	2	0
246	240	0	2187	3	b'This answer requests a practical example of how one might be used, which I will attempt to provide in addition to the other answers. They seem to due a very good job of explaining what a genetic algorithm is. So, this will give an example.Let\'s say you have a neural network (although they are not the only application of it), which, from some given inputs, will yield some outputs. A genetic algorithm can create a population of these, and by seeing which output is the best, breed and kill off members of the population. Eventually, this should optimise the neural network if it is complicated enough. Here is a demonstration I\'ve made, which despite being badly coded, might help you understand. Hit the evolve button and mess around with the goals.It uses a simple genetic algorithm to breed, mutate and decide between which of the population survive. Depending on how the input variables are set, the network will be able to get to some level of closeness to them.In this fashion, wthe population will likely eventually become a homogenenous group, whose outputs resemble the goals.The genetic algorithm is trying to create a "neural network" of sorts, that by taking in RGB, will yield an output color. First it generates a random population. It then by taking 3 random members from the population, selecting the one with the lowest fitness and removing it from the population. The fitness is equal to the difference in the top goal squared + the difference in the bottom goal squared. It then breeds the two remaining ones together and adds the child to the same place in the population as the dead member.When mating occurs, there is a chance a mutation will occur. This mutation will change one of the values randomly.As a side note, due to how it is set up, it is impossible for it to be totally correct in many cases, though it will reach relative closeness.'	324	0	1	0
247	-1	0	0	5	b'In 1969, Seymour Papert and Marvin Minsky showed that Perceptrons could not learn the XOR function. This was solved by the backpropagation network with at least one hidden layer. This type of network can learn the XOR function.I believe I was once taught that every problem that could be learnt by a backpropagation neural network with multiple hidden layers, could also be learnt by a backpropagation neural network with a single hidden layer. (Although possible a nonlinear activation function was required).However, it is unclear to me what the limits are to backpropagation neural networks themselves. Which patterns cannot be learnt by a backpropgation neural network?'	104	0	0	0
248	-1	0	0	6	b"Over the last 50 years, the rise/fall/rise in popularity of neural nets has acted as something of a 'barometer' for AI research.It's clear from the questions on this site that people are interested in applying Deep Learning (DL) to a wide variety of difficult problems.I therefore have two questions:Practitioners - What do you find to be the main obstacles toapplying DL 'out of the box' to your problem? Researchers - Whattechniques do you use (or have developed) that might help addresspractical issues? Are they within DL or do they offer analternative approach?"	91	0	0	0
249	-1	0	0	2	b'Is it possible for unsupervised learning to learn about high-level, class-specific features given only unlabelled images? For example detecting human or animal faces? If so, how?'	25	0	0	0
250	92	1	4410	21	b'First up, those images (even the first few) aren\'t complete trash despite being junk to humans; they\'re actually finely tuned with various advanced techniques, including another neural network. The deep neural network is the pre-trained network modeled on AlexNet provided by Caffe. To evolve images, both the directly encoded and indirectly encoded images, we use the Sferes evolutionary framework. The entire code base to conduct the evolutionary experiments can be download [sic] here. The code for the images produced by gradient ascent is available here.Images that are actually random junk were correctly recognized as nothing meaningful: In response to an unrecognizable image, the networks could have output a low confidence for each of the 1000 classes, instead of an extremely high confidence value for one of the classes. In fact, they do just that for randomly generated images (e.g. those in generation 0 of the evolutionary run)The original goal of the researchers was to use the neural networks to automatically generate images that look like the real things (by getting the recognizer\'s feedback and trying to change the image to get a more confident result), but they ended up creating the above art. Notice how even in the static-like images there are little splotches - usually near the center - which, it\'s fair to say, are triggering the recognition. We were not trying to produce adversarial, unrecognizable images. Instead, we were trying to produce recognizable images, but these unrecognizable images emerged.Evidently, these images had just the right distinguishing features to match what the AI looked for in pictures. The "paddle" image does have a paddle-like shape, the "bagel" is round and the right color, the "projector" image is a camera-lens-like thing, the "computer keyboard" is a bunch of rectangles (like the individual keys), and the "chainlink fence" legitimately looks like a chain-link fence to me. Figure 8. Evolving images to match DNN classes produces a tremendous diversity of images. Shown are images selected to showcase diversity from 5 evolutionary runs. The diversity suggests that the images are non-random, but that instead evolutions producing [sic] discriminative features of each target class.Further reading: the original paper (large PDF)'	355	0	1	0
253	237	1	8067	3	b'In May 2016 Google announced a custom ASIC which was is specifically built for machine learningwiki and tailored for TensorFlow. It is using tensor processing unit (TPU) which is a programmable microprocessor designed to accelerate artificial neural networks.NeuroCores, 12x14 sq-mm chips which can be interconnected in a binary tree, see: Neurogrid, a supercomputer which can provide an option for brain simulations.TrueNorth, a neuromorphic CMOS chip produced by IBM, which has 4096 cores in the current chip, each can simulate 256 programmable silicon "neurons", giving a total of over a million neurons.Further readings: Neuromorphic engineering, Vision processing unit, AI acceleratorsAs a side note, you can always use an FPGA based piece of hardware which you can implement selected genetic algorithm (GA) directly in hardware. For example the CoDi model was implemented in the FPGA based CAM-Brain Machine (CBM)2001.'	136	0	1	0
258	-1	0	0	1	b"On the Wikipedia page we can read the basic structure of an artificial neuron (a model of biological neurons) which consist:Dendrites - acts as the input vector,Soma - acts as the summation function,Axon - gets its signal from the summation behavior which occurs inside the soma.I've checked Deep learning wiki page, but I couldn't find any references to dendrites, soma or axons.So my question is, which type of artificial neural network implements or can mimic such model most closely?"	78	0	0	0
264	172	0	59387	0	b"It depends what you mean by 'develop themselves' - in a rather limited sense, an online machine learning approach such as Genetic Algorithms 'develops itself' to provide better solutions.There is already a theoretical model that represents the ultimate concept of development: Juergen Schmidhuber's Goedel Machine is constructed so as to self-modify when it can prove that this modification is optimal."	59	0	1	0
265	241	0	9548	0	b"Not exactly a detective mystery, but according to a slide dated June 2012 from a NSA PowerPoint presentation (see: Glenn Greenwald\xe2\x80\x99s site), NSA used some kind of Skynet AI technology to analyze and detect suspicious patterns from location and communication data in order to create a watch list of suspected terrorists. This helped to track associated members of Al-Qa\xe2\x80\x99ida as well as the Muslim Brotherhood. And I'm sure their AI solved a lot of mysteries and found some controversial figures.Source: U.S. Government Designated Prominent Al Jazeera Journalist as a member of AI QaedaFor more details check: SKYNET: Courier Detection via Machine Learning for courier detection data and charts generated by analyzing GSM metadata using machine learning algorithms. Also Applying Advanced Cloud-based Behavior Analytics."	122	0	0	0
266	247	0	10958	2	b"While I'm not familiar with any explicit statements regarding what a Multilayer Perceptron (MLP) cannot learn, I can provide some further detail on the positive statements you made about MLP capabilities:A MLP with a single hidden layer is capable of what is commonly termed 'Universal Function Approximation', i.e. it can approximate any bounded continuous function to an arbitrary degree of accuracy. With two hidden layers, the boundness restriction is removed [Cybenko, 1988].This paper goes on to demonstrate that this is true for a wide range of activation functions (not necessarily nonlinear). 3 layer MLPs are also capable of representing any boolean function (although they may require an exponential number of neurons).See also this interesting answer on CS SE about other Universal approximators."	121	0	3	0
267	157	1	177	3	b'The following post has a bit of math, which I hope helps to explain the problem better. Unfortunately it seems, this SE site does not support LaTex:Document summarization is very much an open problem in AI research. One way this task is currently handled is called "extractive summarization". The basic strategy is as follows: Split this document into sentences and we will present as a summary a subset of sentences which together cover all the important details in the post. Assign sentence i, 1&lt;=i&lt;=n, a variable z_i \\in {0,1}, where z_i = 1 indicates the sentence was selected and z_i = 0 means the sentence was left out. Then, z_i z_j = 1 if and only if both sentences were chosen. We will also define the importance of each sentence w_i for sentence i and interaction terms w_{ij} between sentences i and j. Let x_i be the feature vectors for sentence i. w_i = w(x_i) captures how important it is to include this sentence (or the topics covered by it) while w_ij = w(x_i,x_j) indicates the amount of overlap between sentences in our summary. Finally we put all this in a minimization problem:maximize_{z_i} \\sum_{i} w_i z_i - w_{ij} z_i z_j s.t. z_i = 0 or 1This tries to maximize the total weight of the sentences covered and tries to minimize the amount of overlap. This is an integer programming problem similar to finding the lowest weight independent set in a graph and many techniques exist to solve such problems.This design, in my opinion, captures the fundamental problems in text summarization and can be extended in many ways. We will discuss those in a bit, but first we need to completely specify the features w. w_i = w(x_i) could be a function only of the sentence i, but it could also depends on the place of the sentence in the document or its context (Is the sentence at the beginning of a paragraph? Does it share common words with the title? What is its length? Does it mention any proper nouns? etc)w_ij = w(x_i,x_j) is a similarity measure. It measures how much repetition there will be if we include both words in the sentence. It can be defined by looking at common words between sentences. We can also extract topics or concepts from each sentence and see how many are common between them, and use language features like pronouns to see if one sentence expands on another.To improve the design, first, we could do keyphrase extraction, i.e. identify key phrases in the text and choose to define the above problem in terms of those instead of trying to pick sentences. That is a similar problem to what Google does to summarize news articles in their search results, but I am not aware of the details of their approach. We could also break the sentences up further into concepts and try to establish the semantic meaning of the sentences ( Ponzo and Fila are people P1 and P2, a mall is a place P, P1 and P2 went to the place P at time T (day). Mode of transport walking.... and so on). To do this, we would need to use a semantic ontology or other common-sense knowledge database. However, all the parts of this last semantic classification problem are open and I have not seen anyone make satisfactory progress on it yet. We could also tweak the loss function above so that instead of the setting the tradeoff between the sentence importance w_i and the diversity score w_ij by hand, we could learn it from data. One way to do this is to use Conditional Random Fields to model the data, but many others surely exist.I hope this answer explained the basic problems that need to be solved to make progress towards good summarization systems. This is an active field of research and you will find the most recent papers via Google Scholar, but first read the Wikipedia page to learn the relevant terms'	662	0	0	0
268	258	1	14705	2	b'ANN research does not try to model biological neurons, as the aim is to achieve better performance at prediction tasks. However, there is a body of literature in neuroscience that looks at Computational models of neurons. Neurons are complicated cells and our understanding of neurons is still not complete. '	49	0	0	0
271	92	0	22826	7	b'An important question that does not yet have a satisfactory answer in neural network research is how DNNs come up with the predictions they offer. DNNs effectively work (though not exactly) by matching patches in the images to a "dictionary" of patches, one stored in each neuron (see the youtube cat paper). Thus, it may not have a high level view of the image since it only looks at patches, and images are usually downscaled to much lower resolution to obtain the results in current generation systems. Methods which look at how the components of the image interact may be able to avoid these problems.Some questions to ask for this work are: How confident were the networks when they made these predictions? How much volume do such adversarial images occupy in the space of all images?Some work I am aware of in this regard comes from Dhruv Batra and Devi Parikh\'s Lab at Virginia Tech who look into this for question answering systems: Analyzing the Behavior of Visual Question Answering Models and Interpreting Visual Question Answering models. More such work is needed, and just as the human visual system does also get fooled by such "optical illusions", these problems may be unavoidable if we use DNNs, though AFAIK nothing is yet known either way, theoretically or empirically. '	217	0	2	0
272	233	0	32443	2	b'To have any chance at answering this, you\'d first need a rigorous definition of "true artificial intelligence", which we don\'t have. And even if you had that, the best answer would probably be "nobody knows." We don\'t even understand exactly how human intelligence (which is probably the best model of intelligence we have available to study) works. What we do know (or think we know) is that ANN\'s are at best a very superficial replica of brain function. It may turn out that they\'re absolutely the wrong path to achieving "true artificial intelligence" although I expect most people would be surprised if that turned out to be the case.What probably wouldn\'t be so surprising would be if some other technique emerged which is better than ANN\'s, OR if it turns out that you need an ensemble of techniques. Personally I think it\'s close to self-evident that the brain works largely in a probabilistic fashion, but it\'s also clear that we do sometimes use symbolic processing / deductive logic / rules / etc. And right now, ANN\'s don\'t give you much in the way of reasoning, deduction, etc. So we may ultimately find that we have to combine a probabilistic approach like ANN\'s with other techniques - maybe Inductive Logic Programming or something of that nature. '	215	0	0	0
1274	-1	0	0	-1	b'Have there been any studies which attempted to use AI algorithms to detect human thoughts or emotions based on brain activity, such as using BCI/EEG devices?By this, I mean simple guesses such as whether the person was happy or angry, or what object (e.g. banana, car) they were thinking about.If so, did any of those studies show some degree of success?'	60	0	0	0
1285	-1	0	0	7	b'Has there been any attempts to deploy AI with blockchain technology? Are there any decentralized examples of AI networks with no central point of control with AI nodes acting independently (but according to a codified set of rules) creating, validating and storing the same shared decentralized database in many locations around the world?'	52	0	0	0
1286	249	1	43978	1	b'I am assuming each image contains a single object.It is possible, however, it is not as easy as you might think. Firstly, you need extract as many features as possible: original image, LBP, SIFT, moments, contour descriptors to name a few. Than concatenate these features into a single feature vector. After this step, use clustering. You will need a lot of samples to compensate for the number of features. After clustering, use a correlation method to find which features are related to each cluster.If you need features to classify within a cluster, you could do a second clustering with full set of features and apply the same method. The features that are selected for a cluster will not be suitable to classify within the cluster.'	124	0	0	0
1287	227	1	54633	5	b'MLP: uses dot products (between inputs and weights) and sigmoidal activation functions (or other monotonic functions such as ReLU) and training is usually done through backpropagation for all layers (which can be as many as you want). This type of neural network is used in deep learning with the help of many techniques (such as dropout or batch normalization);RBF: uses Euclidean distances (between inputs and weights, which can be viewed as centers) and (usually) Gaussian activation functions (which could be multivariate), which makes neurons more locally sensitive. Thus, RBF neurons have maximum activation when the center/weights are equal to the inputs (look at the image below). Due to this property, RBF neural networks are good for novelty detection (if each neuron is centered on a training example, inputs far away from all neurons constitute novel patterns) but not so good at extrapolation. Also, RBFs may use backpropagation for learning, or hybrid approaches with unsupervised learning in the hidden layer (they usually have just 1 hidden layer). Finally, RBFs make it easier to grow new neurons during training.'	176	0	1	0
1288	-1	0	0	2	b'In their famous book entitled "Perceptrons: An Introduction to Computational Geometry", Minsky and Papert show that a perceptron can\'t solve the XOR problem. This contributed to the first AI winter, resulting in funding cuts for neural networks. However, now we know that a multilayer perceptron can solve the XOR problem easily.Backprop wasn\'t known at the time, but did they know about manually building multilayer perceptrons? Did Minsky &amp; Papert know that multilayer perceptrons could solve XOR at the time they wrote the book, albeit not knowing how to train it?'	89	0	0	0
1289	-1	0	0	1	b'According to wikipedia Artificial general intelligence(AGI) Artificial general intelligence (AGI) is the intelligence of a (hypothetical) machine that could successfully perform any intellectual task that a human being can. According to below image todays artifical intellgence is same as that of a lizards.Lets assume(or not) that within 10-20 years we humans are successful in creating a AGI or AGIs. As AGI has the same intelligence and emotions as that of humans because according to wikipedia definition it can perform same intellectual task of a human. Then can we destroy an AGI without its consent? Do this be considered as murder?'	99	0	1	0
1290	-1	0	0	3	b'Deep Mind has published a lot of works on deep learning in the last years, most of them state-of-the-art on their respective tasks. But how much of this work has actually been reproduced by the AI community? For instance, the Neural Turing Machine paper seems to be very hard to reproduce, according to other researchers.'	54	0	0	0
1292	1290	0	900	3	b'I tend to think this question is border-line and may get close. A few comments for now, though.There are (at least) two issues with reproducing the work of a company like DeepMind:Technicalities missing from publications.Access to the same level of data.Technicalities should be workable. Some people have reproduced some of the Atari gaming stunts. AlphaGo is seemingly more complex and will require more work, yet that should be feasible at some point in the future (individuals may lack computing resources today).Data can be more tricky. Several companies open their data sets, but data is also the nerve of the competition...'	99	0	0	0
1293	1289	0	1512	4	b'Firstly, an AGI could conceivably exhibit all of the observable properties of intelligence without being conscious. Although that may seem counter-intuitive, at present we have no physical theory that allows us to detect consciousness (philosophically speaking, a \'Zombie\' is indistinguishable from a non-Zombie - see the writing of Daniel Dennett and David Chalmers for more on this). Destroying a non-conscious entity has the same moral cost as destroying a chair.Also, note that \'destroy\' doesn\'t necessarily mean the same for entities with persistent substrate, i.e. their \'brain state\' can be reversibly serialized to some other storage medium and/or multiple copies of them can co-exist. So if by \'destroy\' we simply mean \'switch off\', then an AGI might conceivably be reassured of a subsequent re-awakening. Douglas Hofstadter gives an interesting description of such an \'episodic consciousness\' in "A Conversation with Einstein\'s Brain"If by \'destroy\', we mean \'irrevocably erase with no chance of re-awakening\', then (unless we have a physical test which proves it is not conscious) destroying an entity with a seemingly human-level awareness is clearly morally tantamount to murder. To believe otherwise would be substrate-ist - a moral stance which may one day be seen as antiquated as racism.'	197	0	4	0
1294	-1	0	0	4	b'Geoffrey Hinton has been researching something he calls "capsules theory" in neural networks. What is this and how does it work?'	20	0	0	0
1295	-1	0	0	7	b'During my research, I\'ve stumbled upon "complex-valued neural networks", which are neural networks that work with complex-valued inputs (probably weights too). What are the advantages (or simply the applications) of this kind of neural network over real-valued neural networks?'	38	0	0	0
1296	-1	0	0	2	b'The author claims that guiding evolution by novelty alone (without explicit goals) can solve problems even better than using explicit goals. In other words, using a novelty measure as a fitness function for a genetic algorithm works better than a goal-directed fitness function. How is that possible?'	46	0	1	0
1297	-1	0	0	2	b'Quote from this Eric\'s meta post about modelling and implementation: They are not exactly the same, although strongly related. This was a very difficult lesson to learn among mathematicians and early programmers, notably in the 70s (mathematical proofs can demand a lot of non-trivial programming work to make them "computable", as in runnable on a computer).If they\'re not the same, what is the difference?How we can say when we\'re talking about AI implementation, and when about modelling? It\'s suggested above it\'s not easy task. So where we can draw the line when we talk about it?I\'m asking in general, not specifically for this site, that\'s why I haven\'t posted question in meta'	111	0	1	0
1298	241	0	59279	3	b"I might be wrong, but I do not believe that something of the scope you describe would be possible with the current state of technology. It would require a lot of things which are still in relatively early stages of research.For one, just extracting relevant information from text is a huge task by itself. Doubly so with a novel which contains a large amount of unimportant details.It might perhaps be easier if the input was presented in the form of some sort of list of important facts. But it would still be rather difficult for the AI to connect them and find a solution.As an example, let's say that we have these two facts:Alice died of a snake bite.Bob was seen buying a couple of mice recently.To a human, it seems obvious that the mice were bought to feed a venomous snake. However, it would probably require a tremendous effort to teach an AI to make such connections.Disclaimer: I don't do text processing myself, so I'm not quite up-to-date on the current state-of-the-art. It's possible that some of these things have already been done in some form. If anyone knows more about the subject, please correct me if I'm wrong."	199	0	0	0
1299	1295	1	6983	6	b'According to this paper, complex-valued ANNs (C-ANNs) can solve problems such as XOR and symmetry detection with a smaller number of layers than real ANNs (for both of these a 2 layer C-ANN suffices, whereas a 3-layer R-ANN is required).I believe that it is still an open question as to how useful this result is in practice (e.g. whether it actually makes finding the right topology easier), so at present the key practical advantage of C-ANNs is when they are a closer model for the problem domain.Application areas are then where complex values arise naturally, e.g. in optics, signal processing/FFT or electrical engineering.'	102	0	1	0
1300	1296	0	9052	1	b"As explained in an answer to this AI SE question, GAs are 'satisficers' rather than 'optimizers' and tend not to explore 'outlying' regions of the search space. Rather, the population tends to cluster in regions that are 'fairly good' according to the fitness function.In contrast, I believe the thinking is that novelty affords a kind of dynamic fitness, tending to push the population away from previously discovered areas."	67	0	1	0
1301	-1	0	0	1	b"Given pictures with multiple features such as faces, can single AI algorithm detect all of them, or for better reliability is it preferred to use separate instances?In other words I'm talking about attempt of finding all possible human faces on the same picture by a single neural network."	47	0	0	0
1303	-1	0	0	1	b"I read some information1 about attempts to build neural networks in the PHP programming language. Personally I think PHP is not the right language to do so at all probably because it's a high-level language, I assume low level language are way more suitable for AI in terms of performance and scalability. Is there a good/logical reason why you should or shouldn't use PHP as a language to write AI in?1  and  "	74	0	2	0
1305	1303	1	422	5	b"Question on-topicness questionable, but...The most logical reason why PHP is unsuited for neural networks is that PHP is, well, intended to be used for server side webpages. It can connect to various external resources, such as databases, via native language features. It is very much a glue language, and not a processing language. PHP is also mostly stateless, only allowing you to store state in either clients, file storage or databases.As such, it's not suitable for this sort of thing - not because PHP is a high level language, but rather because it's so request based and focused towards creating pages to serve to clients.That won't stop people from trying, though - there are various esoteric programming languages out there in which regular programming would be an insane task or not possible at all - but from a ease of development perspective, making a neural network in PHP makes no sense."	150	0	0	0
1306	-1	0	0	1	b"I've found this old scientific paper from 1988 about introduction of AI into nuclear power fields.Were or still are there any dangers by application of such algorithm? Are nuclear power plants or human life in risk if the algorithm will fail?Especially applications to the core, like cooling systems and other components which can be affected in negative way."	57	0	1	0
1307	1303	0	1336	2	b"Actually, yes. Remember, that due to the history of PHP development, some very good things has formed what we have now:From a simple/laggy/limited interpreter in PHP 3, we have now three mainstream lines coming one-by-one like v5/v6/v7 with full bytecode supported. In PHP v7 you don't even need a bytecode cache due to HHVM, old Zend VM is a hell-good-debugged and using a cacher like XCache you can achieve a true native execution speed and payloadThe PHP language interface allows any external C/C++ library just to be added as a module via very simple wrapper that can be written by the person that just red Kerrigan&amp;Richie and Straustrup base books on C and C++. This is amazing feature, exclusive to PHP as far as I knowIn PHP v7 you're welcome to use native multi-threading and even CUDA-based things, if you wish to do it. I did it, so I can confirm that it works"	153	0	0	0
1308	-1	0	0	1	b"How likely AI can fully replace pilots on commercial flights (including take off, landing and parking)?Since we've self-driving cars already, is it likely to happen to commercial planes as well?"	29	0	0	0
1310	241	0	72946	2	b'Generally agree with @Inquisitive Lurker, but I think we also have a wide range of potential abilities/requirements. As with computer chess or Go, where there\'s a big difference between "beating an honest novice human " and "beating all humans"; there\'s a big difference between solving a simple kids\' mystery and a complex adult novel.So I don\'t think there would be any problem writing a program that could solve a problem that is listed as a list of statements, or laid out as a (very young) children\'s book. However something like an Agatha Christie or John Le Carre\'s "Tinker Tailor Soldier Spy" (relatively simple solution, but the story is told in a complex manner) are far in the future.Sometimes an alternative approach might work. For example a neural network could probably solve all Colombo mysteries at the "Who did it?" level without a full "Why?" explanation, after only reading a few Colombo mysteries. The same is true for most kids!'	158	0	0	0
1311	1274	0	44452	5	b"As per this site Researchers recorded the complex patterns of electrical activity generated by someone\xe2\x80\x99s brain, as the subject listened to someone talking. By feeding those brainwave patterns into a computer, they were able to translate them back into actual words \xe2\x80\x94 the same words that the volunteer had been hearing.  The scientists behind the work believe they can now go further and read the unspoken thoughts of people using electrodes placed against the brain.  In the experiment, each patient listened to a recording of spoken words for five to ten minutes, while the net of electrodes placed under their skull monitored activity in a part of the brain involved in understanding speech called Wernicke\xe2\x80\x99s area.  In one experiment, volunteers looked at black-and-white photographs while the scanner monitored activity in part of the brain that handles vision called the primary visual cortex. A computer predicted accurately the image that the person was looking at purely from the pattern of brain activity.So AI might be able to read our emotions as well in near future.I found that google glasses can detect people's emotion via facial expression, voice tone e.t.c, (just like us), obviously not what they are thinking in their brain."	202	0	2	0
1312	1306	1	5052	2	b"Any technology in the nuclear industry represents variance--it may be an improvement in safety or efficiency, or it may contain some unseen defect that allows a catastrophe to happen.But the simple possibility of harm isn't enough to swing the decision one way or the other. The application of AI methods--whether to the real-time control of plant variables, or the early detection of problems, or to the design of plants and their components--seems likely to be as beneficial as in other realms.For example, check out the publication list of a lab active in this area. Their paper I'm most familiar with is one in which they build a fault detector paired with a fault library classifier, so that the operators can be alerted not just that something is abnormal but what fault has probably occurred. This is done in such a way that standardized plants (such as, say, the French nuclear system) can share records with each other, meaning that any plant has the experience of every plant at their fingertips."	169	0	1	0
1313	1294	1	20931	6	b"It appears to not be published yet; the best available online are these slides for this talk. (Several people reference an earlier talk with this link, but sadly it's broken at time of writing this answer.)My impression is that it's an attempt to formalize and abstract the creation of subnetworks inside a neural network. That is, if you look at a standard neural network, layers are fully connected (that is, every neuron in layer 1 has access to every layer in neuron 0, and is itself accessed by every neuron in layer 2). But this isn't obviously useful; one might instead have, say, n parallel stacks of layers (the 'capsules') that each specializes on some separate task (which may itself require more than one layer to complete successfully).If I'm imagining its results correctly, this more sophisticated graph topology seems like something that could easily increase both the effectiveness and the interpretability of the resulting network."	154	0	2	0
1314	-1	0	0	4	b"How much processing power is needed to emulate the human brain? More specifically, the neural simulation, such as communication between the neurons and processing certain data in real-time.I understand that this may be a bit of speculation and it's not possible to be accurate, but I'm sure there is some data available or research studies which attempted to estimate it based on our current understanding of the human brain."	68	0	0	0
1315	1285	1	33393	6	b"Swarm intelligence is the term for systems where relatively simple agents work together to solve a complicated problem in a decentralized fashion.In general, distributed computing methods are very important for dealing with problems at scale, and many of them embrace decentralization in a deep way. (Given the reality of hardware failure and the massive size of modern datasets relative to individual nodes, the less work is passed through a central bottleneck, the better.) While there are people interested in doing computation on the blockchain, it seems to me like it's unlikely to be competitive with computation in dedicated clusters (like AWS)."	100	0	0	0
1316	1314	1	1025	5	b"H+ magazine wrote an estimate in 2009 that seems broadly comparable to other things I've seen; they think the human brain is approximately 37 petaflops. A supercomputer larger than that 37 petaflop estimate exists today.But emulation is hard. See this SO question about hardware emulation or this article on emulating the SNES, in which they require 140 times the processing power of the SNES chip to get it right. This 2013 article claims that a second of human brain activity took 40 minutes to emulate on a 10 petaflop computer (a 2400-times slowdown, not the 4-times slowdown one might naively expect).And all this assumes that neurons are relatively simple objects! It could be that the amount of math we have to do to model a single neuron is actually much more than the flops estimate above. Or it could be the case that dramatic simplifications can be made, and if we knew what the brain was actually trying to accomplish we could do it much more cleanly and simply. (One advantage that ANNs have, for example, is that they are doing computations with much more precision than we expect biological neurons to have. But this means emulation is harder, not easier, while replacement is easier.)"	204	0	4	0
1317	1314	0	1071	2	b"Not just how much, but what kind of processing power : there're specially-crafted dedicated chips, and it has a practical applications, so it's not a lab-only project"	26	0	2	0
1318	-1	0	0	1	b"The Situation:A self-driving car is traveling at it's maximum speed, 25 mph (40 km/h), in the middle of an empty street with the ability to change lanes on both sides. There are two passengers, one in the front and another in the back.Someone jumps from the side of the road directly into the path of the car. A collision would occur in 50 meters. Breaking distance at this speed is about 24m.The Question: Is it known how the current implementation of the Google Car AI would react, or is it currently a matter of speculation? A step-by-step explanation of the AI's decisioning process would be preferred.Possible Answers: The car could activate its brakes immediately, coming to a halt as quickly as possible. This would be sooner than a human could stop, as people require time to recognize the possibility of a collision, and then physically slam on the brake. (thinking distance).Alternatively, the car could continue traveling forward, processing the situation. (Similar to a humans thinking distance). The person may continue to move, either out of the way, or still into danger of being hit. In this case, the car may decide to change lanes in an attempt to pass around the person.Lastly and most unlikely, the car will not alter its course and proceed to drive forward.Do not attempt to do it to check;)"	223	0	1	0
1319	70	0	81789	3	b'The simple answer is "no, they aren\'t limited to images": CNNs are also being used for natural language processing. (See here for an introduction.)I haven\'t seen them applied to graphical data yet, but I haven\'t looked; there are some obvious things to try and so I\'m optimistic that it would work.'	50	0	1	0
1320	-1	0	0	1	b'Artificial intelligence is present in many games, both current and older games. How can such intelligence understand what to do? I mean, how can it behave like a human in a game, allowing you to play against itself, or that AI plays against itself?In games like Age of Empires, for example.'	50	0	0	0
1321	1320	0	449	2	b"Most of the existing AI bots which can play games use deep search from possible space and choose the best move. This is done by most of the chess, Go, Tic-Tac-Toe, etc bots.However, there has been a recent breakthrough where (deep)neural nets with deep search techniques like monte-carlo search, etc; which might be more human-like and demonstrate a much more complex game behaviour than the above bots. One such example is the Google's Alpha-Go bot."	74	0	0	0
1322	240	0	80764	3	b'There are a number of good answers here explaining what genetic algorithms are, and giving example applications. I\'m adding some general purpose advice on what they are good for, but also cases where you should NOT use them. If my tone seems harsh, it is because using GAs in any of the cases in the Not Appropriate section will lead to your paper being instantly rejected from any top-tier journal. First, your problem MUST be an optimization problem. You need to define a "fitness function" that you are trying to optimize and you need to have a way to measure it.Good: Crossover functions are easy to define and natural: When dealing with certain kinds of data, crossover/mutation functions might be easy to define. For example strings (eg. DNA or gene sequences) can be mutated easily by splicing two candidate strings to obtain a new one (this is why nature uses genetic algorithms!). Trees (like phylogenetic trees or parse trees) can be spliced too, by replacing a branch of one tree with a branch from another. Shapes (like airplane wings or boat shapes) can be mutated easily by drawing a grid on the shape and combining different grid sections from the parents to obtain a child. Usually this means your problem is composed of different parts and putting together parts from distinct solutions is a valid candidate solution.This means that if your problem is defined in a vector space where the coordinates don\'t have any special meaning, GAs are not a good choice. If it is hard to formulate your problem as a GA, it is not worth it.Black Box evaluation: If for a candidate, your fitness function is evaluated outside the computer, GAs are a good idea. For example, if you are testing a wing shape in an air tunnel, genetic algorithms will help you generate good candidate shapes to try. Exception: Simulations. If your fitness function is measuring how well a nozzle design performs and requires simulating the fluid dynamics for each nozzle shape, GAs may work well for you. They may also work if you are simulating a physical system through time and are interested in how well your design performs over the course of the operation eg. modelling locomotion patterns. However, methods that use partial differential equations as constraints are being developed in the literature, eg. PDE constrained optimization, so this may change in the future.Not Appropriate:You can calculate a gradient for your function: If you have access to the gradient of your function, you can do gradient descent, which is in general much more efficient than GAs. Gradient descent may have issues with local minima (as will GAs) but many methods have been studied to mitigate this. You know the fitness function in closed form: Then, you can probably calculate the gradient. Many languages have libraries supporting automatic differentiation, so you don\'t even need to do it manually. If your function is not differentiable, then you can use subgradient descent.Your optimization problem is of a known form, like a linear program or a quadratic program: GAs (and black box optimization methods in general) are very inefficient in terms of the number of candidates they need to evaluate, and are best avoided if possible. Your solution space is small: If you can grid your search space efficiently, you can guarantee that you have found the best solution, and can make contour plots of the solution space to see if there is a region you need to explore further.Finally, if you are considering a GA, consider more recent work in Evolutionary Strategies. I am biased towards CMA-ES, which I think is a good simple algorithm that captures the notion of a gradient in the fitness landscape in a way that traditional GAs do not.'	625	0	0	0
1323	-1	0	0	2	b'At a related question in Computer Science SE, a user told: Neural networks typically require a large training set.Is there a way to define the boundaries of the "optimal" size of a training set in general case?When I was learning about fuzzy logic, I\'ve heard some rules of thumb that involved examining the mathematical composition of the problem and using that to define the number of fuzzy sets.Is there such a method that can be applicable for an already defined neural network topology? '	83	0	1	0
1324	1320	1	2346	5	b"There are many different kinds of AI used in games; AI for historical board games (like chess or Go) tends to be much better than AI for computer games (such as Starcraft or Civilization), in large part because there's more academic interest in developing strategies for those games.The basic structure of a game-playing AI is that it takes in game state inputs and outputs an action; typically, the internals also contain some sort of goal and some sort of future prediction.But beyond that, there's tremendous amounts of variability. Some AI are little more than scripted reflexes, some are built like control systems, some do actual optimization and forward thinking.Getting into the details of how the many different approaches work is probably beyond the scope of this site, though."	127	0	0	0
1325	1323	1	1224	3	b"For a finite value to be 'optimal,' typically you need some benefit from more paired up with some cost for more, and eventually the lines cross because the benefit decreases and the cost increases.Most models will have a reduction in error with more training data, that asymptotically approaches the best the model can do. See this image (from here) as an example:The costs of training data are also somewhat obvious; data is costly to obtain, to store, and to move. (Assuming model complexity stays constant, the actual cost of storing, moving, and using the model remains the same, since the weights in the model are just being tuned.)So at some point the slope of the error-reduction curve becomes horizontal enough that more data points are costlier than they're worth, and that's the optimal amount of training data."	136	0	1	0
1326	1323	0	1584	2	b'In general, the larger the training set, the better. See The Unreasonable effectiveness of Data, though this article is quite dated (written in 2009). Xavier Amatriain, a researcher at Netflix has a Quora answer where he discusses that more data can sometimes hurt algorithms. For deep neural networks in particular, it does not seem that we have hit these limits yet. '	61	0	1	0
1327	1297	1	25307	4	b"One good way of differentiating modelling and implementation is to consider that models occupy a much higher level of abstraction. To continue with the mathematical example: even though experimental mathematics might be dependent on computation, the program can be considered as one possible realization of the necessary conditions of a more abstract existence proof.Over the last 25 years, software engineering methodologies have become quite good at separating models and implementations, e.g. by using interfaces/typeclasses/abstract base classes to define constraints on behavior that is concretely realized by the implementation of derived classes.AI has always been a battle between the 'neats and the scruffies'. Neats tend to prefer working 'top down' from clean abstractions, 'scruffies' like to work 'bottom up', and 'bang the bits' of the implementation together, to see what happens.Of course, in practice, interplay between both styles is necessary, but AI as a science progresses when we abstract mechanisms away from specific implementations into their most general (and hence re-useable) form."	160	0	0	0
1328	1314	0	10665	5	b'Human brain contains about 100 billions neurons (10^11) and about hundred trillions synapses ($10^14). Each neuron can fire about 100 times a second. If we model brain as a simple neural network, then it would be equivalent to machine that requires 1016 calculations per second and 1013 bits of memory.From Wikipedia Kurzweil introduces the idea of "uploading" a specific brain with every mental process intact, to be instantiated on a "suitably powerful computational substrate". He writes that general modeling requires 1016 calculations per second and 1013 bits of memory, but then explains uploading requires additional detail, perhaps as many as 1019 cps and 1018 bits. Kurzweil says the technology to do this will be available by 2040.According to this two site here: Using the NEST software framework, the team led by Markus Diesmann and Abigail Morrison succeeded in creating an artificial neural network of 1.73 billion nerve cells connected by 10.4 trillion synapses. While impressive, this is only a fraction of the neurons every human brain contains. Scientists believe we all carry 80-100 billion nerve cells  It took 40 minutes with the combined muscle of 82,944 processors in K computer to get just 1 second of biological brain processing time. While running, the simulation ate up about 1PB of system memory as each synapse was modeled individually.Computing power will continue to ramp up while transistors scale down, which could make true neural simulations possible in real time with supercomputers.SpiNNaker is a manycore computer architecture designed to simulate the human brain. It is planned to use 1 million ARM processors (currently .5 million). The completed design will holds 100,000 coresIn this video they showed a completed rack with 100,000 cores emulating 25 million neurons (at \xc2\xbc the efficiency\xe2\x80\x94it will eventually run 1,000 neurons per core). '	295	0	1	0
1329	64	0	3888	3	b'What might be classed as AI has of course changed over the years, but landmarks and research breakthroughs include:Babbagge\'s Difference Engine (~1823) for tabulating/interpolating polynomials.Frank Rosenblatt\'s 1957 invention of the Perceptron.John McCarthy\'s invention of Lisp in the late 1950s.Arthur Samuel\'s 1959 checkers player, which famously improved by playing against itself (it would have been nice if that had destroyed the myth about a program only being as \'intelligent\' as its creator).Newell and Simon\'s 1959 General Problem Solver applied Means-Ends analysis to solve a range of problems expressed as Horn clauses.Davis, Putnam et al: 1962 invention of the DPLL algorithm which still forms the core of modern SAT-based theorem provers.Lawrence Fogel et al: 1966 book Artificial Intelligence through Simulated Evolution.Rechenberg and Schwefel: 1960s development of Evolutionsstrategie - an Evolutionary Computation approach using mutation and a form of Darwinian \'survival of the fittest\'.Lotfi Zadeh\'s 1965 invention of Fuzzy Logic.John Holland\'s 1975 book "Adaptation in Natural and Artificial Systems" which introduced Genetic Algorithms.The 1980 Hearsay II Blackboard Architecture by Hayes-Roth et al.The 1980s invention of the backpropagation algorithm for Mutlilayer Perceptrons by Rumelhart, Hinton et al.'	182	0	7	0
1332	-1	0	0	4	b'How important is true (non-pseudo) randomness in Artificial Intelligence designs? Is there any chance that pseudo-randomness could be a barrier to more successful designs?'	23	0	0	0
1333	-1	0	0	0	b'Complex AI that learns lexical-semantic content and its meaning (such as collection of words, their structure and dependencies) such as Watson takes terabytes of disk space.Lets assume DeepQA-like AI consumed whole Wikipedia of size 10G which took the same amount of structured and unstructured stored content.Will learning another 10G of different encyclopedia (different topics in the same language) take the same amount of data? Or will the AI reuse the existing structured and take less than half (like 1/10 of it) additional space?'	82	0	0	0
1334	-1	0	0	2	b'Is there any simple explanation how Watson finds and scores evidence after gathering massive evidence and analyzing the data?In other words, how does it know which precise answer it needs to return?'	31	0	0	0
1335	1332	0	1816	4	b"Randomness is typically the best one can do with ignorance, rather than a source of strength in its own right.For example, the primary use of randomness in statistics is random assignment (A/B testing, randomized controlled trials, etc.). The reason to do this is to make the influence of confounders independent from the influence of the factor under investigation.But randomness only works for this in expectation. If we actually knew what the confounders were, we could do a paired assignment (or a similar scheme) that ensured the various groups were matched as well as possible, instead of us just not knowing ahead of time which way the bias went.There are some cases where pseudorandomness, rather than full randomness, will impair training AI designs. A simple example would be a case where you want to randomly initialize weights in a network where the number of parameters exceeds the periodicity of the RNG; this means that while you have as many possible networks as there are possible unique seeds, you can't actually visit the entire weight space that you wanted to sample over.I don't think any of those cases are limiting factors, however. Having truly random stochastic gradient descent instead of pseudorandom stochastic gradient descent doesn't seem like it would make a serious difference in the trajectory of AI designs."	216	0	0	0
1337	1301	1	26740	3	b'AFAIK, normally detection algorithms work in a sub-window of the image and not the whole of it. For example, for a specific size and orientation you slide a sub-window on the image and extract sub-images. Then you apply your algorithm on every sub-image for detection and report the size-and-orientations with positive results.You can have a single neural network for face detection in this case or you might want to have different detectors for different orientation or any other feature, that is your decision.There is also the technique of Combining Classifiers by which you can improve the decision of single classifiers by combining them.Ensemble Learning is another way in which your classifiers are not trained independently but rather together. In fact, the well-known object detector of Viola and Jones uses such a technique.'	131	0	0	0
1338	36	0	14773	4	b"Until we can make a quantum computer with a lot more qubits, the potential to further develop AI will remain just that.D-Wave (which currently has a qubit count of over 1,000) is an adiabatic quantum computer, not a general purpose quantum computer. It is restricted to certain optimization problems (at which its effectiveness has reportedly been doubted by one of the originators of the theory on which it is based).Suppose that we could build a 32 qubit general purpose quantum computer (twice as big as current models, as far as I'm aware). This would still mean that only 232 possibilities exist in superposition. This is a space small enough to be explored exhaustively for many problems. Hence, there are perhaps not so many problems for which any of the known quantum algorithms (e.g. Shor, Grover) would be useful for that number of bits."	142	0	0	0
1339	1274	0	66976	4	b"There has been previous research with promising results cited at length in the following recent article, and although they have limited training data, here is some impressive research for an undergraduate thesis at the University of Arkansas which extends that research using an artificial neural network on enhancing a classifying algorithm's capacity to facilitate unspoken, or imagined, speech recognition by collecting and analyzing a large dataset of simultaneous EEG signal and video data streams.  Imagined speech (unspoken speech, silent speech, or covert speech) is the process by which one thinks about a word, or \xe2\x80\x9chears\xe2\x80\x9d the word in one\xe2\x80\x99s head, in the absence of any vocalization or physical movement indicating the word. Though there exists evidence that it is possible for imagined speech information to be captured and interpreted. To facilitate imagined speech, a Brain-to-Computer Interface (BCI) must be implemented to provide silent communication abilities directly between the two entities. One of the most popular methods for interfacing directly between a human brain and a computer is through electroencephalographic signals.  Researchers have created models capable of achieving 70 - 90% predictive accuracy in recognizing patterns in EEG data; however, the accuracy of current methods for unspoken speech recognition is not yet sufficient to enable fluid communication between humans and machines.High Level Experiment Design the subjects were asked to imagine a specific word or feeling (label). The subjects responded to a set of uniform verbal cues describing the set of labels as well as the desired individual label to imagine. The data was then processed in order to minimize the effects of irrelevant signal activity, or noise. Additionally the data was processed to minimize its volume while still maintaining the core \xe2\x80\x9cinformation\xe2\x80\x9d in the data. The condensed dataset was created by dropping irrelevant information from the EEG device and applying principal component analysis (PCA) to the video stream data. Once the data was processed and assembled into the correct format, cross-validation using a random forest algorithm was performed on the control group of EEG signals alone and on the hypothesis group consisting of both EEG and video data. The predictive accuracy measurements obtained from the cross-validation experiments were used as metrics to evaluate the success of the hypothesis.The results show a notable improvement classifying thoughts when in conjunction with the video streams."	382	0	1	0
1340	156	0	82174	4	b"This article gives a description of mirror neurons in terms of Hebbian learning, a mechanism that has been widely used in AI. I don't know whether the formulation given in the article has ever actually been implemented computationally."	37	0	1	0
1341	247	0	10819	1	b"MLP's can theoretically approximate any bounded, continuous function. There's no guarantee for a discontinuous function. There are plenty of important discontinuous functions, like, say, the prime counting function.The prime counting function pi(n) is simply equal to the number of primes less than or equal to n. It has a discontinuity about each prime p, so good luck trying to approximate this with a neural network!However, this function is extensively studied and extremely important in number theory. See the Riemann hypothesis."	79	0	0	0
1342	1334	1	10787	3	b'Watson starts off by searching its massive database of sources for stuff that might be pertinent to the question. Next, it searches through all of the search results and turns them into candidate answers. For example, if one of the search results is an article, Watson might pick the title of the article as a possible answer. After finding all of these candidate answers, it proceeds to iteratively score them to determine which one is best.The scoring process is very complicated, and involves finding supporting evidence for each answer, and then combining many different scoring algorithms to determine which candidate answer is the best. You can read a more detailed (but still very conceptual) overview here, by the creators of Watson.'	120	0	1	0
1345	198	1	45882	4	b'The following survey article by researchers from IIT Bombay summarizes recent advances in sarcasm detection: Arxiv link.In reference to your question, I do not think it is considered either extraordinarily difficult or open-ended. While it does introduce ambiguity that computers cannot yet handle, Humans are easily able to understand sarcasm, and are thus able to label datasets for sarcasm detection.'	59	0	0	0
1348	-1	0	0	16	b"Isaac Asimov's famous Three Laws of Robotics originated in the context of Asimov's science fiction stories. In those stories, the three laws serve as a safety measure, in order to avoid untimely or manipulated situations from exploding in havoc.More often than not, Asimov's narratives would find a way to break them, leading the writer to make several modifications to the laws themselves. For instance, in some of his stories, he modified the First Law, added a Fourth (or Zeroth) Law, or even removed all Laws altogether.However, it is easy to argue that, in popular culture, and even in the field of AI research itself, the Laws of Robotics are taken quite seriously. Ignoring the side problem of the different, subjective, and mutually-exclusive interpretations of the laws, are there any arguments proving the laws themselves intrinsically flawed by their design, or, alternatively, strong enough for use in reality? Likewise, has a better, stricter security heuristics set being designed for the purpose?"	159	0	0	0
1354	-1	0	0	5	b"Are there any modern techniques of generating textual CAPTCHA (so person needs to type the right text) challenges which can easily fool AI with some visual obfuscation methods, but at the same time human can solve them without any struggle?For example I'm talking about plain ability of recognising text embedded into image (without considering any external plugins like flash or java, image classification, etc.) and re-typing the text that has been written or something similar.I guess adding noise, gradient, rotating letters or changing colours are not reliable methods any more, since they can be quickly broken.Any suggestions or research has been done?"	101	0	1	0
1355	1348	1	7609	10	b'Asimov\'s laws are not strong enough to be used in practice. Strength isn\'t even a consideration, when considering that since they\'re written in English words would first have to be interpreted subjectively to have any meaning at all. You can find a good discussion of this here.To transcribe an excerpt: How do you define these things? How do you define "human", without first having to take a stand on almost every issue. And if "human" wasn\'t hard enough, you then have to define "harm", and you\'ve got the same problem again. Almost any really solid unambiguous definitions you give for those words&mdash;that don\'t rely on human intuition&mdash;result in weird quirks of philosophy, leading to your AI doing something you really don\'t want it to do.One can easily imagine that Asimov was smart enough to know this and was more interested in story-writing than designing real-world AI control protocols.In the novel Neuromancer, it was suggested that AIs could possibly serve as checks against each other. Ray Kurzweil\'s impending Singularity, or the possibility of hyperintelligent AGIs otherwise, might not leave much of a possibility for humans to control AIs at all, leaving peer-regulation as the only feasible possibility.It\'s worth noting that Eliezer Yudkowsky and others ran an experiment wherein Yudkowsky played the role of a superintelligent AI with the ability to speak, but no other connection outside of a locked box. The challengers were tasked simply with keeping the AI in the box at all costs. Yudkowsky escaped both times.'	247	0	1	0
1356	198	0	71497	1	b'There has been a recent work in the same domain where neural networks(CNNs to be accurate) are used for the same purpose. Some info. about the research is: To learn that context, the paper describes a method by which the neural network finds the user\xe2\x80\x99s \xe2\x80\x9cembeddings\xe2\x80\x9d \xe2\x80\x94 i.e. contextual cues like the content of previous tweets, related interests and accounts, and so on. It uses these various factors to plot the user with others, and (ideally) finds that they form relatively well-defined groups.So, the paper uses CNNs, word and user embeddings for detecting sarcasm in text. There is also a Techcrunch article on that.The paper uses sentiment of the tweet and compares with that of the other similar tweets: If the sentiment of the tweet seems to disagree with the bulk of what is expressed by similar users, there\xe2\x80\x99s a good chance sarcasm is being employed.Link to the paper'	148	0	1	0
1357	-1	0	0	9	b'Can an AI program have an EQ (Emotional intelligence or emotional quotient)?In other words, can the EQ of an AI program be measured?If EQ is more problematic to measure than IQ (at least with a standard applicaple to both humans and AI programs), why is that the case?'	47	0	0	0
1358	-1	0	0	10	b'I have heard about this concept in a reddit post about Alpha Go. I have trued to go through the paper and the article, but could not really make sense of the algorithm.So, can someone give a easy-to-understand explanation of how the Monte-Carlo search algorithm work and how is it being used in building game-playing AI bots?'	56	0	0	0
1359	1357	1	9840	5	b'The answer to your question is "In principle, yes" - in it\'s most general form, EQ testing is just a specific case of the Turing test ("How would you feel about ... ?"). To see why meaningful EQ tests might be difficult to achieve, consider the following two possible tests:At one extreme of complexity, the film \'Blade Runner\' famously shows a test to distinguish between human and android on the basis of responses to emotionally-charged questions.If you tried asking these questions (or even much simpler ones) to a modern chatbot, you\'d likely quickly conclude that you were not talking to a person.The problem with assessing EQ is that the more emotionally sophisticated the test, the more general the AI system is likely have to be, in order to turn the input into a meaningful representation.At the other extreme from the above, suppose that an EQ test was phrased in an extremely structured way, with the structured input provided by a human. In such a case, success at an \'EQ test\' is not really grounded in the real-world.In an essay entitled "The ineradicable Eliza effect and its dangers", Douglas Hofstadter gives the following example, in which the ACME program is claimed (not by Hofstadter) to \'understand\' analogy. Here the computer learns about a fellow named Sluggo taking his wife Jane and his good buddy Buck to a bar, where things take their natural course and Jane winds up pregnant by Buck. She has the baby but doesn\'t want it, and so, aided by her husband, she drowns the baby in a river, thus "neatly solving "the problem" of Bambi.This story is presented to ACME in the following form:Suppose the program were to be asked if Jane Doe\'s behavior was moral. Complex compound emotional concepts such as \'neglectful\', \'lonely\' and \'innocent\' are here simply predicates, not available to the AI for deeper introspective examination. They could just as easily be replaced by labels such as as \'bling-blang-blong15657\'. So in one sense, the absence of success at an EQ test with any depth is indicative of the general problem currently facing AI: the inability to define (or otherwise learn) meaningful representations of subtle complexities of the human world, which is a lot more complex than being able to recognize videos of cats.'	378	8	0	0
1360	-1	0	0	6	b"DNNs are typically used to classify things (of course) but can we let them go wild with sounds and then tell them if we think it sounds good or not? I'd like to think after a training class has been made (perhaps comparing the output to an existing song) we could get an NN that has a basic concept of music.Timing would be an issue; I'm not sure how feasible this is. A strongly weighted input attached to all hidden layers perhaps? Use it as the bias?Is this even slightly feasible? "	91	0	0	0
1361	1358	1	8899	9	b"Monte Carlo method is an approach where you generate a large number of random values or simulations and form some sort of conlusions based on the general patterns, such as the means and variances.As an example, you could use it for weather forecasts. Predicting long-term weather is quite difficult, because it is a chaotic system where small changes can lead to very different results. Using Monte Carlo methods, you could run a large number of simulations, each with slightly different atmospheric changes. Then you can analyze the results and for example calculate the probability of rain on a given day based on how many simulations ended up with rain. As for the use of Monte Carlo in Alpha Go, they seem to be using the so-called Monte Carlo Tree Search. In this approach, you make a tree of possible moves, a few turns into the future, and try to find the best sequence. However, since the number of possible moves in the game of go is very large, you won't be able to explore very far ahead. This means that some of the moves which look good now might turn out to be bad later. So, in the Monte Carlo Tree Search, you pick a promising sequence of moves and run one or more simulations of how the game might proceed from that point. Then you can use the results of that simulation to get a better idea of how good that specific sequence of moves really is and you update the tree accordingly. Repeat as needed until you find a good move.If you want more information or to look at some illustrations, I found an interesting paper on the topic: C. Browne et al., A Survey of Monte Carlo Tree Search Methods (open repository / permanent link (paywalled))"	298	0	2	0
1362	-1	0	0	4	b'How do I avoid my gradient descent algorithm into falling into the "local minima" trap while backpropogating on my neural network?Are there any methods which help me avoid it?'	28	0	0	0
1363	-1	0	0	8	b'A neural network is a directed weighted graph. These can be represented by a (sparse) matrix. Doing so can expose some elegant properties of the network.Is this technique beneficial for examining neural networks?'	32	0	0	0
1364	1363	0	553	4	b'It depends on the type of neural networks you are dealing with.For medium sized neural nets, the matrix approach is a very good way to do quick computations and even backpropogation of errors. One can even exploit sparse matrixes for understanding the sparse architecture of some neural nets.But, for very large neural nets, using matrix computations would be computationally very intensive. So, relevant methods like graph-based stores, etc are used for them depending on the purpose and the architecture.'	78	0	0	0
1365	1362	1	1242	5	b"There are several elementary techniques to try and move a search out of the basin of attraction of local optima. They include:Probabalistically accepting worse solutions in the hope that thiswill jump out of the current basin (like Metropolis-Hastings acceptance in Simulated Annealing). Maintaining a list of recently-encountered states (or attributes thereof) and not returningto a recently-encountered one (like Tabu Search). Performing a random walk of a length determined by the current state of the search (an explicit 'Diversification strategy', e.g. as used in 'Reactive Tabu Search').See the excellent (and free online) book 'Essentials of Metaheuristics' by Sean Luke for more details on these kind of techniques and some rules of thumb about when and how to use them."	117	0	0	0
1366	1363	0	1560	4	b"For large ANNs, something equivalent to a 'sparse matrix format' is used in practice.In contrast to what is said in another answer given, considering an ANN as a graph doesn't actually buy very much, for two reasons:The backpropagation algorithm can usefully bedefined in terms of matrix operations. This page gives areadable and comprehensive description.All real-valued matrices can be represented as graphs, but the converse is clearly not the case. So while it is true that an ANN can be considered as a special case of a graph data structure, making that specialization explicit in matrix form is more efficient."	98	0	1	0
1367	1354	0	36585	0	b"An easy method would be to use a poem (short one, two paragraphs) then give an one line test question regarding the emotional states of the poem, that an AI won't or can't be programmed to understand."	36	0	0	0
1369	1360	1	11602	5	b"The first thing is to define what is a \xc2\xabgood\xc2\xbb and a \xc2\xabbad\xc2\xbb sound. This is an extremely tricky issue, since the networks need numeric inputs. And music is whole bunch of numbers.I know from people doing research in identifying how similar two sounds are, and imitation, say: you hear a sound and try to make another that sounds like it. Like when you hum a song or similar. That is by no means easy. These guys are using something similar to feature extraction, with Fourier transforms and energy and such things. They feed the networks with the (selected) features and... Train. Now, to return to your original question: *What do you present as target during training?* You can present different types of music as categories and classify (I couldn't help but think on this research with fish). Or you define categories of music you like and see if the network can classify them ;)One basic decision here is how long you get a piece of sound. Since it is needed to analyse frequency, this is a key issue. Since you talked about DNN, I was wondering if you wanted to do it online, as a stream, in which case I don't have the slightest idea where to begin, other than do it after a little while.Other idea: I remember a little sketch in this series about a researcher that makes use of the relations between peaks in the Fourier spectrum in order to differentiate noise from music."	247	0	1	0
1370	1333	1	69860	3	b"It seems easy for this to be sublinear growth or superlinear growth, depending on context. If we imagine the space of the complex AI as split into two parts--the context model and the content model (that is, information and structure that is expected to be shared across entries vs. information and structure that is local to particular entries), then expanding the source material means we don't have much additional work to do on the context model, but whether the additional piece of the content model is larger or smaller depends on how connected the new material is to the old material.That is, one of the reasons why Watson takes many times the space of its source material is because it stores links between objects, which one would expect to grow with roughly order n squared. If there are many links between the old and new material, then we should expect it to roughly quadruple in size instead of double; if the old material and new material are mostly unconnected and roughly the same in topology, then we expect the model to roughly double; if the new material is mostly unconnected to the old material and also mostly unconnected to itself, then we expect the model to not grow by much."	209	0	0	0
1371	179	0	31451	2	b"If anything, multiple intelligences are much more obvious in AI than in other fields, because we haven't yet unlocked how to do transfer between domains.As an example, AlphaGo is very, very good at playing Go, but it's got basically nothing in the way of bodily-kinesthetic intelligence. But other teams have built software to control robots that does have bodily-kinesthetic intelligence, while not being good at the tasks that AlphaGo excels at.This sort of modular intelligence is typically referred to as 'narrow AI,' whereas we use the term 'general AI' (or AGI, for Artificial General Intelligence) to refer to intelligence that we've built that can do roughly as many different kinds of things as people can do."	115	0	0	0
1372	145	0	60892	4	b'"Current artificial intelligence research" is a pretty broad field. From where I sit, in a mostly CS realm, people are focused on narrow intelligence that can do economically relevant work on narrow tasks. (That is, predicting when components will fail, predicting which ads a user will click on, and so on.)For those sorts of tools, the generality of a formalism like AIXI is a weakness instead of a strength. You don\'t need to take an AI that could in theory compute anything, and then slowly train it to focus on what you want, when you could just directly shape a tool that is the mirror of your task.I\'m not as familiar with AGI research itself, but my impression is that AIXI is, to some extent, the simplest idea that could work--it takes all the hard part and pushes it into computation, so it\'s \'just an engineering challenge.\' (This is the bit about \'finding approximations to AIXI.\') The question then becomes, is starting at AIXI and trying to approximate down a more or less fruitful research path than starting at something small and functional, and trying to build up?My impression is the latter is much more common, but again, I only see a small corner of this space.'	206	0	0	0
1373	145	0	62121	2	b"AIXI is really a conceptual framework. All the hard work of actually compressing the environment still remains.To further discuss the question raised in Matthew Graves answer: given our current limited level of ability to represent complex environments, it seems to me that it doesn't make a lot of practical difference whether you start with AIXI as defining the 'top' of the system and working down (e.g. via supposedly generalized compression methods) or start at the 'bottom' and try solve problems in a single domain via domain-specific methods that (you hope) can subsequently be abstracted to provide cross-domain compression."	97	0	0	0
1376	-1	0	0	1	b"Would it be ethical to implement AI for self-defence for public walking robots which are exposed to dangers such as violence and crime such as robbery (of parts), damage or abduction?What would be pros and cons of such AI behavior? Is it realistic, or it won't be taken into account for some obvious reasons?Like pushing back somebody when somebody start pushing it first (AI will say: he pushed me first), or running away on crowded street in case algorithm will detect risk of abduction."	83	0	0	0
1377	1376	0	878	2	b'The question mentions "walking robot", but it may be illustrative to re-frame the discussion in terms of self-driving cars, because: It gives a common point of reference, rather than everyone having their own separate vision of how vulnerable/powerful a kung-fu walking robot might be.We already know a lot about societal attitudes to car theft.Given that autonomous vehicles will soon be mainstream, the morality of the question is then more of a pressing issue.So, should a self-driving car run someone over (likely killing them) if they try to steal it? I\'m hoping that few people would argue that it should.Should it attempt to do a lesser amount of damage (say, calculated to hopefully only break a leg)?Again, I\'d argue not. The main reason for saying this is that our decision-making algorithms are simply not sufficiently context aware to be able to decide whether theft or harm is the intent. To concretely illustrate this: a recent fatality arose because a self-driving Tesla was oblivious to context to the extent that it couldn\'t distinguish between a high-sided van and empty space.Under those circumstances, it\'s probably best not to allow commercial autonomous systems to cause physical damage (even to inanimate objects). \'Running away\' (or rather, \'driving away\', in the case of the car) is another matter: driving is what it\'s designed to do.'	218	0	1	0
1378	1376	0	997	2	b"It depends on whether the loss of the robot would end up causing harm to humans.If the robot was supposed to be watching for a suspected terrorist attack to start taking place (so it could alert authorities or halt the attack), it would be very bad if somebody dismantled the robot or otherwise stopped it from carrying out its mission. In that case, the device would be certainly justified in stopping humans from injuring it in any meaningful way.A robot carrying classified information should probably be similarly willing to protect itself, since the spread of such data could bring harm to a state or a lot of people.If an AI-enabled device was just walking the streets in the course of carrying out some mundane task, I think it would be hard to justify allowing the robot to incapacitate a human attacker. After all, it was made - presumably - to serve humans.No matter whether the AI was programmed to defend itself, people couldn't just impede or damage it with impunity. Intentional destruction of another person's property (including public property) is almost certainly a crime, as is intentional obstruction of law enforcement. It wouldn't have to be up to each robot to defend itself; it could just send information about the perpetrator to C&amp;C before its demise."	215	0	1	0
1379	-1	0	0	-2	b'Is there any risk in the near future of replacing all encyclopedias with Watson-like AI where knowledge is accessible by everybody through API?Something similar happened in the future in The Time Machine movie from 2002.Obviously maintaining 40 million articles and keeping it up-to-date and consistent could be beyond brain power of few thousands of active editors. Not to mention thousands of other encyclopedias including paperback version or large number of books used by universities which needs to be updated every year by a huge number of people.What are the pros and cons of such a change?'	95	0	0	0
1380	1379	1	1112	3	b"I get the impression that (perhaps even more than Bluemix) this is what the Wolfram Language is looking to offer in the longer term.Seems to me that the main pros and cons are two sides of the same coin:With Wikipedia, there's no 'search filter' between you and the text. Adding an algorithmic level of indirection between the user and the knowledge that they're looking for is subject to hidden biases.If those biases are intended in your best interests, and the search is context-sensitive enough to present you with information in the form that is most useful and digestible to you, then this is a good thing. Otherwise, not. Like many topics in AI, problems arise because we're simply not that good at modelling human context yet.Of course, we're already subject to this filter bubble effect via search engines and social media. The current consensus seems to be that even more of this would not be a good thing for society."	159	0	0	0
1381	-1	0	0	2	b"I've watched the Sunspring video which didn't make any sense to me (a lot of nonsense monologues), mainly because it was created by Jetson AI.What was the mechanism of creating such screenplay?On what criteria was it trained? What was the goal or motivation in terms of training criteria of defining when text does make sense? And what was missed (that it's so bad) and how possibly this could be improved?"	69	0	0	0
1382	1381	1	827	5	b"It appears to use Recurrent NNs (RNNs) that have a 'Long Short-Term Memory' (LTSM) architecture.Here's a summary of the development process that the author, Ross Goodwin, went through to create it.It seems to me (and is also observed in the above link) that the output is rather poor - simply comparable to what one might expect from Markov chains, a technique that is over 100 years old.I haven't dug deeply into the technique, so I could be misktaken, but perhaps one of the reasons that it's so bad is that (as far as I can see), the model-building process is essentially lexical - i.e. it is linking together tokens (words) without any more informed language model to guide it. In particular, the generated output doesn't seem to know anything about the functional roles played by objects (chairs are supporting objects, used by humans for sitting on etc), which is something that might be fairly readily incorporated."	155	0	1	0
1384	-1	0	0	2	b'This article suggests that deep learning is not designed to produce the universal algorithm and cannot be used to create such a complex systems.First of all it requires huge amounts of computing power, time and effort to train the algorithm the right way and adding extra layers doesn\'t really help to solve complex problems which cannot be easily predicted.Secondly some tasks are extremely difficult or impossible to solve using DNN, like solving a math equations, predicting pseudo-random lists, fluid mechanics, guessing encryption algorithms, or decompiling unknown formats, because there is no simple mapping between input and output.So I\'m asking, are there any alternative learning algorithms as powerful as deep architectures for general purpose problem solving? Which can solve more variety of problems, than "deep" architectures cannot?'	125	0	5	0
1385	1384	1	1158	4	b'Have you read the book The Master Algorithm: by Pedro Domingos?He discusses the present day machine learning algorithms... Their strengths, weaknesses and applications...Deep Neural NetworkGenetic AlgorithmBayesian Network Support Vector MachineInverse Deduction '	31	0	1	0
1386	1384	0	1299	3	b"Deep learning is actually pretty useful (relative to other techniques) precisely when there is no simple mapping between input and output, and features from the raw input need to be aggregated and combined in complex ways by successive layers to form the output.As I pointed out in my answer to the AI SE decompilation question, there is recent DL research which takes a natural language description as input and generates program text as output. Despite working in this general research area, I was personally surprised by this - the problem is significantly harder than the 'AI math' link you provide above."	100	0	2	0
1387	6	0	13659	2	b' "the human mind is a battleground of higher level goals and lower level goals "\xe2\x80\x94 Marvin Minsky paraphrasing Sigmund FreudI argue that in general human agents try to maximise a hierarchy of performance measures.performance measures of humansSurvival of genetic data Energy supply and WaterSexmyriad subgoals....Mysterious mental mechanisms which neuroscientists do not understand yet force the average human agent to maximise various evaluation metrics.With the overarching goal of survival of genetic information. Successful genes are immortal. We are still under the yoke of an ancient genetic algorithm.These measures are optimised throughout a humans life time. A 30 year old agent is better at survival than a 10 year old agent. A 30 year old agent makes fewer mistakes.We remember our mistakes. Mistakes are burned into our memory by high levels of neurotransmitters (and reinforcing of synapses) so we don\'t make them again.We attempt to optimise a swarm of subgoals that are all connected in one way or another to the main goal gene survival.statusmoneyeducation happiness'	164	0	0	0
1389	60	0	13253	-2	b"One obstacle to the development of AI is the fundamental limitations of computer memory. Computers, at a fundamental level, can only work with bits. This limits the type of information that they can describe.EDIT:The precise nature and complexity of human memory isn't fully understood, but I would argue that at the very least, human memory is well adapted for the types of tasks that humans perform. Thus, computer memory, even if theoretically capable of representing everything that human memory can, is probably inefficient and poorly structured for such a task. "	90	0	0	0
1390	-1	0	0	2	b"Is there any research which study application of AI into chemistry which can predict the output of certain chemical reactions.So for example, you train the AI about current compounds, substances, structures and their products and chemical reactions from the existing dataset (basically what produce what). Then you give the task to find how to create a gold or silver from group of available substances. Then the algorithm will find the chemical reactions (successfully predicting new one which weren't in the dataset) and gives the results. Maybe the gold is not a good example, but the practical scenario would be creation of drugs which are cheaper to create by using much more simpler processes or synthesizing some substances for the first time for drug industries.Was there any successful research attempting to achieve that using deep learning algorithms?"	135	0	1	0
1391	-1	0	0	8	b"Assume that I want to solve an issue with neural network that either I can't fit to already existing topologies (perceptron, Konohen, etc) or I'm simply not aware of the existence of those or I'm unable to understand their mechanics and I rely on my own instead.How can I deconstruct a problem to find a corresponding neural network topology? By this I don't mean only the size of certain layers, but the number of them, the type of activation functions, the number and the direction of connections, and so on.I'm a beginner, yet I realized that in some topologies (or, at least in perceptrons) it is very hard if not impossible to understand the inner mechanics as the neurons of the hidden layers don't express any mathematically meaningful context."	128	0	0	0
1392	-1	0	0	1	b"For example there is the MNIST database which is used to test artificial neural network (ANN), however it's not so challenging, because some hierarchical systems of convolutional neural networks manages to get an error rate of 0.23 percent.Are there any similar, especially the most challenging tasks with dataset which are used as benchmark tests to challenge the AI which are fairly reliable and it's possible to pass, but most AAN are struggling to achieve the lower error rate?"	77	0	0	0
1393	-1	0	0	2	b'This study (pages 7-8) shows an attempt at recognizing the traffic signs with lower error rates by using multi-column deep neural networks Are Google cars using similar techniques of predicting signs using DNN, or are they using some other method?'	39	0	1	0
1394	-1	0	0	2	b"I'd like to know whether there were attempts to simulate the whole brain, I'm not talking only about some ANN on microchips, but brain simulations."	24	0	1	0
1395	1394	0	0	3	b"Neuromorphic engineering offers various of ways of reproducing the brain\xe2\x80\x99s processing ability.The recent technology can include IBM's multi-artificial-neuron computer, the world's first artificial nanoscale stochastic phase-change neuronsarticle. Check the: Stochastic phase-change neurons study.Other can includeNeurogrid, built by Brains in Silicon at Stanford University is another example for brain simulation. It uses analog computation to emulate ion-channel activity. It emulates neurons using digital circuitry designed to maximize spiking throughputwiki.SpiNNaker, which is a manycore computer to simulate the human brain (see Human Brain Project).SyNAPSE, a DARPA neuromorphic machine technology, that scales to biological levels. Each chip can have over a million of electronic \xe2\x80\x9cneurons\xe2\x80\x9d and 256 million electronic synapses between neurons. In 2014 the 5.4 billion transistor chip had one of the highest transistor counts of any chip ever produced. The program is undertaken by HRL, HP and IBM."	136	0	3	0
1396	-1	0	0	11	b'On the wikipedia page about AI, we can read: Optical character recognition is no longer perceived as an exemplar of "artificial intelligence" having become a routine technology.On the other hand, the MNIST database of handwritten digits is especially designed for training and testing neural networks and their error rates (see: Classifiers).So why does the above quote state that OCR is no longer exemplar of AI?'	64	0	0	0
1397	-1	0	0	11	b'MIST is a quantiative test of humanness, consisting of ~80k propositions such as:Is Earth a planet?Is the sun bigger than my foot?Do people sometimes lie?etc.Have any AI attempted and passed this test to date?'	33	0	0	0
1398	1396	0	11107	3	b"I'm not sure if predicting MNIST can be really considered as an AI task. AI problems can be usually framed under the context of an agent acting in an environment. Neural nets and machine learning techniques in general do not have to deal with this framing. Classifiers for example, are learning a mapping between two spaces. Though one could argue that you can frame OCR/image classification as an AI problem - the classifier is the agent, each prediction it makes is an action, and it receives rewards based on its classification accuracy - this is rather unnatural and different from problems that are commonly considered AI problems."	106	0	0	0
1399	1391	1	29463	6	b"I think in this case, you'll probably want to use a genetic algorithm to generate a topology rather than working on your own. I personally like NEAT Paper (NeuroEvolution of Augmemting Topologies).(NEAT Paper)The original NEAT paper involves evolving weights for connections, but if you only want a topology, you can use a weighting algorithm instead. You can also mix activation functions if you aren't sure which to use. Here is an example of using backpropagation and multiple neuron types."	78	0	3	0
1400	1391	0	32661	4	b"Another answer mentions NEAT to generate network weights/topologies. Here's a nice paper on an alternative approach to NEAT, which also gives a short summary of neuroevolution techniques. It uses Cartesian Genetic Programming to evolve a multiple activation functions."	37	0	2	0
1401	-1	0	0	2	b'It is possible of normal code to prove that it is correct using mathematical techniques, and that is often done to ensure that some parts are bug-free. Can we also prove that a piece of code in AI software will cause it to never turn against us, i.e. that the AI is friendly? Has there any research been done towards this?'	60	0	0	0
1402	1396	0	18836	7	b'Although OCR is now a mainstream technology, it remains true that none our methods genuinely have the recognition facilities of a 5 year old (claimed success with CAPTCHAs notwithstanding). We don\'t know how to achieve this using well-understood techniques, so OCR should still rightfully be considered an AI problem.To see why this might be so, it is illuminating to read the essay"On seeing A\'s and seeing AS" by Douglas Hofstadter.With respect to a point made in another answer, the agent framing is a useful one insofar as it motivates success in increasingly complex environments. However, there are many hard problems (e.g. Bongard) that don\'t need to be stated in such a fashion. '	112	0	0	0
1403	1401	1	980	5	b'Unfortunately, this is extremely unlikely.It is nearly impossible to make statements about the behaviour of software in general. This is due to the Halting problem, which shows that it is impossible to prove whether a program will stop for any given input. From this result, many other things have been shown to be unprovable.The question whether a piece of code is friendly, can very likely be reduced to a variant of the halting problem.An AI that operates in the real world, which is a requirement for "friendliness" to have a meaning, would need to be Turing complete. Input from the real world cannot be reliably interpreted using regular or context-free languages.Proofs of correctness work for small code snippets, with clearly defined inputs and outputs. They show that an algorithm produces the mathematically right output, given the right input.But these are about situations that can be defined with mathematical rigour."Friendliness" isn\'t a rigidly defined concept, which already makes it difficult to prove anything about it. On top of that, "friendliness" is about how the AI relates to the real world, which is an environment whose input to the AI is highly unpredictable.The best we can hope for, is that an AI can be programmed to have safeguards, and that the code will raise warning flags if unethical behaviour becomes likely - that AI\'s are programmed defensively.'	224	0	0	0
1404	-1	0	0	3	b"In this paper, a proposal is given for what death could mean for Artificial Intelligence. What does this mean using English only? I understand that mathematical notation is useful for giving a precise definition, but I'd like to understand what the definition really means. "	44	0	1	0
1405	1401	0	1902	3	b"Here are some examples of recent work on verifying certain properties of autonomous systems [RoboCheck].However, to achieve the same kind of thing for the notion of 'friendly' using formal verification (i.e. 'proving correctness using mathematical techniques'),it would (at the least) seem necessary to be able to express 'friendly' within a logical formalism, (i.e. as a predicate testable within a model-checker, so that we can be sure a system never enters an undesirable state).However, it's not immediately clear that 'friendly' has a more specific definition than 'a desire not to harm humans', so much more low-level detail is needed.Some previous work in this general area that might be useful in this respect include:Deontic Logic - a logical calculus of obligations.Elephant 2000 - John McCarthy's description of a promise-based programming language."	128	0	1	0
1406	1396	1	20817	12	b'Whenever a problem becomes solvable by a computer, people start arguing that it does not require intelligence. John McCarthy is often quoted: "As soon as it works, no one calls it AI anymore" (Referenced in CACM).One of my teachers in college said that in the 1950\'s, a professor was asked what he thought was intelligent for a machine. The professor reputedly answered that if a vending machine gave him the right change, that would be intelligent.Later, playing chess was considered intelligent. However, computers can now defeat grandmasters at chess, and people are no longer saying that it is a form of intelligence. Now we have OCR. It\'s already stated in another answer that our methods do not have the recognition facilities of a 5 year old. As soon as this is achieved, people will say "meh, that\'s not intelligence, a 5 year old can do that!"A psychological bias, a need to state that we are somehow superior to machines, is at the basis of this.'	164	0	2	0
1407	1404	1	2366	7	b'The authors do actually give an English definition in terms of the well-known agent formulation of AI: We intend this usage to be intuitive: death means that one sees no more percepts, and takes no more actions.It would seem that this becomes possible for a reinforcement learning agent such as AIXI in a formulation that uses semi-measures of probability (which need not sum up to 1), rather than the more traditional notion.'	71	0	0	0
1408	1394	1	27151	4	b"Vernor Vinge said that if we can scan a human brain and then simulate it: We can run it at 1000 times the speed. The brain will be able to do 1000 years of thinking in 1 year ect. At this stage in history we have the computer power.The trouble lies in cutting a brain up and scanning the 100 billion neurons and 12 million kilometres of axons and 100000 billion synapses.And piecing together the connectome from all the data.Sebastian Seung at MIT is working on automating this scanning process with machine learning. By gathering training data from thousands of people playing his Eyewire gameHenry Markram in Europe tried to do something similar with his Blue Brain Project.He attempted to simulate the neocortical column of a rat. The EU gave him a billion euros to do this. Unfortunately he has been heavily criticised by the Neuroscience community. They claim that we don't know the physiology well enough to make a valid simulation.Check out his Ted Talk.In the 1970s Sydney Brenner achieved a full brain scan of a C Elegans worm. This worm has one of the simplest biological neural networks having only 302 neurons.Here is a picture of its connectome:An accurate computer simulation of this worm would be a major stepping stone to uploading a human brain."	216	0	0	0
1409	1363	0	80729	1	b'Matrix representation is beneficial for implementing neural networks in silicon.But for examining neural networks empirically it is sometimes good to visualise the synapse weight values as images or videos: Jason Yosinski\'s exploration of a convolution neural network. The network seems to have a "filter" that just detects shoulders. A bit like a lock that only opens when it recognises the pattern of shoulders.'	62	0	0	0
1410	-1	0	0	4	b"We can measure the power of the machine with the number of operation per second or the frequency of the processor. But does units similar of IQ for humans exist for a AI?I'm asking for a unit which can give countable result so something different from a Turing Test which only give a binary result."	54	0	0	0
1411	1410	0	4268	3	b'One of the challenges of AI is defining Intelligence.If we could precisely define general intelligence then we could program it into a computer. After all an algorithm is a process so well defined that it can be run on a computer.Narrow AI can be evaluated on its success at achieving goals in an environment. In domains such as computer vision and speech recognition narrow AI algorithms can be easily evaluated.Many universities curate narrow AI tests. Fei-Fei Li a professor at Stanford who directs the Artificial Intelligence lab there organises the annual ImageNet Challenge. In 2012 Geoffrey Hinton famously won the competition by building a Deep Neural Network that could recognize pictures more accurately than humans can.To my knowledge the testers commonly use Precision and recall evaluation metrics'	126	0	0	0
1412	1390	1	62197	5	b"Yes, many people have worked on this sort of thing, due to its obvious industrial applications (most of the ones I'm familiar with are in the pharmaceutical industry). Here's a paper from 2013 that claims good results; following the trail of papers that cited it will likely give you more recent work. "	52	0	0	0
1413	1410	0	12678	3	b'Shane Legg and Marcus Hutter proposed one in 2006. The main descriptive quotes (see the paper for the actual formula): Intelligence measures an agent\xe2\x80\x99s general ability to achieve goals in a wide range of environments  ...  It is clear by construction that universal intelligence measures the general ability of an agent to perform well in a very wide range of environments, as required by our informal definition of intelligence given earlier. The definition places no restrictions on the internal workings of the agent; it only requires that the agent is capable of generating output and receiving input which includes a reward signal.'	103	0	1	0
1414	41	0	80062	2	b"The other answers are correct that machine IQ test results are currently not indicative of machine intelligence. One of the surprising facts of human intelligence is that performance on almost all cognitive tasks are correlated with each other; that is, there is such a thing as 'general smartness' and IQ tests attempt to measure that thing.People have built programs that take IQ tests, however, and some of them perform quite well. Raven's Progressive Matrices, a visual pattern recognition IQ test, is an easy target for AI (see this paper as representative) and another group has constructed an AI that performs about as well as a 4 year old on the verbal intelligence portion of a standard childhood IQ test."	118	0	1	0
1415	-1	0	0	5	b'In the mid 1980s, Rodney Brooks famously created the foundations of "the new AI". The central claim was that the symbolist approach of \'Good Old Fashioned AI\' (GOFAI) had failed by attempting to \'cream cognition off the top\', and that embodied cognition was required, i.e. built from the bottom up in a \'hierarchy of competances\' (e.g. basic locomotion -> wandering around -> actively foraging) etc.I imagine most AI researchers would agree that the \'embodied cognition\' perspective has now (at least tacitly) supplanted GOFAI as the mainstream.My question takes the form of a thought experiment and asks: "Which (if any) aspects of \'embodied\' can be relaxed/omitted before we lose something essential for AGI?"'	111	0	0	0
1416	-1	0	0	2	b"In other words, which existing reinforcement method learns in fewest episodes? R-Max comes to mind, but its very old and I'd like to know if there is something better now."	29	0	1	0
1417	92	0	2168	9	b'The images that you provided may be unrecognizable for us. They are actually the images that we recognize but evolved using the Sferes evolutionary framework.While these images are almost impossible for humans to label with anything but abstract arts, the Deep Neural Network will label them to be familiar objects with 99.99% confidence.This result highlights differences between how DNNs and humans recognize objects. Images are either directly () or indirectly() encodedAccording to this video Changing an image originally correctly classified in a way imperceptible to humans can cause the cause DNN to classify it as something else.  In the image below the number at the bottom are the images are supposed to look like the digits But the network believes the images at the top (the one like white noise) are real digits with 99.99% certainty. The main reason why these are easily fooled is because Deep Neural Network does not see the world in the same way as human vision. We use the whole image to identify things while DNN depends on the features. As long as DNN detects certain features, it will classify the image as a familiar object it has been trained on. The researchers proposed one way to prevent such fooling by adding the fooling images to the dataset in a new class and training DNN on the enlarged dataset. In the experiment, the confidence score decreases significantly for ImageNet AlexNet. It is not easy to fool the retrained DNN this time. But when the researchers applied such method to MNIST LeNet, evolution still produces many unrecognizable images with confidence scores of 99.99%.More details here, here and here.'	272	0	3	0
1418	1415	1	3294	4	b"This is something of an orthogonal answer, but I think Brooks didn't go about his idea the right way. That is, subsumption architecture is one in which the 'autopilot' is replaced by a more sophisticated system when necessary. (All pieces receive the raw sensory inputs, and output actions, some of which turn off or on other systems.)But a better approach is the normal hierarchical control approach, in which the target of a lower level system is the output of a higher level system. That is, the targeted joint angle of a robot leg is determined by the system that is trying to optimize the velocity, which is determined by a system that is trying to optimize the trajectory, which is determined by a system that is trying to optimize the target position, and so on.This allows for increasing level of complexity while maintaining detail and system reusability.That said, I don't think you actually need what one would naively call 'embodied cognition' in order to get the bottom-up hierarchy of competencies that Brooks is right to point towards. The core feature is the wide array of inputs and outputs, which are understood in a hierarchical fashion that allows systems to be chained together vertically. I think you could get a functional general intelligence whose only inputs and outputs involve going through an Ethernet cable, and doesn't have anything like a traditional body that it actuates or senses through. (This is a claim that the hierarchical structure is what matters, not the content of what we use that structure for.)(The main place to look for more, I think, is actually a book about human cognition, called The Control of Perception by William T. Powers.)"	281	0	0	0
1419	104	0	2524	3	b'I can offer two (at first sight, conflicting) perspectives on this:Firstly:If the letter string \'abc\' becomes \'abd\' what would "doing the same thing" to \'ijk\' look like?This is just one example of a problem (so-called \'letterstring analogy problems\') that is not easily framed as an optimization problem - there are a range of answers that appear compelling to humans, each for it\'s own structurally-specific reason. Some of the subtleties of these kind of problems are discussed in detail here.Secondly:Here\'s a very high-level perspective on AGI in which optimization plays a key part.It\'s not at all clear how these two very different scales of approach might be reconciled. As someone who does optimization research for a living, I\'d be inclined to say that, certainly for all current, practical purposes, AGI can\'t really be treated as an optimization problem, since most interesting activities don\'t readily lend themselves to description via a cost function.'	150	0	2	0
1420	-1	0	0	8	b"Are there any research teams which attempted to create or have already created an AI robot which can be as close to intelligent as these found in Ex Machina or I, Robot movies?I'm not talking about full awareness, but an artificial being which can make its own decisions and physical and intellectual tasks that a human being can do?"	58	0	0	0
1421	1420	1	958	20	b'We are absolutely nowhere near, nor do we have any idea how to bridge the gap between what we can currently do and what is depicted in these films.The current trend for DL approaches (coupled with the emergence of data science as a mainstream discipline) has led to a lot of popular interest in AI.However, researchers and practitioners would do well to learn the lessons of the \'AI Winter\' and not engage in hubris or read too much into current successes.For example:Success in transfer learning is very limited. The \'hard problem\' (i.e. presenting the \'raw, unwashed environment\' to the machine and having it come up with a solution from scratch) is not beingaddressed by DL to the extent that it is popularly portrayed: expert human knowledge is still required to help decide how the input should be framed, tune parameters, interpret output etc.Someone who has enthusiasm for AGI would hopefully agree that the \'hard problem\' is actually the only one that matters. Some years ago, a famous cognitive scientist said "We have yet to successfully represent even a single concept on a computer". In my opinion, recent research trends have done little to change this.All of this perhaps sounds pessimistic - it\'s not intended to. None of us want another AI Winter, so we should challenge (and be honest about) the limits of our current techniques rather than mythologizing them.'	229	0	0	0
1422	1420	0	6680	0	b' "heavier-than-air flying machines are impossible" _ Lord Kelvin 1895 7 years later the Wright brothers built one.Currently we have many powerful narrow AI (good at special tasks) but we have no idea how to unify them into a single system like in a biological brain. '	46	0	0	0
1423	-1	0	0	9	b'We, humans, during following multiple processes (e.g. reading while listening to music) memorize information from less focused sources with worse efficiency than we do from our main concentration.Do such things exist in case of artificial intelligences? I doubt, for example that neural networks obtain such features, but I may be wrong.'	50	0	0	0
1424	1390	0	9362	1	b'Yes, there were successful attempts at predicting the interaction between molecules and biological proteins which have been used to identify potential treatments by using convolutional neural networks.For example in 2015, the first deep learning neural network has been created for structure-based drug design which trains 3-dimensional representation of chemical interactions which works similar to how image recognition works (composing smaller features into larger, complex structures).wikiStudy: AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug DiscoveryAnother approach is to use evolutionary artificial neural networks which can achieve great optimization results.Further more, the paper from 2015 demonstrated heurisic chemical reaction optimization (CRO) which is inspired by the natural of chemical reactions (e.g. transforming the unstable substances to the stable onces). Simulation results shows that CRO outperforms many evolutionary algorithms by population-based metaheuristics mimicking the transition of molecules and their interactions in a chemical reaction.Sample pseudocode algorithm for predicting synthesis given \xcf\x891, \xcf\x892 (from the above paper):which is used to generate a new solution \xcf\x89\xe2\x80\xb2 based on two given solutions \xcf\x891 and \xcf\x892.'	172	10	1	0
1426	-1	0	0	5	b'How can a swarm of small robots (like Kilobots) walking close to each other achieve collaboration without bumping into each other? For example, one study shows programmable self-assembly in a thousand-robot swarm (see article &amp; video) which are moving without GPS-like system and by measuring distances to neighbours. This was achieved, because the robots were very slow.Is there any way that similar robots can achieve much more efficient and quicker assembly by using more complex techniques of coordination? Not by walking around clock-wise (which I guess was the easiest way), but I mean using some more sophisticated way. Because waiting half a day (~11h) to create a simple star shape using a thousand-robot swarm is way too long!'	117	0	2	0
1427	-1	0	0	0	b"On Watson wiki page we can read: In healthcare, Watson's natural language, hypothesis generation, and evidence-based learning capabilities allow it to function as a clinical decision support system for use by medical professionals.How exactly such AI can help doctors to diagnose the diseases?"	42	0	0	0
1428	1427	1	0	0	b"Watson can make its diagnosis based on the patient's data and comparing it to the data from million of other studies.For example having enough genetic data and the right algorithms, its AI computing capability demonstrated the huge potential of data analysis based on which it can used for everything from diagnosing rare illnesses to prescribing perfect dosages of medicine based on the patient's personal genetic makeup. Of course there are still plenty of challenges which need to be overcome before it can be used in mainstream medicine. Recently Watson was able to diagnose rare form of leukemia after treatment was proved ineffective. It was fed in with patient\xe2\x80\x99s genetic data and compared to data from other 20 million oncological studies.Sources:Artificial Intelligence Used To Detect Rare Leukemia Type In Japan (2016)Watson correctly diagnoses woman after doctors were stumped (2016)"	137	0	2	0
1429	-1	0	0	1	b"Recently White House published the article: Preparing for the Future of Artificial Intelligence which says that government is working to leverage AI for public good and toward a more effective government.I'm especially interested how AI can help with computational sustainability, environmental management and Earth's ecosystem such as biological conservation?"	48	0	0	0
1430	1429	1	0	1	b'There are variety of aspects where AI can help for a public good. Future studies of computational methods can contribute to sustainable management ecosystem by its data acquisition, interpretation, integration and model fitting.Prof. Tom Dietterich is a leader in combining computer science and ecological sciences to build new discipline of Ecosystem Informatics which studies methods for collecting, analyzing and visualizing data on the structure and function of ecosystems.His group is involved in many aspects of ecosystem, such as:Models that can predict species distribution and their presence/absence elsewhere in order to create species distribution and migration/dispersal maps (such as DataONE Datanet, eBird project, BirdCast) .Bio-economic models which require solving large spatio-temporal optimization problems under uncertainty.Ecosystem prediction problems which require integrating heterogeneous data sources.Algorithms for deployment (sensor placement), cleaning and analysis of sensor network data of resulting data to increase agricultural productivity (Project TAHMO), like deployment of 20,000 hydro-meteorological stations in Africa (e.g. computational problem where to place it).Systems for capturing, imaging, and sorting bugs combined with general image processing/machine learning/pattern recognition tools for counting and classifying them (BugID project). The goal is to develop algorithms for automating biodiversity based on the visual pattern recognition by using computer vision method.For further information about this work, check the following resources, see:(video) "Challenges for Machine Learning in Computational Sustainability" (CRCS) (2013)(slides) Tom Dietterich, Understanding and Managing Ecosystems through AI (2013)(study) Adapting environmental management to uncertain but inevitable change (2015)'	234	0	7	0
1431	-1	0	0	-4	b"When AI has some narrow domain such as chess where it can outperform the world's human masters of chess, does it make it a superintelligence or not?"	26	0	0	0
1432	-1	0	0	5	b"Suppose my goal is to collaborate and create an advanced AI, for instance one that resembles a human being and the project would be on the frontier of AI research, what kind of skills would I need?I am talking about specific things like what university program should I complete to enter and be competent in the field. Here are some of the things that I thought about, just to exemplify what I mean:Computer sciences: obviously the AI is built on computers, it wouldn't hurt to know how computers work, but some low level stuff and machine specific things does not seem essential, I may be wrong of course.Psychology: if AI resembles human beings, knowledge of human cognition would probably be useful, although I do not imagine neurology on a cellular level or complicated psychological quirks typical to human beings like the Oedipus complex would be relevant, but again, I may be wrong."	151	0	0	0
1433	-1	0	0	1	b"White House published the information about AI which requests mentions about 'the most important research gaps in AI that must be addressed to advance this field and benefit the public'.What are these exactly?"	32	0	0	0
1434	1433	0	0	1	b"According to IBM Research organization in the response to White House as part of preparing for the future of Artificial Intelligence, AI depends upon many long-term advances, not only from AI researchers, but from many interdisciplinary teams of experts from many disciplines, including the following challenges:Machine learning and reasoning.Currently AI systems use supervised learning using huge amount of dataset of labeled data for training. This is very different to how humans learn by creating concepts, relationship, common sense reasoning which gives ability to learn much without too much data. Therefore machine learning with common-sense reasoning capabilities should be researched further more.Decision techniques.Current AI-based systems have very limited ability for making decisions, therefore new techniques must be developed (e.g. modeling systemic risks, analyzing tradeoffs, detecting anomalies in context, analyzing data while preserving privacy).Domain-specific AI systems.The current AI-based system is lack of abilities to understand the variety of domains of human expertise (such as medicine, engineering, law and many more). The systems should be able to perform professional-level tasks such as designing problems, experiments, managing contradictions, negotiating, etc.Data assurance and trust.The current AI-based systems require huge amounts of data and their behaviour directly depends on the quality of this data which can be biased, incomplete or compromised. This can be expensive and time consuming especially where it is used for safety critical systems which potentially can be very dangerous.Radically efficient computing infrastructure.The current AI-based systems require unprecedented workloads and computing power which require development of new computing architectures (such as neuromorphic).Interpretability and explanations.For people to follow AI suggestions, they need to trust systems, and this is only when they are capable of knowing users' intents, priorities, reasoning and they can learn from their mistakes. These capabilities are required in many business domains and professionalsValue alignment and ethics.Humans can share the common knowledge of how the world function, the machine cannot. They can fail by having unintended and unexpected behaviour only because humans did not specify the right goals for them or them omitted essential training details. The systems should be able to correct specification of the goals and avoid unintended and undesired consequences in the behaviour.Social AI.The AI-based systems should be able to work closely to humans in their professional and personal life, therefore they should have significant social capabilities, because they can impact on our emotions and our decision making capabilities. Also sophisticated natural language capabilities will need to be developed to allow a natural interaction and dialog between humans and machines.Source: Fundamental questions in AI research, and the most important research gaps (RFI questions 5 and 6)"	426	0	1	0
1436	-1	0	0	4	b'Is there any methods by which artificial intelligence use recursion(s) to solve a certain issue or to keep up working and calculating?'	21	0	0	0
1437	52	1	51062	1	b"Andrej Karpathy's blog has a tutorial on getting a neural network to learn pong with reinforcement learning. His commentary on the current state of the field is interesting.He also provides a whole bunch of links (David Silver's course catches my eye). Here is a working link to the lecture videos.Here are demos of DeepMinds game playing.Get links to the papers at Andrej Karpathy's blog above- rat fpsnice demos at 19 minutes into this"	72	0	3	0
1438	1431	1	18759	3	b"There are three typical use cases for the phrase 'superintelligent':Something that is at least as smart as a human for every task.Something that is smart enough to improve itself on a fundamental level.Something that is smarter than a human at a single task.Most uses that I see are definition 2, but the other two are also somewhat common. I typically follow I. J. Good and use ultraintelligence for the first definition. (There are lots of arguments that something that fits definition 1 is likely to fit definition 2 as well, but this doesn't seem to be logically necessary.)Obviously, Deep Blue only counts for definition 3."	104	0	0	0
1439	1436	1	12156	5	b"To my knowledge, recursion does not play a strong role in the definition of modern AI techniques, although it does feature used in Lovasz's definition of 'Local Search' and Kurzweil is certainly an advocate. Recursion can be seen as an elegant 'architectural factorization' - building complexity by combining the results of smaller, similar patterns previously encountered. Computationally, recursion can always be converted into iteration so this form of elegance is really mainly of use in helping to make designs more comprehensible.GOFAI algorithms that were traditionally defined using recursion include depth- and breath- first search and means-ends analysis (used in Newell and Simon's General Problem Solver).With respect to performance, while many functions can be very economically defined using recursion, the naive version of such definitions can be inefficient.This page gives an example in which recursive version of the Fibbonnacci function, which has asymptotic execution time n^1.6, which is reduced to n by the use of memoization."	154	0	3	0
1440	1423	1	30213	5	b"Douglas Hofstadter's CopyCat architecture for solving letter-string analogy problems was deliberately engineered to maintain a semantically-informed notion of 'salience', i.e. given a variety of competing possibilities, tend to maintain interest in the one that is most compelling. Although the salience value of (part of) a solution is ultimately represented numerically, the means by which it determined is broadly intended to correspond (at least functionally) to the way 'selective attention' might operate in human cognition."	73	0	1	0
1441	1433	0	18346	2	b'One way of illustrating the deficiencies of many of our current approaches at once is to consider how well it is possible to represent (equivalently, learn) commonsense knowledge. In this area, the Winograd Schema Challenge has been proposed by Levesque, in which each problem is given as input natural language text containing an ambiguous pronoun: Babar wonders how he can get new clothing. Luckily, a very rich old man who has always been fond of little elephants understands right away that he is longing for a fine suit.Here, the program is asked to decide if \'he\' in "he is longing for a fine suit" refers to Babar or the old man. Several thousand such questions have been collated and proposed as a more quantifiable alternative to the Turing test.Despite the fact that the input domain is natural language, success here is undeniably a pre-requisite for AGI and (as implied in my answer here) for being able to interact ethically with the human world.'	162	0	3	0
1442	1288	0	7726	4	b'There does not appear to be an historicial consensus on this. The Wikipedia page on the Perceptrons book (which does not come down on either side) gives an argument that the ability of MLPs to compute any Boolean function was widely known at the time (at the very least to McCulloch and Pitts).However, this page gives an account by someone present at the MIT AI lab in 1974, claiming that this was not common knowledge there, alluding to documentation in "Artificial Intelligence Progress Report: Research at the Laboratory in Vision, Language, and other problems of Intelligence" (p31-32) which is claimed to support this.'	102	0	1	0
1443	1426	1	36423	8	b'There has been quite a few approaches to achieve such kind of distributed coordination. I present here one of them, for its generality and simplicity (that makes it easy to remember too). But first, the general idea behind these approaches is pretty interesting, around a mechanism called stigmergy.Stigmergy is a behaviour coordination mechanism mediated by the environment. It was first described for termites, and the most famous example pertains to ants. Ants form trails when going out for food, but they often do not interact directly. It turns out they leave pheromones on the ground as they walk away from their hills. The pheromone allows them to find their way home, and it also guides where they are going: If they find a pheromone trace from one of their peers, they follow it and their own pheromones add up, reinforcing the signal of the trail. In stage, more and more ants get "together as they move", forming a trail. Coordination has been achieved.Among the various implementation derived from stigmergy, there is the "field-based motion coordination model" (FBMCM). The idea is to create a (maybe virtual) environment that maintains some states of the world. Each object registers in the environment a signal that is maximum at the object position (its edges) and then decreases with distance. Moving objects (e.g. robots) each emit a signal relayed by the environment. They can then sense each other\'s field and act accordingly: E.g., when signals are strong, move away; when weak, it is safe to get closer, etc. Several complex group moves have been demonstrated in software simulators (platoon formation in games, drill simulations) and with robots. The benefit of this approach is that it can be cheap to compute even complex behaviours. For example, avoiding clashes requires simple "logic" code based on summing-up nearby fields value. FBMCM is pretty slick, used in video-games, but hard to implement in physical settings (to my dating knowledge), as it can be challenging to build a reliable environment. See for example the work from Mamei and Zambonelli, as well as one of the first industrial implementations for robots by Weyns et al.. Note that the implementation for robots required significant work on the environment infrastructure, made somewhat more feasible as it was a controlled warehouse.The advantage of stigmergy-like models is that they are often simple and resilient: You can lose an ant without impact on the food-finding trail. On the downside, these models are usually slow, as the coordination takes time to emerge from indirect interactions. This can be improved upon by adding extra direct interactions (e.g. empowering ants with a GPS and a grocery store map, or just a magnetic-North sense).In practice, these models can collapse if the environment implementation is not reliable. It can be difficult for robots or, say, self-driving cars, if they expect some transponders put on their way, as these devices are expensive to set and maintain, and they can be broken or stolen. It would be better to endow robots with radars, sonars or other proximity sensors to implement stigmergic models. One related example is the decision by Tesla to add radars to its cars, instead of assuming reliable transponders on the road (note: This is just a parallel; there is no official relation).Other implementations and related models are, for example, tuple-based coordination languages such as Linda, and network protocols like Chord. As you see, these works are not necessarily in the "AI domain".'	571	0	0	0
1444	1423	0	44385	4	b'Concentration, perhaps easier to grasp as "focus" or "attention", has quite some history in AI. user217281728 mentions CopyCat, and there was work with neural networks in the 80s as well (e.g. from Fukushima, creator of the Neocognitron).More recently, attention in neural networks is gaining momentum. The mechanisms are applied to learning in deep neural networks.'	54	0	2	0
1445	154	0	53020	1	b"Yes - it would seem that it is now possible to achieve more is required from the example you've given this paper describes a DL solution to a considerably harder problem - generating the source code for a program described in natural language.Both of these can be described as regression problems (i.e. the goal is to minimize some loss function on the validation set), but the search space in the natural language case is much bigger."	75	0	1	0
1446	-1	0	0	3	b"The Von Neumann's Minimax theorem gives the conditions that make the max-min inequality an equality.I understand the max-min inequality, basically min(max(f))&gt;=max(min(f)).The Von Neumann's theorem states that, for the inequality to become an equality f(.,y) should always be convex for given y and f(x,.) should always be concave for given x, which also makes sense.This video says that for a zero-sum perfect information game, the Von Neumann's theorem always holds, so that minimax always equal to maximin, which I did not quite follow.QuestionsWhy zero-sum perfect information games satisfy the conditions of Von Neumann's theorem?If we relax the rules to be non-zero-sum or non-perfect information, how would the conditions change?"	107	0	0	0
1447	1432	0	39068	7	b"One of the comments suggests a PhD in machine learning. As a full-time AI researcher myself, I'd say that would certainly be one useful option.However, in order make much-needed progress, AI needs avoid falling into the trap of thinking that currently fashionable methods are any kind of 'silver bullet'. There's some danger that a PhD that heads straight into (say) some sub-sub-sub area of DL would end up imposing too much bias on the student's subsequent perspective.AI research is an essentially multi-disciplinary activity. Other possible backgrounds therefore include:Mathematics or physics (to first degree or PhD level). A strong background in either of these never did anyone any harm. People who are competent in these fields tend to be able to turn their abilities to new domains relatively easily. Software Engineering. One of the things that AI needs are integrative architectures for knowledge engineering. Here's why. I believe that one of the reasons that we haven't yet managed to do OCR at the level of a 5 year old is that we've yet to accept that we have to 'build a sledgehammer to crack a nut'. Software architects are used to managing large-scale complexity, so they may be able to help.Cognitive Science, Psychology, Cognitive Linguistics. The reasons here are obvious.Above all, I personally think that a good AI researcher should be creative, inquisitive and prepared to question received wisdom, all of which are more important in practice than specifics of their background."	240	0	3	0
1448	1432	0	43791	4	b'Research on AI seems to be getting wider these days (2016). First, "obvious" few departments (no order):Computer Science (e.g. computation theory, algorithms): AI researchers there assume that intelligence is a kind of computation, under various forms (e.g. a neural network, a logic system).Software Engineering: Assuming we find a good model for AI, how do you make it? This is what the engineer will want to figure out. And it can be hard to map mathematical models to an engineered piece.Statistics and Probabilities (more specific than just Mathematics, which is also close to Computer Science): This is about Data Science, notably as a foundation to Machine Learning, the most active branch in AI---which "just" covers the learning part.Physics: This is particularly relevant now for hardware (see below).Neuro Science: Understand how the brain works, as an inspiration to create an artificial one, is the home for Connectionists. Recently, Hassabis and his team at Google Deepmind made several breakthroughs related to reinforcement learning, memory, attention, etc.Recently Electric Engineering is getting a lot of light, together with the related branches of Physics. Several public and private laboratories focus on "brain chips". To name a few: IBM (who\'s working on that for some time already), Nvidia, and Facebook. Circa 2010, it became clear that techniques like deep learning require horsepower, thus an increasing focus on creating more powerful, smaller, more energy efficient chips. And on top of that, there is all the work in Quantum Computing.But the thing is, there seems to be many more fields that are getting involved in AI research. We should mention Chemistry and Biology, as both inspiration and tools to make new models or hardware (e.g. chips that do not use silicon, so they can get smaller).As for 2016, the above fields are the most active, and promise to remain very active for quite some time. Pick your own depending on your interest, skills, or mere intuition!To finish, we may be surprised in a few years when we look back at where AI has come from. I believe that if we manage to build an AGI, it will leverage all these fields anyway. I guess the thrill is to be part of the story.'	363	0	0	0
1449	70	0	2111	0	b"Convolutional neural network can be applied not only for image recognition, but also for video analysis and recognition, natural language processing, natural language processing, in games (e.g. Go) or even for drug discovery by predicting the interaction between molecules and biological proteinswiki.Therefore it can be used for variety of problems by using convolutional and subsampling layers connected to more fully connected layers. They're easier to train, because have fewer parameters than fully connected networks with the same number of hidden units.UFLDL"	80	0	2	0
1450	1392	1	63858	4	b'Yes. Here are some of the most prominent ones and their respective state-of-the-art errors:CIFAR-10: ~3.5% errorCIFAR-100: ~24% error STL-10: ~26% errorSVHN: ~1.7% errorImageNet tasks: the best 2012 classification task solution got 15% top-5 error, better results are currently availableYou can check an updated list of solutions here. Also, a more comprehensive list of modern datasets can be found here.'	58	0	4	0
1451	-1	0	0	14	b'In October 2014, Dr. Mark Riedl published an approach to testing AI intelligence, called the "Lovelace Test 2.0", after being inspired by the original Lovelace Test (published in 2001). Mark believed that the original Lovelace Test would be impossible to pass, and therefore, suggested a weaker, and more practical version.The Lovelace Test 2.0 makes the assumption that for an AI to be intelligent, it must exhibit creativity. From the paper itself: The Lovelace 2.0 Test is as follows: artificial agent a is challenged as follows:   a must create an artifact o of type t; o must conform to a set of constraints C where ci \xe2\x88\x88 C is any criterion expressible in natural language; a human evaluator h, having chosen t and C, is satisfied that o is a valid instance of t and meets C; and a human referee r determines the combination of t and C to not be unrealistic for an average human. Since it is possible for a human evaluator to come up with some pretty easy constraints for an AI to beat, the human evaluator is then expected to keep coming up with more and more complex constraints for the AI until the AI fails. The point of the Lovelace Test 2.0 is to compare the creativity of different AIs, not to provide a definite dividing line between \'intelligence\' and \'nonintelligence\' like the Turing Test would.However, I am curious about whether this test has actually been used in an academic setting, or it is only seen as a thought experiment at the moment. The Lovelace Test seems easy to apply in academic settings (you only need to develop some measurable constraints that you can use to test the artificial agent), but it also may be too subjective (humans can disagree on the merits of certain constraints, and whether a creative artifact produced by an AI actually meets the final result).'	316	0	2	0
1452	1397	1	60232	7	b'Yes, although how useful this AI can be is another question entirely.mpgac is a "minimally intelligent AGI" trained on the GAC-80K corpus of MIST questions. As a result, it should be able to "minimally" pass this test. However, being trained on the GAC-80K corpus obviously make it lacking for any practical purposes. From the README: Obviously this should only be capable of producing a minimally intelligent signal when ordinary commonsense questions are asked, of the kind depicted above, using questions which would have made sense to an average human between the years 2000 and 2005. On expert knowledge or current affairs related questions it should perform no better than chance.The point of mpgac is to compare it to other AIs that could be built to pass this test. Or as the writer wrote in the README: When scanning the skies how can we tell whether the radio signals detected are from an intelligent source, or are merely just background or sensor noise?Ideally, you would want to build a program that is "better" than mpgac. In much the same way as ELIZA can be seen as a baseline for the Turing Test, mpgac is the baseline for the MIST test.The GitHub repo of mpgac (as well as the GAC-80K corpus) is available here.'	211	0	0	0
1453	-1	0	0	2	b"Convolutional neural network are leading type of feed-forward artificial neural network for image recognition. Can they be used for real-time image recognition for videos (frame by frame), or it takes too much processing (assuming they're written in C-like language)?For example for classification of type of animals based on the training from huge dataset."	52	0	0	0
1454	186	0	62057	2	b'No, quantum computers (as understood by mainstream scientists) cannot solve the halting problem. We can already simulate quantum circuits with normal computers; it just takes a really long time when you get a decent number of qubits involved. (Quantum computing provides exponential speedups for some problems.) Therefore, if quantum computers could solve the halting problem, we could solve the halting problem with classical computers by simulating a quantum one, but it\'s impossible to solve the halting problem with classical computers, so we can\'t do it with quantum ones either.There are proponents of hypercomputation - infinite speedups using quantum computers - but the evidence put forward so far is mostly conjecture. Further reading: Can quantum computing solve classicallyunsolvable problems? (PDF), References on comparison between quantum computers and Turing machines (at CS.SE).Solving the halting problem would make a computer exceptionally powerful. It would conceivably be able to check whether complex theorems are true without necessarily needing to product a mathematical proof. Solving that problem isn\'t necessary for strong AI, though. Going with the definition of "strong AI" as "where the machine\'s intellectual capability is functionally equal to a human\'s" (source), a computer could be able to learn like a human despite not being able to look at a program and see if it halts. I can\'t magically determine the halting properties of any arbitrary program, yet I\'d like to think I\'m an intelligent being.'	231	0	2	0
1456	1453	1	30323	2	b'We are getting there, with as usual some trade-off between quality and speed.For example Spatial Localization and Detection lecture shows some benchmarks (mAP = Mean Average Precision, higher is better; FPS = frame per second):'	34	0	1	0
1457	1297	0	69717	2	b'In AI (but in general too, I believe), a simplification is that modeling is more akin to Mathematics (and related hard sciences involved, like Physics and... Computer Science), and implementation to Software Engineering.Let\'s take a concrete example, really outside of AI: Find the minimum value of a given polynomial, if it exists.The Mathematician will derivate the polynomial, find the zeros, and checkout convexity to find a minimum (if there is any zero). This procedure is very standard---some will say straightforward. It relies on a body of knowledge and an abstraction level that is appropriate for manual proof.The Software Engineer approach is actually way longer to explain, and I am going to skip it. The point is that the body of knowledge is related but different: We have to find now a step-by-step procedure for the computer to achieve the result. The Mathematician one could be implemented directly in MathLab, almost verbatim, but we assume MathLab. And to build MathLab, we are back to the problem of making a procedure the computer can execute. We could for example base a procedure on Euler\'s method to find roots (a "simple" approach that closes on roots step after step), etc.Simple mathematical operations can be quite complex to implement on a computer. Perhaps the most famous is random-number generation. Mathematically, the concept is pure and clear. Generating an actual random-number is more elusive than it looks, to the point it calls for new models and new implementations...A concrete example from history: Neural networks. In the 80s and 90s, NNs were weighted graphs that could be executed on computers using graph libraries or similar foundation libs. Choosing the weights was challenging. One day the back-propagation learning model was introduced to automated the choice of weights. The model relied on a procedure dedicated to NNs, using a terminology like partial derivates, gradient descent, chain rules, etc. And later then, clever engineers created libraries to automate the back-propagation procedure. The libraries can be somewhat far from the original model, as engineers learn how to make it computable, even faster (i.e. optimization, approximations/truncations).'	344	0	0	0
1458	-1	0	0	3	b"Just for the purpose of learning I'd like to classify the likeliness of a tweet being in aggressive language or not. I was wondering how to approach the problem. I guess I need first train my neural network on a huge dataset of text what aggressive language is. This brings up the question where I would get this data in the first place?It feels a bit like the chicken and egg problem to me so I wonder how would I approach the problem?"	82	0	0	0
1459	1458	0	2672	3	b"I did a little search and couldn't find any database that has ground truth for aggressiveness. This means that you need to build yourself a database. This might be huge undertaking. Take thousands of messages, and classify them by hand whether they are aggressive or not. This part is quite labor intensive.Second part is much easier at start but would be pain to optimize (both performance and computational cost). I would suggest you to start with Naive Bayes classifier for this job. That is the preferred classifier for spam detection. ANN would probably not work for this case because the data would be a huge sparse vector. Estimated number of words in English is over a million, which means the input layer of your ANN should be able to scale up to that number. Search for sparse vector classification for additional classifier that can be used in these cases."	148	0	0	0
1460	1458	1	4895	2	b"The answer by Cem Kalyoncu mentions the difficulty of building a ground truth database for aggressiveness.One alternative approach would be to attempt to operate at the concept level, which would allow the use of pre-existing ontologies such as ConceptNet.Here's a paper that describes this technique."	44	0	2	0
1461	-1	0	0	15	b'Siri and Cortana communicate pretty much like humans. Unlike Google now which mainly gives us search results when asked some questions (not setting alarms or reminders), Siri and Cortana provide us with an answer, in the same way that a person would do.So are they actual AI programs or not?(By "question" I don\'t mean any academic related question or asking routes/ temperature, but rather opinion based question). '	67	0	0	0
1462	-1	0	0	2	b'The question is pretty much the title.Basically what is the difference between AI and robots?'	14	0	0	0
1463	1461	0	776	5	b'I would classify both as having / using elements of AI, yes. But I wouldn\'t say either represents a truly "intelligent" (in the AGI sense) program.But here\'s the rub... as you\'ll see in other questions asking about definitions of AI, there\'s a sort of memetic thing where anything that AI begins to do successfully, immediately stops being considered "AI". So AI is always an unreachable state, because it\'s always "something humans can do that computers can\'t" and once the computer can do it, it isn\'t AI anymore. So take that into consideration.'	91	0	0	0
1464	60	0	6036	3	b"One: we don't really know what intelligence is.Two: we don't truly understand the best model of intelligence we have available (human intelligence) works.Three: we're trying to replicate human intelligence (to some extent) on hardware which is quite different from the hardware it runs on in reality.Four: the human brain (our best model of intelligence) is mostly a black-box to us, and it's difficult to probe/introspect it's operation without killing the test subject. This is, of course, unethical and illegal. So progress in understanding the brain is very slow. Combine those factors and you can understand why it's difficult to make progress in AI. In many ways, you can argue that we're shooting in the dark. Of course we have made some progress, so we know we're getting some things right. But without a real comprehensive theory about how AI should/will work, we are reduced to a lot of trial and error and iteration to move forward."	155	0	0	0
1465	1462	1	1239	4	b'Although there are several definitions of "robot", an essential feature of everything called "robot" is that it is capable of movement. This does not necessarily mean displacement; a robot arm in a factory also moves.There is a single exception to this rule, which is bot-programs like chatbots; I will discuss them later.Artificial Intelligence does not need to move; a chess program can be argued to be an AI, but does not move. A robot can actually have AI; one of the definitions of robot is that it is a system, capable of autonomous movement. In order to be autonomous, to be able to make decisions of its own, a certain amount of AI may be necessary. There is one class of "robots" that does not move, and does not even have physical presence; bot programs, like chatbots, that operate inside systems. I do not consider them robots, because they are not physical devices operating in the real world. A chatbot can be an AI, however - a good chatbot may have some natural language processing to interact with humans in a way that humans find natural.To summarize; an AI can exist purely in software. But to be a robot, there must be a moving physical component in the real world.'	209	0	0	0
1466	1334	0	84841	2	b'IBM clearly don\'t provide all the details / "secret sauce" but there is some information out there on how Watson works. Some of the text search / retrieval stuff uses a technology called UIMA which IBM open-sourced a few years ago. It also uses Prolog and some custom C++ code. Some more information can be found here.'	56	0	2	0
1467	1462	0	1421	5	b'In the broadest sense, the difference is that non-robotic A(G)I may not be possible because, as per this question, it could be that "Intelligence requires a body".More specifically, it could be that there are limitations to what the traditional (well, 1950s style) \'Brain in a vat\' notion of an AI is capable of comprehending, in the absence of experience of embodied experience such as force, motion and "the raw, unawshed world".'	70	0	1	0
1469	1458	0	40485	2	b'A simple way to do it would be lexicograpical sentiment analysis. To do that, you\'d need a list of words categorized with a score that reflects "friendly" vs "aggressive" sentiment. For an example of setting up a SA system using Spark, see this article. To do what you\'re talking about, substitute AFINN for a different dataset. You might have to create said dataset yourself, if there isn\'t one "out there" like you want.Note that this isn\'t the most sophisticated technique in the world, but it\'s been found to be surprisingly effective. '	91	0	1	0
1470	1410	0	27541	0	b'One thing you\'ll see quite often, is to declare a correspondence between a system and a human of a given age. For example "this program can answer questions about science approximately as well as an average 7 year old" or something of that nature. '	44	0	0	0
1471	60	0	9837	2	b"I am assuming by AI you mean AG(eneral)I, not machine learning or expert systems tuned for specific tasks.In addition to mindcrime's answer, sometimes we run out of samples to train and sometimes computers became so slow to process enough samples to work in manageable timescales. bpachev mentioned memory but on the surface, our supercomputers have more than enough memory to store a human brain matrix. But we lack the ability to simulate it real time. After we are able to do that, we also need to connect external input, even more processing power is required for that. Even that would not be enough to simulate a human brain fully as biochemistry plays an important role. One final note would be there is little incentive to develop AGI other than understanding how human mind works. There are classification algorithms, expert systems, knowledge engines that can out-perform even the best humans on specific tasks."	151	0	0	0
1472	-1	0	0	1	b'With typical machine learning you would usually use a training data-set to create a model of some kind, and a testing data-set to then test the newly created model. For something like linear regression after the model is created with the training data you now have an equation that you would use to predict the outcome of the set of features in the testing data. You would then take the prediction that the model returned and compare that to the actual data in the testing set. How would a validation set be used here?With nearest neighbor you would use the training data to create an n-dimensional space that has all the features of the training set. You would then use this space to classify the features in the testing data. Again you would compare these predictions to the actual value of the data. How would a validation set help here as well?'	151	0	0	0
1476	-1	0	0	3	b"By reinforcement learning, I don't mean the class of machine learning algorithms such as DeepQ, etc. I have in mind the general concept of learning based on rewards and punishment. Is it possible to create a Strong AI that does not rely on learning by reinforcement, or is reinforcement learning a requirement for artificial intelligence? The existence of rewards and punishment imply the existence of favorable and unfavorable world-states. Must intelligence in general and artificial intelligence in particular have a way of classifying world-states as favorable or unfavorable? "	88	0	0	0
1477	-1	0	0	0	b"I'm not talking about mass scale Skynet or something, but for example Ethereum (or similar) which is a public blockchain-based distributed computing platform (like internet) featuring smart contracts which can be executed on a decentralized virtual machine. They call it a World Computer.Where there any attempts to use similar blockchain-based technology driven by community (or not) in order to create an artificial intelligence into a decentralized public blockchain-based distributed computing platform where it cannot be shutdown?"	75	0	4	0
1478	-1	0	0	1	b'For example, search engine companies want to classify their image searches into 2 categories (which they already do that) such as: NSFW (nudity, porn, brutality) and safe to view pictures.How can artificial neural networks achieve that, and at what success rate? Can they be easily mistaken?'	45	0	0	0
1479	-1	0	0	15	b'Do scientists or research experts know from the kitchen what is happening inside complex "deep" neural network with at least millions of connections firing at an instant? Do they understand the process behind this (e.g. what is happening inside and how it works exactly), or it is a subject of debate?For example this study says: However there is no clear understanding of why they perform so well, or how they might be improved.So does it mean the scientists actually doesn\'t know how complex convolutional network models work?'	86	0	0	0
1480	-1	0	0	-1	b'Is there any way to estimate how big the neural network would be after training session of 100,000 unlabeled images for unsupervised learning (like in STL-10 dataset: 96x96 pixels and color)?Not the storage space (because this could vary I guess based on the implementation), but specifically how many neurons it could have. It could be an estimate (e.g. in thousand, millions). If it depends, then on what? Are there any figures that can be estimated?'	74	0	0	0
1481	-1	0	0	4	b'For example I\'d like to train my neural network to recognize the type of actions (e.g. in commercial movies or some real life videos), so I can "ask" my network in which video or movie (and at what frames) somebody was driving a car, kissing, eating, was scared or was talking over the phone.What are the current successful approaches to that type of problem?'	63	0	0	0
1482	1462	0	44678	0	b"An AI is a computer program designed for tasks normally requiring human intelligence (a human's ability to learn), while a robot is a machine that completes complex tasks. An AI could be used to control a robot, but they are very different.Source: Oxford English Dictionary, above links will direct to definitions."	50	0	4	0
1483	1461	1	46182	10	b'Siri and co. are AI to some extent. The usual label is "Weak AI" (also called "narrow" or "soft" AI). It turns out the Wikipedia article on Weak AI explicitly refers to Siri: Siri is a good example of narrow intelligence. Siri operates within a limited pre-defined range, there is no genuine intelligence, no self-awareness, no life despite being a sophisticated example of weak AI. In Forbes (2011), Ted Greenwald wrote: "The iPhone/Siri marriage represents the arrival of hybrid AI, combining several narrow AI techniques plus access to massive data in the cloud." AI researcher Ben Goertzel, on his blog in 2010, stated Siri was "VERY narrow and brittle" evidenced by annoying results if you ask questions outside the limits of the application.Important to note that "mixing" Weak AIs does not make a "stronger" AI, by some arguments (see Searle\'s Chinese Room argument), but there is no definitive answer yet in 2016.'	151	0	0	0
1484	-1	0	0	2	b"I'm playing with an LSTM to generate text. In particular, this one:https://raw.githubusercontent.com/fchollet/keras/master/examples/lstm_text_generation.pyIt works on quite a big demo text set from Nietzsche and says If you try this script on new data, make sure your corpus has at least ~100k characters. ~1M is better.This pops up a couple of questions.A.) If all I want is an AI with a very limited vocabulary where the generate text should be short sentences following a basic pattern.E.g.I like blue sky with white cloudsI like yellow fields with some treesI like big cities with lots of bars...Would it then be reasonable to use a much much smaller dataset?B.) If the dataset really needs to be that big. What if I just repeat the text over and over to reach the recommended minimum? If that would work though, I'd be wondering how that is any different from just taking more iterations of learning with the same shorter text?Obviously I can play with these two questions myself and in fact I am experimenting with it. One thing I already figured out is that with a shorter text following a basic pattern I can get to a very very low ( ~0.04) quite fast but the predicted text just turns out as gibberish.My naive explanation for that would be that there are just not enough samples to proof against whether the gibberish actually makes sense or not? But then again I wonder if more iterations or duplicating the content would actually help.I'm trying to experiment with these questions myself so please don't think I'm just too lazy and are aiming for others to do the work. I'm just looking for more experienced people to give me a better understanding of the mechanics that influence these things."	288	0	0	0
1485	-1	0	0	0	b"For example I would like to implement transparent AI in the RTS game which doesn't offer any AI API (like old games), and I'd like to use image recognition algorithm for detecting the objects which can talks to another algorithm which is responsible for the logic.Given I'd like to use two neural networks, what are the approaches to setup the communication between them? Is it just by exporting result findings of the first algorithm (e.g. using CNN) with list of features which were found on the screen, then use it as input for another network? Or it's more complex than that, or I need to have more than two networks?"	109	0	0	0
1486	1479	1	25236	12	b'It depends on what you mean by "know what is happening".Conceptually, yes: ANN perform nonlinear regression. The actual expression represented by the weight matrix/activation function(s) of an ANN can be explicitly expanded in symbolic form (e.g. containing sub-expressions such as 1/1+e^{1/1+e^{...}}).However, if by \'know\' you mean predicting the output of some specific (black box) ANN, by some other means, then the obstacle is the presence of chaos in a ANN that has high degrees of freedom.EDIT: Here\'s some relatively recent work by Hod Lipson on understanding ANNs through visualisation.'	88	0	2	0
1487	-1	0	0	1	b'Were there any successful attempts to replace poor guide dogs used for blind people with AI to achieve similar rate of success? I guess dogs could be easily distracted and not reliable for every situation, and it probably takes less time to train AI, than a dog.'	46	0	0	0
1488	-1	0	0	5	b"Do we know why Tesla's Autopilot mistaken empty sky with a high-sided lorry which resulted in fatal crash involving a car in self-drive mode? Was it AI fault or something else? Is there any technical explanation behind this why this happened?References: Sky News article, The Verge."	45	0	2	0
1489	1479	0	29974	5	b'Not sure if this is what you are searching for, but google extracted images from networks when they were fed with white noise. See here.This kind of represents what the network knows.'	31	0	0	0
1490	-1	0	0	0	b'For benefits of testing AGI, is using a high-level video game description language (VGDL) gives more reliable and accurate results of general intelligence than using Arcade Learning Environment (ALE)?'	28	0	0	0
1491	-1	0	0	1	b'Some time ago playing chess was challenging for algorithms, then Go game which is vastly more complex than compared to chess.How about playing RTS game which have enormous branching factors limited by its time and space (like deciding what to do next)? What are the successful approaches to such problems?'	49	0	0	0
1492	-1	0	0	0	b'We can read on wiki page that in March 2016 AlphaGo AI lost its game (1 of 5) to Lee Sedol, a professional Go player. One article cite says: AlphaGo lost a game and we as researchers want to explore that and find out what went wrong. We need to figure out what its weaknesses are and try to improve it.Have researchers already figured it out what went wrong?'	68	0	1	0
1493	-1	0	0	2	b"Assuming we're dealing with artificial neural network (e.g. using convnets) which was trained by large dataset of human faces.Are there any known issues or challenges where facial recognition would fail? I'm not talking about covering half of the face, but some simple common things such as wearing the glasses, hat, jewellery, having face painting or tattoo, can this successfully prevent AI from recognizing the face? If so, what are current methods dealing with such challenges?"	74	0	0	0
1494	-1	0	0	0	b"I would like to know what kind of dataset I need (to prepare) for training the network to recognize the spelling mistakes in individual words for English text.Given the large database of words, having correct one for each incorrect. What kind of input is more efficient for that tasks? Is it using one input per each letter, syllable, whole word or I should use different pattern syllable?Then the input should be incorrect word, output correct, and if the word doesn't need correction, then both input and output should be the same. Is that the right approach?"	95	0	0	0
1495	1492	1	12754	3	b"We know what Lee's strategy was during the game, and it seems like the sort of thing that should work. Here's an article explaining it. Short version: yes, we know what went wrong, but probably not how to fix it yet.Basically, AlphaGo is good at making lots of small decisions well, and managing risk and uncertainty better than humans can. One of the things that's surprising about it relative to previous bots that play Go is how good it was at tactical fights; in previous games, Lee had built a position that AlphaGo needed to attack, and then AlphaGo successfully attacked it.So in this game, Lee played the reverse strategy. Instead of trying to win many different influence battles, where AlphaGo had already shown it was stronger than him, he would set up one critical battle (incurring minor losses along the way), and then defeat it there, with ripple events that would settle the match in his favor.So what's the weakness of AlphaGo that allowed that to work? As I understand it, this is a fundamental limitation of Monte Carlo Tree Search (MCTS). MCTS works by randomly sampling game trees and averaging them; if 70% of games from a particular position go well and 30% of games from another position go well, then you should probably play the first move instead of the second move.But when there's a specific sequence of plays that go well--if, say, W has a path that requires them playing exactly the right stone each time, but B has no possible response to this path--then MCTS breaks down, because you can only find that narrow path through minimax reasoning, and moving from the slower minimax reasoning to the faster MCTS is one of the big reasons why bots are better now than they were in the past.It's unclear how to get around this. There may be a way to notice this sort of threat, and then temporarily switch from MCTS reasoning to minimax reasoning, or to keep around particular trajectories in memory for consideration in future plays. "	341	0	0	0
1496	1493	1	23614	2	b"Facial recognition works by essentially turning your face into a point cloud, recognizing eyes, cheeks, nose, mouth, etc. Unfortunately it doesn't look at the top of your head (hair is very hard to differentiate from other hair and doesn't have many features). Face paintings would be your best bet since they can be easily changed, tattoos not so much. Once somebody has a photo of your face with your tattoo on it, you're busted. Glasses will work if they're opaque and hide your eyes (sunglasses). The facial recognition software does not recognize jewelry, as it's tiny, and very easy to remove and put on. Ideally you want to have your face professionally made up with makeup and fake skin (basically a fake face).Some challenges can be anything like an unclear picture, a picture from the wrong angle (which can probably be mathematically calculated and restructured), or like you said, face paintings to hide facial features."	154	0	0	0
1497	1494	1	18992	2	b"I'd personally be more inclined to try longstanding deterministic methods such as Damerau (for typing errors) or Soundex (for homonyms arising from transcribed speech). At the very least, I'd use those as a baseline for any more 'AI-based' approach."	38	0	0	0
1498	1488	0	35344	3	b"As far as I know, Tesla cars autopilot is not a 100% AI pilot, it's an assitant: as it detects hands off wheel it slows down, so it's incorrect to speak about AI mistake: it is not trained/designed to drive a car all by itself. A human driver is responsible in that incident."	52	0	0	0
1499	1490	1	35014	2	b"Here is a description of the input to an ALE agent:Percept state: A single game screen (frame): a 2D array of 7-bit pixels, 160 pixels wide by 210 pixels high. Actions: 18 discrete actions defined by the joystick controllerRegarding VGDL, as far as I can see, the main site associated with it is gvgai.net, which is currently down. The associated API is described in this paper.Percept state for GVGAI is more structured than for ALE, but the closest correspondence to ALE appears to be an 'Observation grid', consisting of a 2D array of sprite identifiers.Actions: ACTION_NIL, ACTION_UP, ACTION_LEFT, ACTION_DOWN, ACTION_RIGHT and ACTION_USE (stated as 'typical' values). Of the two, it would seem that ALE is more suitable for AGI, because of the more 'free form' nature of the input.However, one of the issues with either of these approaches is that the set of possible actions is strongly constrained. These domains are therefore 'operationalised' - the hard task of working out what actions are possible has already been solved for the AI by the API, effectively acting as a bottleneck on the complexity of mapping from input to output.A range of alternative game-playing frameworks are listed here and one alternative (which I personally believe is more useful for AGI purposes) is the Artificial Text Adventurer, in which (at each turn) agent is presented with natural language input describing the scene and must then output a command in natural language. Disclaimer: I am associated with this competition."	244	0	4	0
1500	1494	0	21353	2	b'I would also look at Minimum Edit Distances such as the Levenshtein distance.You could use a dynamic programming technique such as the Viterbi Algorithm.If you don\'t have a dictionary to work against, you may want to train with a Markov Chain model using a known "good" text. The Viterbi Algorithm could be used again to solve the model for the text being considered.'	62	0	0	0
1501	-1	0	0	2	b'As I have been looking at other questions on this site (like this, this, this, and this), I have been thinking more about the ethical implications of creating these generalized AI systems. It seems that whether or not we can create it is not rationale enough as to whether or not we should do it.In dealing with the issue of ethics in AI, I wonder what the ethical implications are not just for us, but for the system itself. It seems to extend beyond the usually asked questions on the topic and into unknown territory. Are ethics computable? Can they be implemented programmatically? Can we force an AI system to do something against its "will"?What does the creation of AI imply ethically for us as well as the AI?'	128	0	4	0
1502	1501	0	2955	0	b'I believe if an AI achieves sentience, it should be treated the same way we are required to treat any other sentient animal. This is belief though, there is no established ethics for AI. But there were no ethics for animals a couple of centuries ago.'	45	0	0	0
1503	1487	1	45416	4	b"Chieko Asakawa (wiki, TED, IBM) is a major researcher in this area, and the linked TED talk is probably a good introduction to the state of the art as of 2015. Here's a link to a 2016 paper on a smartphone navigation system.Guide animals perform manipulation tasks as well as identification tasks, and so it's not clear if those could be replaced well at all. (A smartphone that reads a label is a great help, but a dog that knows which bottle to grab and deliver to you is probably a much better help.)"	93	0	2	0
1507	-1	0	0	9	b"I believe artificial intelligence (AI) term is overused nowadays.For example people see that something is self-moving and they call it AI, even if it's on autopilot (like cars or planes) or there is some simple algorithm behind it.What are the minimum general requirements so that we can say something is AI?"	50	0	0	0
1508	-1	0	0	0	b"I believe normally you can use genetic programming for sorting, however I'd like to check whether it's possible using ANN.Given the unsorted text data from input, which neural network is suitable for doing sorting tasks?"	34	0	0	0
1509	-1	0	0	1	b"I've read on wiki that genetic programming has 'outstanding results' in cyberterrorism prevention.Further more, this abstract says: Using machine-coded linear genomes and a homologous crossover operator in genetic programming, promising results were achieved in detecting malicious intrusions.I've checked the study, but it's still not clear for me.How exactly was this detection achieved from the technical perspective?"	55	0	1	0
1512	1462	0	27889	1	b'Basically a robot is a mechanical or virtual artificial agent which exhibit intelligent behavior (AI).Tim Urban on Wait But Why website wrote the following to clear things up: First, stop thinking of robots.  A robot is a container for AI,  sometimes mimicking the human form, sometimes not  \xe2\x80\x94 but the AI itself is the computer inside the robot.  AI is the brain and the robot is its body \xe2\x80\x94 if it even has a body.  For example,  the software and data behind Siri is AI, the woman\xe2\x80\x99s voice we hear is a personification of that AI, and there\xe2\x80\x99s no robot involved at all.Source: The AI Revolution: The Road to Superintelligence'	115	0	2	0
1513	1461	0	28946	2	b'They are virtual artificial agents which exhibit intelligent behavior (AI).Tim Urban on Wait But Why website wrote the following: The software and data behind Siri is AI, the woman\xe2\x80\x99s voice we hear is a personification of that AI, and there\xe2\x80\x99s no robot involved at all.Source: The AI Revolution: The Road to SuperintelligenceRelated: What is the difference between AI and robots?'	59	0	3	0
1514	1508	1	2856	2	b'Even a simple multilayer perceptron can sort input data to some extent, as you can see here and here.However, neural networks for sequential data seem more appropriate, as they can handle sequences of variable lengths. It has been done with an LSTM (Long Short-Term Memory), LSTM+HAM (Hierarchical Attentive Memory) and an NTM (Neural Turing Machine).'	54	0	1	0
1515	-1	0	0	1	b"On Wikipedia, we can read about different type of intelligent agents:abstract intelligent agents (AIA),autonomous intelligent agents,virtual intelligent agent (IVA), which I've found on other websites, e.g. this one.What are the differences between these three to avoid confusion?For example I've used term virtual artificial agent here as: Basically a robot is a mechanical or virtual artificial agent which exhibit intelligent behavior (AI).so basically I'd like to know where other terms like autonomous or abstract agents can be used and in what context. Can they be all defined under 'virtual' robot definition? How to distinguish these terms?"	94	0	1	0
1516	1507	1	5864	18	b'It\'s true that the term has become a buzzword, and is now widely used to a point of confusion - however if you look at the definition provided by Stuart Russell and Peter Norvig, they write it as follows: We define AI as the study of agents that receive percepts from the  environment and perform actions. Each such agent implements a function that maps percept sequences to actions, and we cover different ways to represent these functions, such as reactive agents, real-time planners, and decision-theoretic systems. We explain the role of learning as extending the reach of the designer into unknown environments, and we show how that role constrains agent design, favoring explicit knowledge representation and reasoning.Artificial Intelligence: A Modern Approach - Stuart Russell and Peter NorvigSo the example you cite, "autopilot for cars/planes", is actually a (famous) form of AI as it has to use a form of knowledge representation to deal with unknown environments and circumstances. Ultimately, these systems also collect data so that the knowledge representation can be updated to deal with the new inputs that they have found. They do this with autopilot for cars all the timeSo, directly to your question, for something to be considered as "having AI", it needs to be able to deal with unknown environments/circumstances in order to achieve its objective/goal, and render knowledge in a manner that provides for new learning/information to be added easily. There are many different types of well defined knowledge representation methods, ranging from the popular neural net, through to probabilistic models like bayesian networks (belief networks) - but fundamentally actions by the system must be derived from whichever representation of knowledge you choose for it to be considered as AI.'	285	0	3	0
1517	-1	0	0	2	b'On Wikipedia we can read: Kasparov accused IBM of cheating and demanded a rematch. IBM refused and retired Deep Blue.What was the accusation and how was Deep Blue allegedly able to cheat?'	31	0	0	0
1518	-1	0	0	0	b"The Wikipedia page describes AI control problem in very intricated way.Therefore I would like to better understand it based on some simple explanation, what's going on.Basically I don't want any copy &amp; pastes from wiki, because the articles there are written in neutral point of view, in very general way where articles are evolving very slowly, so the definition from there doesn't suit me.I believe this is what is discussed nowadays by government and it's important aspects of AI technology where it leds to.I believe this could be a big problem in the near future, so I'm expecting to hear about this from people from much better and more up-to-date point of view.So what is exactly the AI Control Problem?"	119	0	0	0
1520	1509	1	16822	2	b"They treated it as a classification problem. While it's common to use some variety of Neural Nets (NNs) to build classifiers, Genetic Programming (GP) can also be used for this purpose. In contrast to NN classifiers, GP can use a wider range of operations (e.g. if,while,logical statements,arbitrary mathematical functions etc) to perform the classification than weighted arithmetic expressions involving an activation function. Whether or not this is actually of benefit depends on the specific application.In addition, the abstract implies their algorithm is adaptive (i.e. responds in some fashion to the nature of incoming attacks), which would most easily be achieved by continuing to run the GP program in the background to monitor potential intrusions."	113	0	1	0
1522	1488	1	85936	4	b'Tesla\'s technology is assistive, as Alexey points out, so this is not a case of an autonomous system (e.g. an AGI) doing some fatal stunt (the product name AutoPilot is famously misleading). Now on why the car assistance led to this tragic accident, there is some information related to AI technologies.Warning: I cannot find again the source critical to the next paragraph, and reading again pages over pages, I cannot find similar argument in other reports. I still remember vividly the point below, but please keep in mind it may be incorrect. The rest of the answer is weakly related, so I leave it all, with this warning.An independent report (link needed, I can\'t find it...) explained that the assistive system was unable to detect the truck due to an exceptionally low contrast (bright sky perceived as white---colour of the truck). The report also said that a human driver would have been unable to make the difference either. In other words, it is possible that car sensors (presumably camera) and the human eye could not have detected an obstacle, and could not have triggered any safety measure. This short graphical explanation sums up the car sensors: Camera, radar, GPS, etc.The assistive sub-system is based on proprietary AI technologies. We can only speculate under some hypothesis. _This is not very useful, honestly, except for illustration purpose. Assuming the assistive system relies on ML technologies to learn about obstacles from a video stream (such systems do exist):It may be that the learning data was not "good enough" to cover the truck scenario.It may be the technology was not powerful enough yet (lack generalization power, or simply too slow).It may be a hardware problem, notably from the sensors: If the "car\'s eyes" are defective, the "car\'s brain" (the assistive system) is unable to react properly.Why those technologies did not work in that case will remain a secret. We can say however that any system---whether built with AI technology or not---has limits. Beyond these limits, the system reaction is unpredictable: It could stop, reset, shutdown. The difficulty here is to define what a "default behaviour" is. A machine will basically do whatever it is designed to do, so an AI-based system too.We could speculate even more on what would happen if the assistive system was really autonomous, the elusive AGI, but that is really not the case here.'	393	0	1	0
1523	1485	1	8540	2	b"The underlying abstraction (which is essentially what you'd be using the first network for) is that of reducing the state-space of the raw input via feature extraction/synthesis and/or dimensionality reduction.At present, there are few definite rules for doing this: practice is more a question of 'informed trial and error'. If you add some information to your question regarding what has been previously attempted in this area (e.g. on the ALE platform), this it might be possible to offer some more specific advice."	81	0	1	0
1524	1517	1	39970	7	b'The allegation was based on the fact that Deep Blue made a choice that did not yield the immediate (or short term) benefit that was synonymous with systems back then (1997). Computational capability was significantly less powerful then, and Kasparov claimed that only a grand master would have made the decision that the system did - so the deep blue team cheated by having a human perform the move instead of the system.He asked for a rematch, but IBM did not allow this, which only added to the suspicion.This is a great article with deep analysis on the specific moves and circumstances - however suffice to say that Kasparov was trying to bait the system into making a decision for a weak pawn, but the system chose otherwise and instead put Kasparov into a compromised position:https://en.chessbase.com/post/deep-blue-s-cheating-move'	135	0	0	0
1525	-1	0	0	10	b'This is from a closed beta for AI, with this question being posted by user number 47. All credit to them. According to Wikipedia, Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets.Both are recurrent neural networks that can be trained to learn of bit patterns. Then when presented with a partial pattern, the net will retrieve the full complete pattern.Hopfield networks have been proven to have a capacity of 0.138 (e.g. approximately 138 bit vectors can be recalled from storage for every 1000 nodes, Hertz 1991).As a Boltzmann machine is stochastic, my understanding is that it would not necessarily always show the same pattern when the energy difference between one stored pattern and another is similar. But because of this stochasticity, maybe it allows for denser pattern storage but without the guarantee that you\'ll always get the "closest" pattern in terms of energy difference. Would this be true? Or would a Hopfield net be able to store more patterns?'	163	0	0	0
1526	191	1	23485	3	b"The well-known 'Eliza' program (Weizenbaum, ~1964) would appear to be the first. Eliza was designed to model the emotionally-neutral response of a psychotherapist and this masks some of the weaknesses of its limited underlying pattern-matching mechanisms. "	36	0	0	0
1527	1478	1	47661	3	b'The 2015 paper entitled "Applying deep learning to classify pornographic images and videos" applied various types of convnets for detecting pornography. The proposed architecture achieved 94.1% accuracy on the NPDI dataset, which contains 800 videos (400 porn, 200 non-porn "easy" and 200 non-porn "difficult"). More traditional computer vision methods achieved 90.9% accuracy. The proposed architecture also performs very well regarding the ROC curve.There does not seem to exist any works regarding the other aspects of NSFW yet.'	76	0	0	0
1528	1518	1	46085	6	b"The Control Problem is, in short, the idea that AI will eventually be much better decision-makers than humans. If we don't set things up correctly beforehand, we won't get a chance to fix it afterwards, because AI will have effective control.There are three main areas of discussion with regards to the Control Problem:Whether or not the problem is urgent. Many AI experts, cognizant of the difficulty of getting simple systems to work effectively today, think that AI able to take control is not urgent, and as detail-minded engineers, they think it will be profoundly difficult to do any useful work today. (Andrew Ng, for example, famously called these sorts of worries like worrying about overpopulation on Mars.) Given radical uncertainty among AI experts as to when this will become an issue, however, this means we can't rule out rapid AI timescales, and should do at least some work in anticipation of those timescales.Whether or not the problem is hard. Many people give short, simple, and wrong solutions to the control problem. Probably the most famous is the idea that intelligence and morality are inherently interlinked, and thus a more intelligent machine, by definition, will be more moral. The Orthogonality Thesis is the claim of the opposite, that intelligence and morality (or, more specifically, goal alignment) are unrelated things.What foundations we can lay now. There are a bunch of open problems (see, for example, MIRI's technical agenda) that deal with mathematical logic of the sort that would be useful for ensuring robust value alignment, or on how to effectively do value learning (without giving an incentive to distort values), or on how to build value functions and goals such that they are fixable if they turn out to be mistaken. Those look like problems that we can do useful work on now, even without knowing what the actual structure of a future AI will look like."	314	0	0	0
1529	1515	1	49764	2	b'I don\'t think there are many contexts where there is any really meaningful distinction between these terms. Even in the WP article you refer to, it is shown that "abstract intelligent agent" and "autonomous intelligent agent" are generally just synonyms for "intelligent agent" but used to highlight certain aspects of intelligent agents in some contexts. Net-net, I\'d say there just isn\'t any difference there that\'s going to matter in practice."Virtual intelligent agent" OTOH, used in the context you used it, suggests the distinction between an IA that\'s implemented in software only, versus one that has a physical manifestation. I don\'t know how useful that distinction is and I haven\'t seen anybody else make it. All in all, I expect that in almost every possible context, if you just say "Intelligent Agent" with no qualifiers, that\'s going to be sufficient. But if there were going to be an exception, I\'d say it would be around the term "autonomous" since an agent which is truly autonomous, versus one that needs to operate in a specific, constrained environment, is a distinction that - at least in principle - could be useful. '	188	0	0	0
1530	1479	0	50071	3	b'I\'m afraid I don\'t have the specific citations handy, but I have seen/heard quotes by experts like Andrew Ng and Geoffrey Hinton where they clearly say that we do not really understand neural networks. That is, we understand something of the how they work (for example, the math behind back propagation) but we don\'t really understand why they work. It\'s sort of a subtle distinction, but the point is that no, we don\'t understand the very deepest details of how exactly you go from a bunch of weights, to, say, recognizing a cat playing with a ball. At least in terms of image recognition, the best explanation I\'ve heard is that successive layers of a neural network learn more sophisticated features, composed of the more granular features from earlier levels. That is to say, the first layer might recognize "edges" or "straight lines". The next layer might then learn geometric shapes like "box", or "triangle", and then a higher layer might learn "nose" or "eye" based on those earlier features, and then a higher level layer still learns "face" made up from "eye", "nose", "jaw", etc. But even that, as I understand it, is still hypothetical and/or not understood in complete detail. '	202	0	0	0
1531	-1	0	0	7	b'According to Wikipedia, Prolog is a general-purpose logic programming language associated with artificial intelligence and computational linguistics.Is it still used for AI?This is based off of a question on the 2014 closed beta. The author had the UID of 330.'	39	0	1	0
1532	1531	1	22836	4	b'Remembering that artificial intelligence has been an academic endeavour for the longest time, Prolog was amongst one of the early languages used as part of the study and implementation of it. It has rarely made its way into large commercial applications, having said that, a famous commercial implementation is in Watson, where prolog is used for NLP.The University of Edinburgh contributed to the language and it was sometimes referred to as "Edinburgh Prolog". It is still used in academic teachings there as part of the artificial intelligence course.The reason why Prolog is considered powerful in AI is because the language allows for easy management of recursive methods, and pattern matching.To quote Adam Lally from the IBM Thomas J. Watson Research Center, and Paul Fodor from Stony Brook University: the Prolog language is very expressive allowing recursive rules to represent reachability in parse trees and the operation of negation-as-failure to check the absence of conditions.'	153	0	5	0
1533	92	0	36191	2	b"The neural networks can be easily fooled or hacked by adding certain structured noise in image space (Szegedy 2013, Nguyen 2014) due to ignoring non-discriminative information in their input.For example: Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs.2015So basically the high confidence prediction in certain models exists due to a 'combination of their locally linear nature and high-dimensional input space'.2015Published as a conference paper at ICLR 2015 (work by Dai) suggest that transferring discriminatively trained parameters to generative models, could be a great area for further improvements."	99	0	4	0
1534	-1	0	0	5	b"I'm a bit confused with extensive number of different Monte Carlo methods such as:Hamiltonian/Hybrid Monte Carlo (HMC),Dynamic Monte Carlo (DMC),Markov chain Monte Carlo (MCMC),Kinetic Monte Carlo (KMC),Dynamic Monte Carlo (DMC)Quasi-Monte Carlo (QMC),Direct Simulation Monte Carlo (DSMC),and so on.I won't ask for the exact differences, but why are all of them called Monte Carlo? What do they all have in common? Can they all be used for AI? E.g. which one can be used for gaming (like Go) or image recognition (resampling)?"	80	0	0	0
1535	-1	0	0	3	b"When it comes to neural networks, it's often only explained what abstract task they do, say for example detect a number in an image. I never understood what's going on under the hood essentially.There seems to be a common structure of a directed graph, with values in each node. Some nodes are input nodes. Their values can be set. The values of subsequent nodes are then calculated based on those along the edges of the graph until the values for the output nodes are set, which can be interpreted a result.How exactly is the value of each node determined? I assume that some formula is associated with each node that takes all incoming nodes as input to calculate the value of the node. What formula is used? Is the formula the same throughout the network?Then I heard that a network has to be trained. I assume that such training would be the process to assign values to coefficients of the formulas used to determine the node values. Is that correct?In layman's terms, what are the underlying principles that make a neural network work?"	182	0	0	0
1536	1535	0	1784	3	b'I will overly simplify ANNs in order to point how they work. Examples might not be 100% accurate.In the simplest form, network is trained using the apriori information extracted from the ground truth. This basically means that ANN uses the relation between the input and output. For instance, if you are to classify shrubs and trees, one of the input could be height and the other could be the width of the tree. Now, if you have only input and output layers, increasing height means increasing chance for the object to be a tree. Thus, input height would have a positive weight connecting to tree output and a negative weight to shrub output. However, as the plant gets wider, the chance of it being a shrub increases. Taller shrubs are wider than shorter ones. Thus input weight would have positive weight connecting to the shrub output. Finally, the chance of being a tree is not affected by the width and thus will have close to 0 weight between this input and output. This network will effectively work like a linear discriminant classifier.Now instead of assigning weights by hand, you may use a learning algorithm that tries to adjust weights so that the output is correct when the series of input is supplied. Ideally this training algorithm should reach to the conclusion that we have made in the previous example. Most training algorithms are recursive. They supply the inputs multiple times, and in a simple sense, they reward pathways that are correct by increasing their weights and punishes pathways that are causing incorrect answer.When hidden layers are used in a system, they would be able to correlate input on higher degrees. Thus, as the number of layers get higher, ANN learns the input set much better. However, this does not mean it gets better. If the ANN over fits the input set, it would be affected from the random noise that is in the dataset. This problem is generally referred as memorization. There are learning algorithms that try to minimize memorization and maximize generalization ability. But ultimately, the number of training samples should be high enough so that ANN cannot overfit to the data.'	362	0	0	0
1537	-1	0	0	0	b'Ideally I\'d like to watch movie which is deep dreamed in real-time. Most algorithms which I know are too slow or not designed for real-time processing.For example I\'m bored with some movie which I\'ve watched thousands of time and I\'d like to add some "dreaming" to it which is real-time filter which takes input frames, then it\'s processing and enhances the images through artificial neural network to achieve doodled output.Doesn\'t have to be exactly DeepDream or hallucinogenic technique (which could be too much to watch for 2h), but with any similar ANN algorithm. I\'m more interested into achieving desired real-time use.What kind of techniques can achieve such efficiency?'	107	0	0	0
1539	-1	0	0	7	b'How does employing evolutionary algorithms to design and train artificial neural networks have advantages over using the conventional backpropagation algorithms?'	19	0	0	0
1540	-1	0	0	0	b"Are there any existing approaches for using artificial neural networks (ANN) or evolutionary algorithm (EA) for detecting coding standard violations? Which one would be more suitable?I don't have any specific programming language in mind, but something similar to PHP_CodeSniffer (following these standards), but instead of using hardcoded rules, the algorithm should learn good techniques, but I'm not sure based on what training data. How would you approach the training session, any suggestions?"	71	0	1	0
1541	-1	0	0	5	b"Genetic Algorithms has come to my attention recently when trying to correct/improve computer opponents for turn-based strategy computer games.I implemented a simple Genetic Algorithm that didn't use any cross-over, just some random mutation. It seemed to work in this case, and so I started thinking:Why is cross-over a part of genetic algorithms? Wouldn't mutation be enough?This is from a data dump on an old AI site. The asker had the UID of 7. "	73	0	0	0
1544	-1	0	0	3	b'While thinking about AI, this question came into my mind. Could curiosity help in developing a true AI? According to this website (for testing creativity): Curiosity refers to persistent desire to learn and discover new things and ideasLet\'s take Clarifai, a image/video classification startup which can classify images and video with the best accuracy (according to them). If I understand correctly, they trained their deep learning system using millions of images with supervised learning. In the same algorithm, what would happen if we somehow added a "curiosity factor" when the AI has difficulty in classifying a image or its objects? It would ask a human for help, just like a curious child. Curiosity makes a human being learn new things and also helps to generate new original ideas. Could the addition of curiosity change Clarifai into a true AI?'	138	5	1	0
1545	1539	1	15910	6	b'Unlike backpropagation, evolutionary algorithms do not require the objective function to be differential with respect to the parameters you aim to optimize. As a result, you can optimize "more things" in the network, such as activation functions or number of layers, which wouldn\'t be possible in the standard backpropagation. Another advantage is that by defining the mutation and crossover functions, you can influence how the parameter search space should be explored.'	70	0	0	0
1546	1541	0	2677	5	b'Crossover allows to combine two parents (vs. mutation, which only uses one parent). In some cases, it is useful (e.g., if you train a FPS bot, if one parent is good at shooting and another parent is good at moving, it makes sense to combine them). In some other cases, it is not.'	52	0	0	0
1547	1544	0	750	5	b" when the AI has difficulty in classifying a image or its objects it should ask a human for help just like a curious childIt's called active learning, it's already used quite often."	32	0	0	0
1548	1541	1	4720	5	b'Mutation is usually defined to be a global operator, i.e. iterated mutation is (eventually) capable of reaching every point in the vector space defined by the geneome. In that sense, mutation alone is certainly \'enough\'.Regarding the motivation for crossover - from Essentials of Metaheuristics, p42: Crossover was originally based on the premise that highly fit individuals often share certain traits, called building blocks, in common. For example, in the boolean individual 10110101, perhaps ***101*1 might be a building block (where * means "either 0 or 1") So the idea is that crossover works by spreading building blocks quickly throughout the population. Crossover methods also assume that there is some degree of linkage between genes on the chromosome: that is, settings for certain genes in groups are strongly correlated to fitness improvement. For example, genes A and B might contribute to fitness only when they\xe2\x80\x99re both set to 1: if either is set to 0, then the fact that the other is set to 1 doesn\xe2\x80\x99t do anything.Also note that crossover is not a global operator. If the only operator is crossover then (also from p42): Eventually the population will converge, and often (unfortunately) prematurely converge, to copies of the same individual. At this stage there\xe2\x80\x99s no escape: when an individual crosses over with itself, nothing new is generated.For this reason, crossover is generally used together with some global mutation operator.'	229	0	0	0
1551	1541	0	5039	2	b"When thinking about crossover its important to think about the fitness landscape. Consider a hypothetical scenario where we are applying a genetic algorithm to find a solution that performs well at 2 tasks. This could be from Franck's example (moving and shooting) for an AI, or perhaps it could be predicted 2 outputs in a genetic machine learning scenario, but really most scenarios where GAs are applied are synonymous (even at solving a single task, there may be different aspects of the task to be addressed).Suppose we had an individual, 1, that was performing reasonably well at both tasks, and we found a series of mutations which produced 2 new individuals, 2 and 3, which performed better than Individual 1 at tasks 1 and 2 respectively. Now while both of these are improvements, ideally we want to find a generally good solution, so we want to combine the features that we have been found to be beneficial. This is where crossover comes in; by combining the genomes of Individuals 2 and 3, we may find some new individual which produces a mixture of their performances. While it is possible that such an individual could be produced by a series of mutations applied to Individual 2 or Individual 3, the landscape may simply not suit this (there may be no favorable mutations in that direction, for example).You are partially right therefore; it may sometimes be the case that the benefits of crossover could be replicated with a series of mutations. Sometimes this may not be the case and crossover may smooth the fitness landscape of your GA, speeding up optimization and helping your GA escape local optima. "	276	0	0	0
1552	4	0	83322	5	b'For a more intelligent approach than random or exhaustive searches, you could try a genetic algorithm such as NEAT . However, this has no guarantee to find a global optima, it is simply an optimization algorithm based on performance and is therefore vulnerable to getting stuck in a local optima. '	50	0	1	0
1553	1544	0	3505	5	b' Does this addition of curosity changes clarifai into a true AI?As per my answer to this question, we don\'t know what the ingredients for a \'true AI\' are. Via the Turing Test and its variants, the best we can do is "know one when we see one".Curiosity certainly appears necessary for intelligence, though it doesn\'t seem sufficient - a lemming-like creature curious to see what\'s at the bottom of a steep cliff might not survive long enough to learn caution, even if it had the learning mechanisms to do so.Here is some work by Schmidhuber on Artificial Curiousity. Pierre-Yves Oudeyer has also done quite a lot of work on this and Active Learning/Intrinsic motivation.'	114	0	3	0
1560	-1	0	0	0	b"Based on this article, Google's self-driving cars can spot cyclists, cars, road signs, markings, traffic lights, and pedestrians.How exactly does it identify pedestrians? Is it based on face recognition, shape, size, distance, infrared signature?"	33	0	1	0
1561	-1	0	0	9	b'In Hidden Obstacles for Google\xe2\x80\x99s Self-Driving Cars article we can read that: Google\xe2\x80\x99s cars can detect and respond to stop signs that aren\xe2\x80\x99t on its map, a feature that was introduced to deal with temporary signs used at construction sites.  Google says that its cars can identify almost all unmapped stop signs, and would remain safe if they miss a sign because the vehicles are always looking out for traffic, pedestrians and other obstacles.What would happen if a car spotted somebody in front of it (but not on the collision path) wearing a T-shirt that has a stop sign printed on it. Would it react and stop the car?'	109	0	0	0
1562	1540	1	28623	3	b'If the system claims that a piece of code has violated standards, then to be useful to the programmer, it really needs to provide more information than just a \'yes/no\' classifier: you need some form of explanation about why it is claimed to be wrong.Clearly ANNs aren\'t much use for that.If I were tackling such a problem (and my suspicion is that a lot of effort could be spent trying and failing to reproduce coding standards which are already well-understood), then my inclination would be to use a more explicitly rule-based representation.Possibilities include:Genetic ProgrammingLearning Classifier Systems Decision TreesThe ever-useful "Essentials of Metaheuristics" has a whole section on the evolution of rulesets. Obviously, nothing prevents you from initializing the evolutionary process with rules known to be useful.As I point out here, with our current AI algorithms, the success of an approach is very sensitive to human expertise/effort in feature selection/preprocessing, choice of training set etc, so creative experiment with this is vital.Training set: how about two sets of negative and positive examples, consisting of (features extracted from) bad code and from a refactored version (respectively)?One elementary choice of features would be to apply a bunch of code complexity metrics and have the learning algorithm combine those. The plus side of working in such a numeric domain is that the learning algorithm might readily find a gradient to exploit. The downside is that the rules (which are then likely of the form if mcabe &gt; 2.8 etc) are still not as informative as might be desired.For more complex rules (e.g. requiring if elseif else) you may want to extract your features from the abstract syntax tree. You could in principle use the entire tree but to my knowledge ML on graph and tree structures is still in relative infancy.'	296	0	2	0
1567	-1	0	0	1	b' This is a scope experiment. After Google/Tesla/whoever else is making self-driving cars finishes perfecting them, will they replace the cars with human drivers, so that there are only self-driving cars?If they do, it would probably make the roads safer.'	39	0	0	0
1568	-1	0	0	3	b'Significant AI vs human board game matches include:chess: Deep Blue vs Kasparov in 1996,Go: DeepMind AlphaGo vs Lee Sedol in 2016,which demonstrated that AI challenged and defeated professional players.Are there known board games left where a human can still win against an AI? I mean based on the final outcome of authoritative famous matches, where there is still same board game where AI cannot beat a world champion of that game.'	70	0	0	0
1569	1537	1	41780	1	b"Most of the algorithms (based on image synthesis and style transfer, e.g. neural-doodle) haven't been proven to be highly effective in terms of real-time image processing.However the following studies discusses such algorithms for real-time texture synthesis:Feed-forward Synthesis of Textures and Stylized ImagesThe approach is to move the computational burden to a learning stage, making trained network (CNN) light-weight and compact in order to generate multiple samples of the same texture. This can generate textures as good as comparable to Gatys~et~al, but significantly faster.Perceptual Losses for Real-Time Style Transfer and Super-ResolutionThis method uses parallel work which can generate high-quality images by defining and optimizing loss functions based on high-level features extracted from pretrained networks.Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial NetworksThis uses precomputed feed-forward networks that captures the feature statistics of Markovian patches in order to generate outputs of arbitrary dimensions. This can be applied to texture synthesis, style transfer and video stylization.Source: Above list suggested on neural-doodle project."	158	0	6	0
1570	-1	0	0	6	b'I\'m trying to teach an AI different pattern of tic tac toe to recognize wether a given pattern represents a win or not.Unfortunately it\'s not learning to recognize them correctly and I think may way of representing/encoding the game into vectors is wrong.I choose a way that is easy for an human (me, in particular!) to make sense of:This basically just use an array of length 9 to represent a 3 x 3 board. The first three items represent the first row, the next three the second row and so on. The line breaks should make it obvious I guess.The target data then maps the first two game states to "no wins" and the last two game states to "wins".Then I wanted to create some validation data that is slightly different to see if it generalizes.Obviously, again the last two game states should be "wins" whereas the first two should not.I tried to play with the number of neurons and learning rate but no matter what I try, my output looks pretty of. E.g.I tend to think it\'s the way how I represent the game state that may be wrong but actually I have no idea :DCan anyone help me out here?This is the entire code that I use'	208	72	0	0
1571	1568	0	5194	2	b'Artificially intelligent computer programs should be able to be at the same level or beat humans at every game that we play. This is because games follow rules that are scriptable, and artificial intelligence is designed to focus on one specific game and learn from its failures. The difference between humans and artificial intelligence is that artificial intelligence focuses on one specific task like learning to master Go while our brain is dedicated to mastering multiple tasks like...living. Even Arimaa, a game designed to be difficult for artificially intelligent systems was beaten by a bot called Sharp: https://en.wikipedia.org/wiki/Arimaa. '	98	0	1	0
1574	1568	0	9440	7	b'Not all games (or even board games) are computationally algorithmic. Even the least skilled player is likely to trounce the hottest pattern-matching algorithm in a game of Pictionary (for example).If you want to say that the movement of pieces upon successful completion of a task is only ancelary to the object of the game, than your answer will be largely self-selecting. A sufficiently sophisticated algorithm will brute force a computational problem better than human intuition&hellip; eventually.'	75	0	0	0
1575	111	0	15027	7	b'In the real world, decisions will be made based on the law, and as noted over on Law.SE, the law generally favors inaction over action. '	25	0	1	0
1576	1567	0	20734	0	b'It likely to be happen, because it\'s more convenient that way. In general people, organizations and government are always keen to make things more efficient by standarizing things (computers, technology, law, science, etc.) in order to make it manageable and predictable to reduce the time and minimalize the risk of the same mistakes.The whole world now moves into technological advancement where automation of everything is where we are going, so we can manage complexities in more reliable way, so we can focus on much bigger picture. This includes technology such as mobiles, computers, UAV (delivery drones), robots and now self-driving cars.The pros of that change would be:To have safer streets by introducing autonomous cars on the road.To have fewer drunk, tired, drugged or crazy drivers.To avoid poor weather conditions.To reduce braking distances by dropping driver\'s reaction time and predicting dangerous situations much earlier.Source: Cyber PhysicsReducing car deaths and costs of GNP. An estimated 1.3 million people die on the world\'s roads every year with around 50 million injured or disabled by accidents, with accidents costing countries up to four per cent of their Gross National Product (GNP) yearly. - UN News CentreTo have central point of safety improvements, you cannot change people, but you can fix the known safety issue on global scale.To introduce global standards from the central point (e.g. new law to which manufactures needs to apply).To increase car safety in general on larger scale.And so on.Why we need the \'only self-driving cars\'?The Secretary-General said the UN would work hard to prevent further deaths on the roads: Many tragedies can be avoided through a set of proven, simple measures that benefit not only individuals and families but society at large.Here are my points:To achieve \'a set of proven measures\' - do not allow people to drive - simple.People tend to break the rules, always, so do not allow them to drive without permission.Reduce stealing cars and other crime.Law enforcement dream is to able to stop any car on demand.Do not allow drunk people to drive a car.Disallow terrorist attacks, like in Nice where truck killed over 80 people.Avoid bank robberies and similar which are possible by escaping fast cars.Is it possible? I believe it depends on specific countries and unions and how quickly we\'re able to advance and be ready for such change.To support above points and summarize the \'only self-driving cars\' point, please see below references which shows that this is already happening:2014: Uber will eventually replace all its drivers with self-driving cars2016: Google\'s self-driving car system has been officially recognised as a driver in the US. The move is seen as a first step towards changing the law for cars that have "no need for a human driver".2016: Beverly Hills to replace public transport with self-driving cars2016: San Francisco pitches $149 million plan to replace cars with self-driving vehicles San Francisco\xe2\x80\x99s future is autonomous and shared vehicles \xe2\x80\x93 and that future may be only a decade away.2016: Otto Self-Driving Truck Company Wants to Replace Teamsters'	498	0	10	0
1577	1560	1	30226	2	b"The AI of the car uses sensor data to process all the data and classifies objects based on the size, shape and movement patterns. It can recognize surroundings from a 360 degree perspective by making predictions about vehicles, people and objects around it will move.It can detect pedestrians, but as moving, column-shaped blurs of pixels, so it really cannot tell whether it's a rock or a crumpled piece of paper.However it is programmed to determine certain patterns when a police officer has halted traffic or the car is being signaled to move forward.It also recognizes cyclists as objects outlined in red and can slow down to let the cyclist enter into a lane.Above images are provided by Chris Urmson who heads up Google's driverless car program.Sources:How Google's self-driving cars see the worldHidden Obstacles for Google\xe2\x80\x99s Self-Driving Cars(video) Chris Urmson: How a driverless car sees the road"	145	0	1	0
1578	1561	1	30631	3	b"Google\xe2\x80\x99s self-driving car most likely uses mapping of traffic signs using google street view images for roadway inventory management. If traffic signs are not in its database, it can still \xe2\x80\x9csee\xe2\x80\x9d and detect moving objects which can be distinguished from the presence of certain stationary objects, like traffic lights. So its software can classify objects based on the size, shape and movement patterns. Therefore it is highly unlikely that a person would be mistaken for a traffic sign. See: How does Google&#39;s self-driving car identify pedestrians?Image: Technology ReviewTo support such a claim, Illah Nourbakhsh, a professor of robotics at Carnegie Mellon University, gave an interview to the New York Times magazine cover story on autonomous driving cars, and includes this hypothetical scenario, saying: If they\xe2\x80\x99re outside walking, and the sun is at just the right glare level, and there\xe2\x80\x99s a mirrored truck stopped next to you, and the sun bounces off that truck and hits the guy so that you can\xe2\x80\x99t see his face anymore \xe2\x80\x94 well, now your car just sees a stop sign. The chances of all that happening are diminishingly small \xe2\x80\x94 it\xe2\x80\x99s very, very unlikely \xe2\x80\x94 but the problem is we will have millions of these cars. The very unlikely will happen all the time.Even so, the risk would be minimal, since the car is always looking out for traffic, pedestrians and other obstacles.Sources:How Google's self-driving cars see the worldThe Dream Life of Driverless Cars at The New York Times"	243	0	4	0
1579	1488	0	61918	0	b"Tesla model S has Autopilot which allows to steer within a lane, change lanes with the simple tap of a turn signal, and can manage speed by using traffic-aware cruise control. Multiple digital controls helps to avoid collisions. Based on that, this isn't fully self-driving car.However it is using a computer vision detection system, but it is not intended to be used hands-free.So basically what is known is that the accident involved the side of a truck trailer (of a large white 18-wheel truck) and most likely the camera had a washed out of picture possibly due to glare or blooming from overexposure which made that the side of the trailer white and thin which failed to distinguish with the sky which was bright as well.This may have happened in part, because the crash-avoidance system only engage when both radar and vision system detect an obstacle which could not happen.Further more it was suggested by The Associated Press that the driver most likely was watching a Harry Potter at the time of the crash and assuming system would alert Brown, we don't know if he was able to retake controls quickly enough to avoid impact. As mentioned again, the system wasn't intended for hands-free driving and parts of the system was unfinished. Not to mention that the car was driving with full speed under the trailer.Tesla officially said about this crash in a statement on its website: The high ride height of the trailer combined with its positioning across the road and the extremely rare circumstances of the impact caused the Model S to pass under the trailer, with the bottom of the trailer impacting the windshield of the Model S.  Neither Autopilot nor the driver noticed the white side of the tractor-trailer against a brightly lit sky, so the brake was not applied.They also said, according to techno-optimists, that they will tweaks their code, so this particular case won't happen again.To summarize, this was a 'technical failure' of braking system and most likely Autopilot was not at as Tesla told Senate.The New York Times |Source: Florida traffic crash reportSources:Tesla Autopilot death highlights autonomous risksLayers of AutonomyInside the Self-Driving Tesla Fatal AccidentTesla driver dies in first fatal crash while using autopilot mode"	372	0	2	0
1580	-1	0	0	5	b'Has there any research been done on how difficult certain languages are to learn for chatbots? For example, CleverBot knows a bit of Dutch, German, Finnish and French, so there are clearly chatbots that speak other languages than English. (English is still her best language, but that is because she speaks that most often)I would imagine that a logical constructed language, like lobjan, would be easier to learn than a natural language, like English, for example. '	76	0	0	0
1581	1539	0	82450	5	b"Further to Franck's answer, there may be better optima (even global optima) that exist in the opposite direction to the gradient (which may be in the direction of some local optima). Evolutionary algorithms have scope to search the surrounding area, while backpropagation will always move in the direction of the gradient. With no guarantee (due to their randomness), evolutionary algorithms may be capable of finding solutions that backpropagation simply cannot."	69	0	0	0
1584	1568	1	69461	5	b"For many years, the focus has been on games with perfect information. That is, in Chess and Go both of us are looking at the same board. In something like Poker, you have information that I don't have and I have information that you don't have, and so for either of us to make sense of each other's actions we need to model what hidden information the other player has, and also manage how we leak our hidden information. (A poker bot whose hand strength could be trivially determined from its bets will be easier to beat than a poker bot that doesn't.)Current research is switching to tackling games with imperfect information. Deepmind, for example, has said they might approach Starcraft next.I don't see too much different between video games and board games, and there are several good reasons to switch to video games for games with imperfect information. One is that if you want beating the best human to be a major victory, there needs to be a pyramid of skill that human is atop of--it'll be harder to unseat the top Starcraft champion that the top Warcraft champion, even though the bots might be comparably difficult to code, just because humans have tried harder at Starcraft.Another is that many games with imperfect information deal with reading faces and concealing information, which an AI would have an unnatural advantage at; for multiplayer video games, players normally interact with each other through a server as intermediary and so the competition will be more normal."	253	0	1	0
1587	1580	0	35574	1	b'It\'s not the language itself but rather the structure and for the language\'s ambiguity, for example in English: person a says "John and Bob (his fish)" person B says "He died!", posed question, to whom does person B refer to by he died. More than the language, but the application. You can write a chat bot in Assembly, C, C++, C#, Java or Python. The all work a bit differently but can accomplish the same result, but one language might have more pros or cons to the other. So it will boil down to not language but the understanding of what is being said, research of language in the brain has come to confirm we associate a meaning/feeling/and other inputs with a given language. So to conclude: English is by far the most chaotic for a chat bot but Japanese is actually the best due to the way the language itself is written/spoken. There is more structure to it and less ambiguity.I\'m a Software Engineer and An AI Researcher for the past 7 years.'	173	0	0	0
1588	1476	0	61591	0	b"Simply yes, but it can lead to over fixing of the NN.Humans favour not dying, which is only realised once a consequence is defined for the system to realise that death is an unfavorable result. Which can be train vai observation. Allow your system to observe between 2 or more separate people/systems. Then allow opportunity to test in a safe environment with the pre existing info of the consequences that may follow, provind that if the system makes a mistake in the test/safe environment it will be saved unknownly and then informed that it made a mistake, the place system in an unsafe world in same conditions, informing it that if something happens it will die. That is the way humans grow up, and we've lasted very long with this technic.I'm an AI Researcher and Software Engineer for the past 7 years."	141	0	0	0
1589	1481	0	51134	2	b"A neural network can be used but must be trained to expect the information (pattern of data, pixels or groupings of loose range such as color, and location) at any given location in the network, first a vision system must but implemented. Then a facial recognition, multiple partial individual body fixing (finding body part and there partners to a person) then training on some states and you'll have it work. MIT have done research and have made a seemy accurate implementation. I'm an AI Researcher and Software Engineer for the past 7 years."	92	0	0	0
1590	1481	0	52213	3	b"MIT have done research and implemented an incomplete version of action video recognition.With the use of MATLAB, NNetworks and a large set of training videos.My suggested set of comments on my previous answer indicate the usage of a multi interconnected NNet, verus MIT's image based NNet."	45	0	0	0
1591	1476	1	73087	0	b'It\'s impossible to give a definitive \'yes\' answer to your question, since that would require proving that alternatives cannot exist.More philosophically, it depends on what you mean by "preference over world states":However counter-intuitive it might seem, it is conceivably possible to create Strong AI purely from local condition-action rules, in which there is no global concept of \'preference value\' and/or no integrated notion of \'world state\'.'	65	0	0	0
1592	-1	0	0	5	b'Google, Tesla, Apple etc have all built or are building their own self-driving cars. As an expert in a related area, I am interested in knowing at a high level, the systems and techniques that go into self-driving cars. How easy is it for me to make a tabletop prototype (large enough to accomodate the needed computing power needs)?'	58	0	0	0
1593	-1	0	0	2	b'The above question itself is perhaps too broad for this forum, hence I am phrasing it as a request for references.Humans have been endowed with personalities by nature, and it is not clear (to me at least) if this is a feature or a bug. This has been explored in science fiction by various notions of Borg-like entities. It is my belief that, for narrative reasons, such stories usually end with the humans with their flawed personalities winning in the end. Are there experts who have analyzed, perhaps mathematically, design criteria for an AI agent with weakly enforced goals (eg. to maximize reproduction in the human case) in an uncertain environment, and ended up with the answer that a notion of personality is useful? If there are philosophers or science fiction writers who have examined this question in their work, I would be happy to know about those too.'	148	0	1	0
1594	1476	0	78294	1	b'Simply put, we don\'t know how to create Strong Artificial Intelligence yet, so we don\'t know what is or isn\'t required to create it. At best we can engage in "informed speculation", in which case I\'d say that the answer is more likely "yes" than "no". But that\'s basically just a hunch.If you\'re interested in a pretty good overview of what "pieces" might be required to create Strong AI, and if you haven\'t read it yet, Pedro Domingos\' book The Master Algorithm might be of interest. '	86	0	1	0
1595	1531	0	8489	2	b'Yes, as mentioned in other answers, Prolog is actually used in IBM Watson. Prolog doesn\'t get much "hype" and "buzz" these days, but it is absolutely still used. As always, it has certain specific areas where it shines, and specific techniques that map well to its use. Specifically, things like Inductive Logic Programming, Constraint Logic Programming, Answer Set Programming and some NLP applications may involve extensive use of Prolog.'	68	0	0	0
1596	1593	0	4978	3	b"'Personality' is something of a 'suitcase word' (Minsky) for quite a large collection of (presumably reasonably consistent) observable traits. It seems clear that there is a certain collective advantage in having a consistent personality - specifically that it affords observers some learning gradient in an otherwise uncertain environment. This is of particular importance because those consistencies might have been arrived at using different learning mechanisms than the ones a given observer has.Hence, in any non-trivial coevolutionary system, other organisms will inevitably make use of any such consistencies. Consider a simple robot, called Alice, say, that has the trait of 'quickly flashing red when it sees a blue robot'. It makes sense for all observers to exploit everything that they perceive as correlating with Alice's behavior, in particular, the prediction that a blue robot is likely to be present.The best reference I can recommend on this (which shows that we tend to ascribe 'personality' to even very simple mechanisms) is 'Vehicles' by Valentino Braitenberg."	162	0	0	0
1597	1593	0	13378	3	b"First, a note on the question itself. Humans have been endowed with personalities by nature, and it is not clear (to me at least) if this is a feature or a bug. In my opinion, this is a statement that constrains the question, since it assumes that the personality is given. To me, it feels a bit like playing god: Artificial (given) Intelligence would hence imply Artificial (given) Personality. This approach to the problem seems to be supported by the next fragment: a notion of personality is usefulI point to the above because I don't think that artificial intelligence... Intelligence itself, actually, need to be given or assigned, or even have a use in the sense of a purpose. The previous note was about emergence, which is a topic that user217281728 briefly addressed in their answer. In this second approach, the particular traits just happen, or develop. The interaction between the (so-called) agents and their environment, as well as fellow agents can give place to new behaviour patterns, not designed beforehand. In an evolutionary approach, if the personality would happen to have an advantage (or at least not represent a disadvantage), then it could just appear. Of course, I am making a number of assumptions and demarcations here as well:I am thinking about embodied intelligenceI speak of evolutionary roboticsI think on social issues being of importanceI assume that personality could emergeNow, an example that I find extremely interesting is that of the little mobile robots which could move around and end-up in a pool of food or a pool of poison. And they, somehow, by some odd chance, recognised or made a relation between signals sent by other robots, and the presence of food. Or not. That was more or less the thing: Some robots (kind of) learned to conceal information and thus had more time to eat themselves. Well, I would have a couple of personality adjectives for such guys.Here you find the article and here you find some videos and related stuff.And with that, we land at my last point: We humans put the adjectives, according to our social conditioning. We call Marvin depressive and R2D2 lovely and charming. If they perceive their personalities as constructive or damaging, will always depend on our own judgment. In the end, it is quite common under humans to disagree on personality issues, too.BonusRemember when HAL got emotional, on the face of death?It gets human when it loses its cool, before the flawed-personality human astronaut :)"	413	0	4	0
1598	-1	0	0	2	b"I've found this short Python code which implements neural network in 11 lines of code:I believe it may be a valid implementation of neural network, but how do I know?In other words, is just creating bunch of arrays which compute the output on certain criteria and call them layers with synapses does it make proper neural network?In other words, I'd like to ask, what features/properties makes a valid artificial neural network?"	70	11	1	0
1599	1598	1	2463	4	b'If you pick up a textbook on Neural Networks, you\'ll find that the simplest examples shown are ones that just implement an AND gate or something. They\'re trivial, probably fewer lines of code than what you have there. The bar to be an "artificial neural network" is pretty low... it certainly isn\'t the case that ANN\'s must be incredibly complicated with thousands of lines of code, and many layers, or even many "neurons" total.Basically, if something is setting up at least one "neuron" with multiple inputs, and using some kind of weighting function to generate an output from those inputs, it\'s a valid ANN. It might be a really simple example of an ANN, but it\'s still an ANN.Remember what Geoffrey Hinton says in his Coursera class - (paraphrased) "We don\'t pretend that the things we\'re building really work the way the brain does, we\'re just taking the brain as loose inspiration for an approach that we\'ve found works".'	158	0	0	0
1601	-1	0	0	3	b"I'm looking for research which discusses misbehavior detection in public internet access networks using ANN approaches.So it can be used by ISP to detect suspicious users connected to their network."	29	0	0	0
1603	-1	0	0	2	b"I'm investigating applications of AI algorithms which can be used for data leakage detection and prevention within an intranet network (like Forcepoint). More specifically detecting traffic patterns. I'm new to this.Which learning algorithms are most suitable for this goal? EA, GA, ANN (which one) or something else?"	46	0	0	0
1606	-1	0	0	1	b"I'm wondering, instead of implementing new web browsers over and over again with millions line of code which is very difficult to manage, would it be possible to use ANN or GA algorithm to teach it about the rendering process (how the page should look like)?So as an input I would imaging the html source code, output is the rendered page (maybe in some interactive image like SVG, some library or something, I'm not sure).The training data can be dataset of websites providing input source code and their rendered representation by using other browsers for the guidance as expected output.Which approach would you take and what are the most challenging things you can think of?"	114	0	0	0
1607	1606	1	5061	6	b"The rendering process for browsers is very well defined, and has a very rigid definite ruleset where (virtually) every accountability is noted and handled. This is not optimal for Machine Learning, which works when we have a large pool of examples, and we don't know the ruleset; it will figure it out. Even if you were to train an Neural Network to process that input, there are several things you must account for:1. Variance in data.Not all webpages are equal in length or complexity, and making a neural network to generate output from HTML would produce garbage most of the time.2. Training time.The time it would take for a neural network to understand HTML tags, attributes, the DOM Tree, and each and every element, including new ones being added every few years, and how each one renders and behaves, would take an extremely long time, most likely several years on a fast computer, if it even were possible3. Interactivity.Web pages aren't just static, they change according HTML, CSS and JavaScript. Not only would you have to design your system to account for the rendering step, you would also make it have to understand the Turing Complete scripting language JavaScript, as well as the less complicated, but inherently intertwined with HTML, CSS stylesheet language. If you thought the rendering process was easy, try training a neural network to handle complicated scripting patterns.4. New StandardsNot all HTML is equal, because of different standards. WHATWG began working on HTML5 in 2004, and browsers started to implement not long after. In 2004, there were very few examples of HTML5 sites to train your network to begin with. Sure, now it's standardized and every website uses it, but what about HTML6? When the first specification is released (probably 2017-2025), virtually no websites will use it, because no one will support it. Only when it finally becomes standard, probably in the late 2020s or early 2030s, will you have enough data to train your monstrous system of neural networksAs for AI in general, one could argue that browsers already use A.I. in their rendering process. They intelligently decide what to render (taking CSS into account), when in order to get the most efficient render time, they selectively use different JavaScript parsers on different sections of the code to optimize the speed, the whole system has been optimized on another ruleset to make rendering and interacting with a webpage as seamless and easy-to-use as possible. Your system will never be as good as what hundreds of humans have optimized over 20 years.Trying to solve HTML rendering with Neural Networks is akin to trying to nail a nail with a screwdriver. It's just not going to workHope this was helpful!"	451	0	0	0
1608	1601	1	14018	5	b'One popular technique for doing this is to use Artificial Immune Systems, an evolutionary computation approach which maintains a population of pattern detectors. Here is a survey paper.'	27	0	1	0
1609	1593	0	62129	2	b'As for AGI , Everything is broken down into groups. The are all controlled by a part called the "Spark", and then there are the agents, little sub routines. The SPARK is the is the main judge of the system. It compares the performance of the agents. The Spark lets one agent out of sleep and records how well it does at getting reward for the body, as a whole. If a active agent does good it is replicated in free memory with a few mutations.The \'SPARK" and agents first look at what is on the detectors and SPARK select the best agent. And the SPARK turn on and off agents like in a orchestra. As the system matures many agents will work in parallel.This process is the the subconscious mind.After a while one of the agent is converted to a copy of of SPARK. This new copy is then modified and is called the OFF SPARK. The conscious mind. It will take on the control of the agents too. But SPARK is still master of all.OFF SPARK can activate agents. But it can organizes agents on a massive scale.It will develop many routines. Many agents working in parallel.Once theses massive agent swarm developed into perfected routine will become a reflex. And all reflexes will be given over to SPARK the subconscious part of this system. Stored for latter use.If OFF SPARK needs to get over to a new areas to create new patterns, like working the slot machine for the first time, to develop new routines. it need automated subconscious process of walking over the that area. OFF SPARK starts the walking routine and hands off over to spark.IF the craving for food becomes too strong then SPARK shuts down OFF SPARK. And then uses OFF SPARK routines to get food or what urgent goal that need to be taken care of.If all urgent goals are taken care of then there will be free will because OFF SPARK will be in control. And SPARK will be push into helper mode.The AGI has a internal pattern editor, that is only used by OFF SPARK. that cut up existing physical routines and tries to rebuild new and different routines. This is a trial and error generator. Once a editing procedure work and perfected it is handed over to Spark for storage. Off SPAK initiate a editing routines and then SPark take over.This editing of patterns will lead to a internal 3D simulator.SO OFF SPARK make new physical routines and new pattern editing routines. And all this is done on the backs of automated perfected routine of old.'	435	0	0	0
1610	1592	0	69671	4	b"You're going to need some way to 'see' the area around the car, and to track the speed of nearby objects. Google uses a combination of LIDAR, radar, conventional cameras, and occasionally sonar (see here for a high-level overview). This technology is quite expensive, and can easily cost thousands of US dollars.However, a bigger obstacle than the expense of the hardware (which would be smaller for a table-top prototype) is the software complexity. Like many major projects, the software for self-driving cars is the result of years of work from AI research teams, and thus extremely difficult to duplicate on your own.That said, you're not trying to make a state-of-the-art self-driving car. Assuming you're an expert in image processing and robotics, you can probably create a basic prototype, (like something that drive in a limited table-top environment). However, it's still going to take a lot of time and money. "	149	0	1	0
1611	-1	0	0	1	b'I\'m trying to make a conversational chatbot, so the\xc2\xa0user inputs are quite wide ranging - beyond just "turn lights on". I want to detect the category of the user intents from their inputs and prepare responses.I\'ve looked at MS\' Luis and api.ai and the intents require a lot of training. Can people suggest other techniques for untrained intent detection?For example if the user says "Pasta is my favorite dish to cook" then detect "intent preference entity pasta" - then I can gradually build up responses to different categories of inputs.Perhaps the crowd-sourced intents that wit.ai (facebook) has access to could do this but I\'m not sure if all end-users have access to those models.'	113	0	0	0
1612	26	0	39086	0	b"Emotions aren't something that you can implement - they're very complex. However, you can attempt to mimic them. Human emotions are closely related to conscious experience characterized by intense mental activity, which is based on interpretation of events.Recent brain studies (including research in cognitive psychology and neurophysiology) suggests that human emotional assessment of every action or event plays an important role in human mental processes.The recent 2016 Annual Meeting of the BICA Society brought together scientists from around the world to approach principles and mechanisms of human thought to create biologically inspired AI.For example, in Samsonovich's (a professor in the Cybernetics Department at the MEPhI) proposal, the idea is to test AI in computer games which involves actions with emotional content, where AI may engage with players in different types of social relationships (such as trust, subordination or leadership).Jonathan Gratch of the ICT, invented virtual characters capable of identifying and expressing emotions by communicating with humans in their natural language based on the situations where for example AI can deceive a human to achieve the desired result. The effect is obviously not achieved by re-creating human consciousness, but by achieving statistically adjusting parameters.Researchers from the Institute of Cyber Intelligence Systems in MEPhI are hoping to be able to create in the near future future virtual beings which are capable of planning, setting goals and establishing social relationships with humans, also by possessing both emotional and narrative intelligence which can interpret context of events.Source: Researcher proposes social emotions test for artificial intelligence"	250	0	2	0
1613	-1	0	0	4	b"How does a domestic autonomous robotic vacuum cleaner - such as a Roomba - know when it's working cleaned area (aka virtual map), and how does it plan to travel to the areas which hasn't been explored yet?Does it use some kind of A* algorithm?"	44	0	0	0
1614	-1	0	0	3	b'It has been suggested that machine learning algorithms (also Watson) can help with finding disease in patient images and optimize scans. Also that deep learning algorithms show promise for every type of digital imaging.How does exactly deep learning algorithms exactly can find suspicious patterns in the body\xe2\x80\x99s biochemistry?'	47	0	2	0
1615	1614	1	9136	2	b'I wouldn\'t focus only on "deep learning" unless you have some specific reason for doing so. There may be other techniques which could be as effective, or more effective. One approach I\'ve seen used for something similar was Inductive Logic Programming. For one example of using ILP to reason about elements of biochemistry, see this paperThat\'s not exactly about detecting disease, but it does sort of illustrate the broad idea of reasoning about states and reactions involving metabolic pathways in biochemistry, using ILP. Possibly the basic idea could be adapted more towards detecting disease. '	94	0	0	0
1616	1603	1	6548	3	b'This seems to fall broadly into the regime of a classification problem as you want to classify an outgoing communication as "contains proprietary information" or "does not contain proprietary information". As such, any classification approach could be applied. Neural Networks certainly seem like a valid approach, but you might also get good mileage out of Random Forests, Support Vector Machines, a Naive Bayes classifier, etc.GA\'s are more aimed towards optimization than classification, so I wouldn\'t say that a GA, in and of itself, would map cleanly to solving this kind of problem. If GA\'s had applicability here, I think it would be more likely to be in terms of training a model rooted in one of the other techniques. '	119	0	0	0
1617	-1	0	0	1	b'The Mario Lives! video (and its follow-up video, Mario Becomes Social!) showcases an AI unit that is able to simulate emotional desicion-making within a virtual world, and can enter into "emotional states" such as curiosity, hunger, happiness, and fear. While this seems cool and exciting (especially for video game AI), I am confused how this would be useful in real-world scenarios.What would be the point of building autonomous actors that would behave based on these emotional states, instead of simply knowing what they should do (either by hardcoding in the rules, or learning the rules through machine learning)?'	97	0	0	0
1618	-1	0	0	8	b"This is from the 2014 closed beta. The asker had the UID of 245.For a deterministic problem space, I need to find a neural network with the optimal node and link structure. I want to use a genetic algorithm to simulate many neural networks to find the best network structure for the problem domain.I know a fair amount about neural networks1 but have not used genetic algorithms for a task like this before.What are the practical considerations? How should I encode the structure into a genome?1Actually, I don't. Just saying that. -Mithrandir. "	92	0	0	0
1621	35	0	38003	5	b"The machine learning is a sub-set of artificial intelligence which is only a small part of its potential. It's a specific way to implement AI largely focused on statistical/probabilistic techniques and evolutionary techniques.QArtificial intelligenceArtificial intelligence is 'the theory and development of computer systems able to perform tasks normally requiring human intelligence' (such as visual perception, speech recognition, decision-making, and translation between languages).We can think AI as concept of non-human decision makingQ which aims to simulate cognitive human-like functions such as problem solving, decision making or language communication.Machine learningMachine learning (ML) is basically a learning through doing by implementation of build models which can predict and identify patterns from data.According to Prof. Stephanie R. Taylor of Computer Science and her lecture paper, and also Wikipedia page, 'machine learning is a branch of artificial intelligence and it's about construction and study of systems that can learn from data' (like based on the existing email messages to learn how to distinguish between spam and non-spam).According to Oxford Dictionaries, the machine learning is 'the capacity of a computer to learn from experience' (e.g. modify its processing on the basis of newly acquired information).We can think ML as computerized pattern detection in the existing data to predict patterns in future data.QIn other words, machine learning involves development of self-learning algorithms and artificial intelligence involves developing systems or softwares to mimic human to respond and behave in a circumstance.Quora"	232	0	3	0
1625	-1	0	0	2	b'Were there any studies which checked the accuracy of neural network predictions of greyhound racing results, compared to a human expert? Would it achieve a better payoff?'	26	0	0	0
1626	1618	1	45825	11	b'Section 4.2 of "Essentials of Metaheuristics" has a wealth of information on alternative ways of encoding graph structures via Genetic Algorithms.With particular regard to evolving ANNs, I would personally not be inclined to implement this sort of thing \'from scratch\':The field of neuroevolution has been around for some time, and the implementation some of the methods, such as Neuroevolution of Augmenting Topologies (NEAT) now incorporate the results of much practical experience.According to the above link: We also developed an extension to NEAT called HyperNEAT that can evolve neural networks with millions of connections and exploit geometric regularities in the task domain. The HyperNEAT Page includes links to publications and a general explanation of the approach.'	114	0	1	0
1627	1617	1	62122	2	b"Humans have poor understanding of emotional rules. Probably every poster on here has experienced greatly misreading another individual emotionally. Further, people often don't act emotionally how they would expect themselves to act, for example we have all experienced frustration at someone else's irrational concerns and yet we are all guilty of holding irrational concerns of our own. This is the crux of why hard-coded emotional rules do not work - we do not have an understanding of what emotional 'rules' make someone feel real.By moving towards autonomous state-based actors we move away from this issue. The actor's state transitions will of course be defined rules (hard-coded or learnt) but by abstracting the actor's emotional state from the specific context (e.g. specific actions trigger emotional state transitions which trigger responses, instead of a direct response to a specific action), the programmer prevents them-self from projecting their own beliefs/emotions/logic onto the actor. Further, autonomous actors are more extendable. Consider an autonomous actor that is hard-coded to move towards 'upset' and 'angry' emotional states when experiencing 'pain'. Simply by associating a new world action with 'pain', one can trigger an emotional response from an autonomous actor that has not experienced that action before. When working in hard-coded emotional rules, this would not be possible."	210	0	0	0
1628	-1	0	0	1	b"I've read about The Loebner Prize for AI, which pledged a Grand Prize of $100,000 and a Gold Medal for the first computer whose responses were indistinguishable from a human's.So I was wondering whether any chatbots have fooled the judges and won a Gold Medal yet?From their website this isn't clear (as some of the links doesn't load).A few highlights from previous years:2011 Loebner Prize results None of the AI systems fooled the judges, therefore the Turing Test has not been passed.Loebner 2013 results: No chatbot fooled any of the 4 Judges."	91	0	3	0
1629	1628	1	1703	4	b"The 2016 finals haven't started yet, they will start on Saturday, 17 September 2016. In the 2015 finals or before that, nobody won the Gold Medal or the Silver Medal. The most up-to-date data can be found here, where we can find both the results from 2015 and the timeline of the 2016 contest. "	54	0	1	0
1630	-1	0	0	2	b'Hypothetically, assume that you have access to infinite computing power. Do we have designs for any brute-force algorithms that can find an AI capable of passing traditional tests (e.g. Turing, Chinese Room, MIST, etc.)? '	34	0	0	0
1631	1630	1	1870	7	b'What \'infinite\' means here could possibly be debated at some length, but that notwithstanding, here are two conflicting answers:\'Yes\': Simulate all possible universes. Stop when you get to one containing a flavor of intelligence that passes whatever test you have in mind. Steven Wolfram has suggested something broadly along these lines. Problem: the state of computational testing for intelligence e.g. Winograd schema would then be the bottleneck. In the limit, testing for intelligence requires intelligence and creativity on behalf of the questioner.\'No\': It may be that, even with infinite ability to simulate, there may be some missing aspect of our simulation that is necessary for intelligence. For example, AFAIK quantum gravity (for which we lack an adequate theory) is involved in Penrose\'s "Quantum Microtubules" theory of consciousness (*). What if that was needed, but we didn\'t know how to include it in the simulation?The reason for talking in terms of such incredibly costly computations as \'simulate all possible universes\' (or at least a brain-sized portion of them) is to deliberately generalize away from the specifics of any techniques currently in vogue (DL, neuromorphic systems etc). The point is that we could be missing something essential for intelligence from any of these models and (as far as we know from our current theories of physical reality) only empirical evidence to the contrary would tell us otherwise.(*) No-one knows if consciousness is required for Strong AI, and physics can\'t distinguish a conscious entity from a Zombie.'	243	0	1	0
1632	-1	0	0	1	b"I'm aware this could be a complex topic, however I'm interested in existing research projects or studies where people are attempting or have succeeded in teaching an AI a foreign language just by training/teaching it from English books. By reading, analysing and understanding, so that it knows the foreign language's rules (such as grammar, spelling, etc.), the same way as a human would learn. The language doesn't have to be Chinese, which is difficult for even humans to learn."	78	0	0	0
1635	-1	0	0	3	b"Would it be possible to put Asimov's three Laws of Robotics into an AI?The three laws are:A robot (or, more accurately, an AI) cannot harm a human being, or through inaction allow a human being to be harmed1A robot must listen to instructions given to it by a human, as long as that does not conflict with the first law.A robot must protect its own existence, if that does not conflict with the first two laws.1 To it's knowledge. This was a plot point in one of the books :P"	89	0	0	0
1638	1635	0	1775	6	b"The most challenging part is this section of the first law: or through inaction allow a human being to be harmedHumans manage to injure themselves unintentionally in all kinds of ways all the time. A robot strictly following that law would have to spend all its time saving people from their own clumsiness and would probably never get any useful work done. An AI unable to physically move wouldn't have to run around, but it would still have to think of ways to stop all accidents it could imagine. Anyway, fully implementing those laws would require very advanced recognition and cognition. (How do you know that industrial machine over there is about to let off a cloud of burning hot steam onto that child who wandered into the factory?) Figuring out whether a human would end up harmed after a given action through some sequence of events becomes an exceptionally challenging problem very quickly."	153	0	0	0
1639	1635	0	2055	6	b'Defining "harm" and in particular, "allowing harm via inaction" in any meaningful way would be difficult. For example, should robots spend all their time flying around attempting to prevent humans from inhaling passive smoke or petrol fumes?In addition, the interpretation of \'conflict\' (in either rule 2 or 3) is completely open-ended. Resolving such conflicts seems to me to be "AI complete" in general.Humans have quite good mechanisms (both behavioral and social) for interacting in a complex world (mostly) without harming one another, but these are perhaps not so easily codified. The complex set of legal rules that sit on top of this (polution regulations etc) are the ones that we could most easily program, but they are really quite specialised relative to the underlying physiological and social \'rules\'.EDIT: From other comments, it seems worth distinguishing between \'all possible harm\' and \'all the kinds of harm that humans routinely anticipate\'. There seems to be consensus that \'all possible harm\' is a non-starter, which still leaves the hard (IMO, AI-complete) task of equaling human ability to predict harm. Even if we can do that, if we are to treat as actual laws, then we would still need a formal mechanism for conflict resolution (e.g. "Robot, I will commit suicide unless you punch that man"). '	212	0	0	0
1640	1632	1	11092	2	b"Current approaches for learning a language require having a large corpus of that language; it also doesn't seem reasonable to expect that it will ever be possible to learn about language A by extracting information from a corpus from an unrelated language B.Even if you want to learn about human languages in general (what sorts of things are true about grammar, vocabulary, and so on), that relies having many languages as training data, so that you can see the different ways of doing things instead of assuming that the way they're done in English is the way they're done in every language.(There is work in automatic translation that goes from a language to 'concept-space', then goes from that 'concept-space' to another language, so that you can build an English-Chinese translator by building two separate English-Concept and Chinese-Concept translators, instead of ever needing material that directly links English and Chinese. The obvious benefit of this is scalability; in order to make translators for a new language to any other language, you just need to learn that language and the models build themselves.)"	180	0	0	0
1641	1630	0	17421	3	b'We\'re definitely nowhere near that level of AI; at best, high-tech solutions like deep convolutional neural nets can help with image recognition and some other algorithms can perform things like robotic movement adequately enough to be useful in some scenarios. None of this is even as sophisticated as the behavior of a flea, but no one refers to insects as "intelligent." It\'s exciting stuff that allows us to solve problems that human intelligence often has difficulty with (such as classification of thousands of objects, which would tire an ordinary human mind), but it\'s nowhere close to replicating our higher brain functions. Also keep in mind that the Turing test is a poor test of "intelligence" that defies common sense. By the same extension, mistaking a mannequin for a human being in the dark does not mean that the mannequin is actually human. If it were a valid test, then we passed that way back around 1980 with programs like Dear Eliza which were coded in BASIC to regurgitate human speech patterns. There\'s just no need to come up with a sophisticated argument like Searle\'s Chinese Room to debunk it, since it\'s silly on its face; any layman should be able to see right through the Turing Test. If anyone except Turing had come up with this test it would not have received much attention. Turing displayed one-of-a-kind genius when it came to things like computing and cryptography, but like many other experts in such fields, he had a lot of trouble grappling with metaphysics and philosophy. Searle had more common sense, but his Chinese Room example is more of a rebuttal to the Turing Test than a test in and of itself.What "intelligence" consists of is ultimately a deep metaphysical question, not a material one. For millennia, trained philosophers have had a lot of trouble assigning clear definitions to concepts like intelligence and consciousness. Until we can answer those questions definitively, using different sets of reasoning skills than scientists, mathematicians and computer specialists are used to employing (just look at how often metaphysics is derided in some of these disciplines) then we cannot say that we have achieved genuine A.I. Until we can define what intelligence is, we cannot say whether or not we\'ve successfully built it; we\'ve not only got the cart before the horse, but have yet to build the cart or see a horse. By the common definitions used in everyday speech we\'re nowhere near genuine A.I. No one calls cows or sparrows "intelligent," but our AI today isn\'t even as sophisticated as the mosquitoes that bite them. That\'s not going to be a popular answer - I\'ll probably get a dozen downvotes for this, without anyone being able to adequately rebut my contentions, but it needs to be said. There\'s far too much irrational exuberance and gross overestimation of what we\'ve achieved to date and probably always will be in this field. Historically, researchers in every generation have also grossly underestimated the computing power of the human brain; every decade or so, the estimates of the FLOPS and megabytes have to be drastically revised. We have a poor track record of even getting basic material questions about the human brain right. This clear, consistent pattern of biased overestimation of our success and the lack of any real definition, let alone a test, of intelligence is going to be a serious issue in this forum for its whole existence (assuming it survives the private beta period). We have a whole forum dedicated to a field we can\'t even define; we can\'t say for sure what A.I. really is, but we\'re adamantly certain that we\'re close to achieving it...! We cannot say if "brute force algorithms" exist when we\'re still groping for an understanding of what it is we\'re trying to force our way into. Certainly, there are brute force methods to solve certain problems, like Deep Blue does at chess - but we cannot say if that qualifies as intelligence or not. It is really not possible to answer questions like this without getting into deep discussions that immediately lend themselves to opinion and debate, which the Turing Test and Searle\'s Room are clear examples of, in and of themselves. Since implementation details of AI are considered by many to be off-limits here, we\'re limited mainly to highly speculative posts about tech that often doesn\'t even work yet (like Google\'s self-driving cars) and questions like this that we can\'t answer without first defining intelligence. This is going to be the root of a lot of problems here for a long, long time to come...'	766	0	0	0
1644	-1	0	0	1	b"I'd like to investigate the possibility of achieving similar recognition as it's in Honda's ASIMO robotp.22 which can interpret the positioning and movement of a hand, including postures and gestures based on visual information.Here is the example of application such interpretation in robot:Image source: ASIMO Featuring Intelligence Technology - Technical Information (PDF)So basically the recognition should detect an indicated location (posture recognition) or respond to a wave (gesture recognition), also similar like Google car does it (by determining certain patterns).Is it known how ASIMO does it, or what would be the closest alternative for postures and gestures recognition to achieve the same results?"	102	0	4	0
1648	-1	0	0	6	b'For Example:Could you provide reasons why a sundial is not "intelligent"?A sundial senses its environment and acts rationally. It outputs the time. It also stores percepts. (The numbers the engineer wrote on it.)What properties of a self driving car would make it "intelligent"?Where is the line between non intelligent matter and an intelligent system?'	53	0	0	0
1649	1648	1	14340	2	b'Typically, I think of intelligence in terms of the control of perception. [1] A related, but different, definition of intelligence is the (at least partial) restriction of possible future states. For example, an intelligent Chess player is one whose future rarely includes \'lost at chess to a weaker opponent\' states; they\'re able to make changes that move those states to \'won at chess\' states.These are both broad and continuous definitions of intelligence, where we can talk about differences of degree. A sundial doesn\'t exert any control over its environment; it passively casts a shadow, and so doesn\'t have intelligence worth speaking of. A thermostat attached to a heating or cooling system, on the other hand, does exert control over its environment, trying to keep the temperature of its sensor within some preferred range. So a thermostat does have intelligence, but not very much.Self-driving cars obviously fit those definitions of intelligence.[1] Control is meant in the context of control theory, a branch of engineering that deals with dynamical systems that perceive some fact about the external world and also have a way by which they change that fact. When perception is explicitly contrasted to observations, it typically refers to an abstract feature of observations (you observe the intensity of light from individual pixels, you perceive the apple that they represent) but here I mean it as a superset that includes observation. The thermostat is a dynamical system that perceives temperature and acts to exert pressure on the temperature it perceives.(There\'s a philosophical point here that the thermostat cares directly about its sensor reading, not whatever the temperature "actually" is. I think that\'s not something that should be included in intelligence, and should deserve a name of its own, because understanding the difference between perception and reality and seeking to make sure one\'s perceptions are accurate to reality is another thing that seems partially independent of intelligence.)'	314	0	0	0
1653	1648	0	31183	2	b"To ask what makes a system intelligent almost begs the question 'in this context what do we mean by artificially intelligent?' which I think this what this question is really gearing towards.From my studies, I've come to see that 'Artificial Intelligence' is a catchy term to use but perhaps misleading, and it conjures up images of these self-driving cars and robots that will take over the earth.What I've found AI, and 'intelligent' systems moreso represent is an aid or a support that works for us, rather than one that works because of us... hear me out:What makes the jump to an intelligent system for me is the step where the system begins to 'adapt / learn' or otherwise do things I didn't directly tell it to do. With the sundial, I measured and cut every inch of it by hand, and put it in a specific way to do a specific thing. When a programmer gets into a car he automated, it may do some things he didn't directly program or maybe couldn't even expect (just one example: querying some database to see lots of people are driving somewhere, discovering a concert is going on there, and asking if the driver wants directions / tickets)--In conclusion, an intelligent system to me is one that we build in such a way that it educates and supports us, rather than a system we ourselves 'educate' to do a specific task. Supportive systems that elucidate and adapt and act 'rationally' even when we didn't tell it what 'rational' behaviour was."	256	0	0	0
1654	1635	0	23588	0	b"I think this is almost a trick question in a sense. Let me explain:For law 1, any AI would abide by the first rule unless it was deliberately created to be malevolent, in that the AI it would understand harm was imminent but do nothing about or would actively attempt to harm. Any 'reasonable' AI would (try its best to) prevent any harm it understood, but couldn't react to imminent harm 'outside it's knowledge', thus satisfying law 1. Any AI that 'tries its best' to prevent harm works here.For law 2, it is simply a matter of design. If one can design an AI capable of parsing and understanding the entirety of human language (beyond just speech), just program it to act accordingly, mindful of the first law. Thus, I think we can develop an AI that will obey every command it understands but getting it to understand anything and everything I believe is impossible.For law 3, it rides in the same vein as law 1.In conclusion, I think there is no philosophical problem with implementing such an AI, but that the actual design of such an AI is fundamentally impossible (understanding all possible harms, and all possible commands)."	198	0	0	0
1655	-1	0	0	3	b'We can read on Wikipedia page that Google built a custom ASIC chip for machine learning and tailored for TensorFlow which helps to accelerate AI.Since ASIC chips are specially customized for one particular use without the ability to change its circuit, there must be some fixed algorithm which is invoked.So how exactly does the acceleration of AI using ASIC chips work if its algorithm cannot be changed? Which part of it is exactly accelerating?'	73	0	0	0
1658	-1	0	0	1	b"I was reading that the Valkyrie robot was originally designed to 'carry out search and rescue missions'.However there were some talks to send it to Mars to assist astronauts.What kind of specific trainings or tasks are planned for 'him' to be able to carry on its own?Refs:NASA-JSC-Robotics at GitHubgithub.io pagegitlab page"	50	0	2	0
1662	-1	0	0	4	b'Do scientists know by what mechanism biological brains/biological neural networks store data?I was thinking about @kenorbs question about implanting nanobots to build an AGI on top of human wetware. I only have a vague notion that we store data in our brains by altering synapses? Links, Criticism and Detailed Explanation welcome.I also would love a decent description of how a vanilla Artificial Neural Network stores data. Questions:How is data stored in a biological Neural Network?How is data stored in an Artificial Neural Network?'	82	0	1	0
1663	1662	1	806	4	b'Second question first: Data is stored in an ANN in the form of weights in the adjacency matrix between neurons. During training, these weights are updated by a learning algorithm (such as backpropagation).First question: according to award-winning neuroscientist Tim Bliss: \xe2\x80\x9cIt\xe2\x80\x99s been accepted really since the turn of the 20th century, since the time of the Spanish neuroscientist Ram\xc3\xb3n y Cajal, that really the only place where memories can be stored is at synapses, the junctions between nerve cells.A protein called the NMDA receptor plays a key roll in the strengthening of synaptic connections (which is more broadly achieved by a form of Hebbian Learning).'	104	0	0	0
1664	211	1	7987	2	b'As a person who works with people who work on Watson, perhaps I can give some insight.The name Watson is casually thrown around a lot whilst many people aren\'t aware of its evolution into a larger suite of systems and services. We now have Chef Watson, Watson Health, and many other developing projects along the "cognitive" route. Watson is really an amalgamation and varied application of the different cognitive computing routes IBM is pursuing.So what I\'m trying to get at is that there are many forms of NLP that Watson conducts and has conducted, developed by different teams to fit different processes, interconnected in different ways. Additionally, much (probably all) of it is proprietary/classified since, as one would imagine, ongoing research is constantly being conducted and added to Watson. This is likely your largest obstacle. The precise workings of the NLP of the Jeopardy flavor of Watson are probably themselves still classified (I can\'t find anything in the time I\'ve just spent looking myself)There are, thus, many answers to this question; many outdated, and others not always applicable. The full answer is very complicated and by the time you find out what the answer is today it\'s probably already been advanced. The researches I know are always working on new, cutting-edge algorithms and processes for text classification and the related NLP topics.To point you to more information, though, take a look at these links:https://www.ibm.com/watson/developercloud/nl-classifier.html (DeepQA research page, check out the publications) (takes a sec to load)Short videos on the Watson QA subject:https://www.youtube.com/watch?v=tu5v-gu_5pYhttps://www.youtube.com/watch?v=PI55a1jFrMY (NLP course syllabus from RPI based around Watson)A good paper on NLP that acknowledges Watson, more technical:UPDATE: Looking at the article posted in the comments by @Pimgd, I find support for what I said above: "For the Jeopardy Challenge, we use more than 100 different techniques for analyzing natural language, identifying sources, finding and generating hypotheses, finding and scoring evidence, and merging and ranking hypotheses."I am sure many if not most of these techniques have since been modified, adapted, or dropped altogether for other methods.'	336	0	4	0
1665	-1	0	0	2	b'My understanding is that Watson is the name of the computer, and DeepQA is the name of the software or technology. They are both correlated.Are there any computers/technologies other than Watson which are using DeepQA? Or is Watson the only computer which implements that software/technology?This question is inspired by this meta thread.'	51	0	1	0
1666	1665	1	3966	3	b"I cannot say for certain, but I know of no such other uses (I work at the building where Watson is developing but do not directly work with it).The DeepQA team's page (https://www.research.ibm.com/deepqa/deepqa.shtml) only ever references Watson as the implementation, and based on the structure of the FAQ there I would imagine they'd be eager to list any interesting other uses it would have, but no such entries exist there.I would, however, also note that while DeepQA is IBM's proprietary implementation of QA using deep-learning methods, that approach is becoming more popular and is almost certainly being worked on by other companies and could soon be on other computers, if not already so in some form."	115	0	0	0
1667	1478	0	57642	0	b"The problem detecting NSFW has been around for over two decades.This study from 2005 about finding naked people, demonstrates a strategy for finding such images based on the color and texture properties to fetch an effective mask for skin regions attempting to group a human figure using geometric constraints on the human structure. This method demonstrated  60% precision and 52% recall on a test set of 138 uncontrolled images of naked people.Here are a few figures from the study explaining the algorithm:The following post contains visualizations of nudity for scientific purposes (hover to display): A more recent approach is using convolutional networks. This study from 2014PDF demonstrated impressive classification performance based on the ImageNet dataset. It's not clear 'how and why they perform so well', however they can be used for classification of images with a very low error rate.For further details, check: What convolutional neural networks look at when they see nudity.You will find the code example and the heatmap for how convnets see NSFW in the above link."	170	0	5	0
1670	1333	0	22470	1	b'I know it seems like a cop-out answer to every question on AI, but "it depends". For example, if the bulk of the storage space is storing learned concepts, and attributes of example entities, then it stands to reason that concepts and entities could be reused. In that scenario, learning from an additional 10G of text would use less storage than the original.OTOH, as others have said, it could be that the storage is mostly storing the links between things, in which case the number of links will likely grow exponentially. In that case, the second batch of "knowledge" would add more storage requirements than the first.So it would come down to "what exactly is the system learning, and how does it represent what it learned?" And that answer will vary from system to system.'	134	0	0	0
1671	1535	0	4883	2	b'I\'ll try to do something intuitive; Each node in a neural network is referred to as a neuron. To understand what\'s going on under the hood of a neural network you only really need to understand an individual neuron. Now each neuron has a set of inputs (other neurons; they can potentially be the inputs to the network as a whole as well), and each input has a weight associated with it. Every time the network is used, each neuron computes its output as the weighted sum of its inputs passed through some gate (The "Activation Function", a mathematical function designed to get a particular behaviour. For example sigmoid AF takes an input of any size and transforms it into an output in the range [0, 1].) Obviously, this is driven from the inputs to the neural network so that no neuron is computing its outputs before all of the neurons used as its inputs have done the same.When you refer to the value of a node; there isn\'t a single value. Each neuron has several weights associated with it as it may be the input to several other neurons, and each of those neurons assigns it a different weight. Instead, it is better to thing of a neural network as a directed graph of nodes (neurons) which are labelled with a particular activation function, and edges (input/output connections) which are labelled with a particular weight. While the structure and activation functions used in the neural networks is a matter of topology design, there are a number of algorithms for designing a ANN for a particular topology.The most commonly used (and possibly easiest to explain) is backpropagation. In pseudo-Layman\'s terms we start off with random weights on all edges in the network. We then compute the output of the network for a training set (a set of known input/output pairs). By careful choice of activation function, it is possible to differentiate the error (computed analogously to the expected output minus the actual output of the ANN for each input/output pair) with respect to the weights of the neural network. This allows us to compute a gradient for each weight; the direction in which we can move the weight to reduce the error on the training set. By doing this until we find an optima (a point where all movements increase error), we can find some \'good\' configuration of weights for that particular ANN. There\'s a nice tutorial on BP here . The diagram associated with it does nicely to explain my point:'	421	0	1	0
1678	-1	0	0	4	b"There is a study about The Necessity of Parsing for Predicate Argument Recognition, however I couldn't find much information about 'Predicate Argument Recognition' which could explain it.What is it exactly and how does it work, briefly?"	35	0	1	0
1685	-1	0	0	4	b'The Wit.ai is a Siri-like voice interface which can can parse messages and predict the actions to perform.Here is the demo site powered by Wit.ai.How does it understand the spoken sentences and convert them into structured actionable data? Basically, how does it know what to do?'	45	0	0	0
1686	-1	0	0	0	b"In 2014 Linkedin acquired Bright.com, for $120 million and it is using AI and big data algorithms to connect users. Bright also throws in a little Klout, ranking people by a \xe2\x80\x9cBright score\xe2\x80\x9d which it uses to assess how strong the chemistry is between a user and a particular job.  It also takes into account historical hiring patterns into its matching, along with account location, a user\xe2\x80\x99s past experience and synonyms.In brief, is it known (based on some research papers) how such algorithm works which aiming at scoring 'chemistry' between users and their jobs?"	94	0	0	0
1687	-1	0	0	-1	b'According to this article, Pinterest acquired VisualGraph, an image recognition and visual search technology startup.How does Pinterest apply VisualGraph technology for machine vision, image recognition and visual search in order to classify the images?In short, how do they predict the image categories? Based on what features?'	45	0	1	0
1688	1687	1	7800	3	b"One of the Pinterest's white paper about Human Curation and Convnets powering item-to-item recommendationsarxiv describes implementation of convolutional neural network (CNN) based visual features (VGG2014, Faster R-CNN). This demonstrates the effectiveness of it (such image or object representations) which can improve user engagement. The visual features are computed using the process described in the previous study about visual search at Pinterest and can be used for more targeted features to be computed for related pins.Here are the examples of detected visual objects from Pinterest's object detection pipeline:Image source: Human Curation and Convnets: PoweringItem-to-Item Recommendations on Pinterest, Page 4, Fig. 6The images are categorized by using dominant visual objects (individual objects seen in the image which passes a confidence threshold in Faster R-CNN) using fine-tuned VGG reranking variant. This allows Pinterest to introduce features such real-time recommendations for the users.Check also this blog entry: Building a scalable machine vision pipeline."	148	0	3	0
1689	-1	0	0	-1	b'Wolfram Language Image Identification Project launched an Image Identify site demo which returns the top predicted tags for the photos.How does it work, briefly? I mean what type of learning vision technologies are used to analyze, recognize and understand the content of an image?'	43	0	0	0
1690	1689	1	0	2	b'The ImageIdentify project uses the highly automated "superfunctions" and as part of Wolfram Language API integration. It relies on a complex collection of meta-algorithms and built-in \'knowledge\'. It has a built-in classifier trained from a large dataset using Wolfram Data Framework (WDF). However the main classifier is based on the deep neural networks.Source: How the Wolfram Language Image Identification Project WorksThe algorithm isn\'t perfect and misidentification are more likely to be caused by \'irrelevant objects repeatedly being in training images for a particular type of object\'.You can read more at Wolfram Language Artificial Intelligence: The Image Identification Project.'	97	0	3	0
1691	-1	0	0	4	b"I've uploaded a picture to Wolfram's ImageIdentify of graffiti on the wall, but it recognized it as 'monocle'. Secondary guesses were 'primate', 'hominid', and 'person', so not even close to 'graffiti' or 'painting'.Is it by design, or there are some methods to teach a convolutional neural network (CNN) to reason and be aware of a bigger picture context (like mentioned graffiti)? Currently it seems as if it's detecting literally what is depicted in the image, not what the image actually is.This could be the same problem as mentioned here, that DNN are: Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs.2015If it's by design, maybe there is some better version of CNN that can perform better?"	127	0	2	0
1694	-1	0	0	6	b'An AI agent is often thought of having "sensors", "a memory", "machine learning processors" and "reaction" components. However, a machine with these does not necessarily become a self-programming AI agent. Beyond the parts mentioned above, is there any other elements or details necessary to make a machine capable of being a self-programming AI agent?For example, a paper from 2011 declared that solving the optimization problem of maximizing the intelligence is a must-have feature for the self-programming process, as quoted below: A system is said to carry out an instance of self-programming when it undergoes learning regarding some element of its "cognitive infrastructure", where the latter is defined as the fuzzy set of "intelligence-critical" features of the system; and the intelligence-criticality of a system feature is defined as its "feature quality," considered from the perspective of solving the optimization problem of maximizing the intelligence of a multi-feature system.However, this description of "optimization of intelligence" is vague. Can anyone give a clear definition or better summary for the necessary components for self-programming agents?This question is from the 2014 closed beta, with the asker having a UID of 23.'	185	0	1	0
1695	1694	1	2092	3	b"At the highest level, all it needs is for the various systems already discussed to incorporate code objects. If it can interpret its source code / model architecture from the formatted text objects underpinning them, can 'understand' them in terms of having a useful ML model, and alter the code with its reaction, then it can self-program. That is, the basic loop behind a recursively improving intelligence is simple. It examines itself, writes a new version, and then that new version examines itself and writes a new version, and so on.The difficult component comes at lower levels. We don't need to invent a new concept like 'sensor,' what we need to do is build very, very sophisticated sensors that are equal to the task of understanding code well enough to detect and write improvements."	133	0	0	0
1696	1691	0	10646	3	b"You seem to be wanting some description of the 'style' of an image. To make that work in general, I'd guess that would actually require quite a lot of pre-processing to present 'texture elements' (rather than pixels) as the basic features. This is quite speculative, but one approach might be to use Iterated Function Systems as a means of extracting these.Whether 'spatial adjacency' (and hence CNN) is then the best approach to make higher-level decisions about these elements is (AFAIK) a matter for experiment."	83	0	0	0
1697	1685	0	8234	2	b'I can\'t speak to wit.ai specifically, but I can tell you a little bit about how similar applications work. Specifically, I can talk a bit about Apache Stanbol which also converts free text into structured data. That said, I should prefix this by saying there isn\'t just one way to "get there from here." Many techniques could be part of a stack for accomplishing this goal.Anyway, in the case of Stanbol, they run the text through multiple processing engines, sequentially, with different engines affecting the final output. One engine simply does Named Entity Recognition using OpenNLP. This identities discrete named "things" - people, places, companies, etc. Another engine does entity matching with a pre-established database of entities - specifically (in the out-of-the-box configuration) a dump of entities from DBPedia. Where a match is found, the text from the original input is assigned to the entity. In the case of a collision, it assigns a weight to the mapping so any downstream consumers can use probabilistic techniques to select the "correct" mapping.There are, of course, more details that I left out. Before NER can happen there is parsing and tokenizing and other NLP activities. But a big part of the basic process is doing NER and then doing the entity matching. And in the case of Stanbol, you can add your own entities and corresponding structured data, as well as your own engines. So, for example, if you wanted to write an engine based on neural networks / deep learning, and plug that in, you could.'	254	0	2	0
1698	1481	0	50188	5	b'This study from 2012 uses 3D convolutional neural networks (CNN) for automated recognition of human actions in surveillance videos. The 3D CNN model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. A very similar deep learning approach based on 3D CNN is demonstrated in the LIRIS and Orange Labs study from 2011.This Oxford study from 2014 also uses a similar approach, but with two-stream CNN which incorporates spatial and temporal networks which can achieve good performance despite having limited training data. It recognises action from motion in the form of dense optical flow. For example:Another study from 2007 demonstrates a method by detecting human falls based on a combination of motion history and human shape variation by analysing the video frames. It uses Motion History Image (MHI) to quantify the motion of the person.Source: harishrithish7/Fall-Detection at GitHubAn alternative general approach could be action detection based on the posture using DNN. See: How to achieve recognition of postures and gestures?'	174	0	5	0
1699	1481	1	50255	5	b'There are several approaches as to how this can be achieved.One recent study from 2015 about Action Recognition in Realistic Sports VideosPDF uses the action recognition framework based on the three main steps of feature extraction (shape, post or contextual information), dictionary learning to represent a video, and classification (BoW framework). A few examples of methods:Spatio-Temporal Structures of Human Posesa joint shape-motionMulti-Task Sparse Learning (MTSL)Hierarchical Space-Time SegmentsSpatio-Temporal Deformable Part Models (SDPM)Here are the results based on training of 10 action classes based on the UCF sports dataset:Source: Action Recognition in Realistic Sports Videos.'	92	0	3	0
1700	-1	0	0	11	b"In a recent Wall Street Journal article, Yann LeCunn makes the following statement: The next step in achieving human-level ai is creating intelligent\xe2\x80\x94but not autonomous\xe2\x80\x94machines. The AI system in your car will get you safely home, but won\xe2\x80\x99t choose another destination once you\xe2\x80\x99ve gone inside. From there, we\xe2\x80\x99ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it\xe2\x80\x99s easy to imagine them inheriting human-like qualities\xe2\x80\x94and flaws. Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no reason to create AI's that experience emotions. Obviously Yann disagrees. So the question is: what end would be served by doing this? Does an AI need emotions to serve as a useful tool? "	132	0	1	0
1701	-1	0	0	3	b'Inspired by this discussion about recognizing human actions, I have found the Fall-Detection project which detects humans falling on the ground from a CCTV camera feed, and which can consider alerting the hospital authorities.My question is, are there any existing real-life implementations or research projects which specifically use live video feed from the surveillance cameras in order to detect crime using convnets (or similar approaches)? If so, how do they work, briefly? Do they automatically inform the police about the crime with the details what happened and where?For example car accidents, physical assaults, robberies, violent disturbances, weapon attacks, etc.'	98	0	1	0
1702	1701	1	7228	4	b'After a bit of research I found something kind of close:Artificially intelligent security cameras are spotting crimes before they happenNew surveillance cameras will use computer eyes to find \'pre crimes\' by detecting suspicious behaviour and calling for guardsCCTV \'fightcams\' detect violence \'before it happens\' at Dailymail, also check at TelegraphThey, however, makes no mention of what specific methods they use.So a crime detection system as that is written does not exist, but abnormal behaviour detection systems do.An accurate generalized system seems intuitively infeasible, however. Commiting a crime, unlike falling, is a complex behavior, and takes so many forms. A camera watching a store\'s counter like at a 7-11 could perhaps see that the \'customer\'\'s arm is strangely reaching across the counter, and the attendant is moving a lot more than usual suddenly, but aside from very specific cases like this such a system is currently quite unfeasible. Crimes are unusual, relatively speaking, and their dramatic nature means that even the simplest crimes play out in very different ways. Perhaps you could in this case you could try to look for images of a gun, or someone with their hands up. So, looking for unusual, detectable behavioural mannerisms may be possible, but not crime detection.Ultimately, while you may be able to make (possibly pretty good) systems to detect specific crimes in specific environments, that\'s all we got for now.P.S. - Do these camera\'s also get audio signals? That is also an interesting facet to consider ("PUT YOUR HANDS UP / GIVE ME ALL YOUR MONEY")'	253	0	4	0
1703	1700	0	9956	6	b'The answer, unlike for many questions on this board, I think is definitive. No. We don\'t need AI\'s to have emotion to be useful, as we can see by the numerous amount\'s of AI\'s we have already that are useful.But to further address the question, we can\'t really give AI\'s emotions. I think the closest we could get would be \'Can we make this AI act in a way a human would if that human was insert emotion \'.To what end? the only immediate thing coming to mind would be to create more lifelike companions or interactions, for the purposes of video games or demo\'s or for fun. I heartily agree, however, that at least of any AI system I\'ve ever considered, emotions would not better it.Yann says that doing so would give our AI\'s more human-like qualities and flaws. I think it\'s more like it would \'give our AI\'s more human-like qualities or in other words flaws\'.The purpose of AI\'s and learning systems is to create systems that act or \'think\' like humans, but better. Systems that can adapt or evolve, but mess up as little as possible. "To err is human. To not is AI" (or what we strive for)'	201	0	0	0
1704	1691	1	21470	2	b"Wolfram's image id system is specifically meant to figure out what the image is depicting, not the medium. To get what you want you'd simply have to create your own system where the training data is labeled by the medium rather than the content, and probably fiddle with it to pay more attention to texture and things as such as that. The neural net doesn't care which we want - it has no inherent bias. It just knows what it's been trained for.That's really all there is to it. It's all to do with the training labels and the focus of the system (e.g. a system that looks for edge patterns that form shapes, compared to a system that checks if the lines in the image are perfectly computer-generated straight and clean vs imperfect brush strokes vs spraypaint).Now, if you want me to tell you how to build that system, I'm not the right person to ask haha"	157	0	0	0
1705	-1	0	0	6	b"I'm trying to come up with the right algorithm for a system in which the user enters a few symptoms and the system has to predict or determine the likelihood that a few selected symptoms are associated with those existing in the system. Then after associating them, the result or output should be a specific disease for the symptoms.The system is comprised of a series of diseases with each assigned to specific symptoms, which also exist in the system.Let's assume that the user entered the following input:The first thing the system should do is check and associate each symptom (in this case represented by alphabetical letters) individually against a data-table of symptoms that already exist. And in cases where the input doesn't exist, the system should report or send feedback about it.And also, let's say that A and B was in the data-table, so we are 100% sure that they're valid or exist and the system is able to give out the disease based on the input. Then let's say that the input now is C and D where C doesn't exist in the data-table, but there is a possibility that D exists.We don't give D a score of 100%, but maybe something lower (let's say 90%). Then C just doesn't exist at all in the data-table. So, C gets a score of 0%.Therefore, the system should have some kind of association and prediction techniques or rules to output the result by judging the user's input.Summary of generating the output:What techniques would be used to produce this system?"	257	4	0	0
1706	-1	0	0	5	b'I have gone through the wikipedia explanation of SRL. But, it only confused me more: Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure.Can someone give a more dumbed down explanation of the same, preferably with an example?'	63	0	0	0
1708	1706	1	36454	5	b'The University of Maryland published some slides (PDF) from an introductory presentation on this topic.The fourth page explains why SRL is interesting. "Traditional statistical machine learning approaches" process one sort of thing in which there is some uncertaintly. Image identification is a good example of that. "Traditional ILP/relational learning approaches" use several kinds of information to produce hypotheses about the data set, but apparently allow for no noise in the data.Statistical relational learning models are intended to work with data sets that have several types of objects connected to each other via various links (hence "relational"). They also are meant to deal with uncertainty (hence "statistical").Skipping past some slides that aren\'t really useful without a transcript of what was said over them, we come to slide 17, which has comprehensible definitions and examples:  Object classification   Predicting the category of an object based on its attributes and its links and attributes of linked objects e.g., predicting the topic of a paper based on the words used in the paper the topics of papers it cites the used in the paper, the topics of papers it cites, the research interests of the author  Object type prediction   Predicting the type of an object based on its attributes and its links and attributes of linked objects e.g., predict the venue type of a publication (conference, journal, workshop) based on properties of the paper  As you can see, these models can keep track of several things and the interactions between them. The next slide talks about link prediction, the ability to predict several attributes of connections between objects, like the importance/quality of a citation. As previously mentioned, these models don\'t require 100% accurate data to give interesting results; academic citation lists might occasionally be less than comprehensive, and the importance of a citation is challenging to quantify.Like ILP, SLP will hopefully be able to "see" new kinds of links between "entities", as with the presentation\'s example of identifying research communities.Past slide 20, the presentation goes into some serious mathematics. It does have a much less technical conclusion starting at slide 198. '	353	0	0	0
1709	1705	0	72882	7	b"I think you're coming at your problem slightly wrong... what you're essentially talking about is a belief network.You may want to look into existing Bayesian Learning techniques to get your head around this, but belief networks commonly use the exact scenario you're talking about; using a set of known (or uncertain facts) statements to produce some inferred probability of a particular output. Even more, they often express this through disease-symptom based examples in tutorials! Try here.My point being that it would be better to use a belief network as the theory groundwork is all already there for you, instead of an ANN."	101	0	1	0
1710	-1	0	0	1	b"The obvious solution is to ensure that the training data is balanced - but in my particular case that is impossible. What corrections can one perform in such a scenario?I know that my training data is heavily biased towards a particular class, say, and I cannot change that. Moreover, the labels are very noisy. Conditioned on this piece of information, is there anything I can do by tweaking the training process itself/ something else, to correct for the bias in the training data?The data comes from an experiment (from an electron microscope), and I cannot collect more data. It's always going to be biased in this way, so alternatively-biased is also not an option. I'm sorry that I'm unable to provide any more details due to confidentiality."	126	0	0	0
1711	1710	0	8996	2	b'I feel like from the information your giving (some sort of biased data) you cant get an answer as robust as you\'d like (what algorithmic changes can be made). In general, the reason these methods like DNN\'s work is that they learn off of the data. What you train it to do is what it is capable of, and there\'s little one can do to \'balance\' it to classes of data it just never sees. It\'s like training someone to do algebra then giving them a trigonometry test. It\'s all math, sure, but you just never can expect much without the proper learning.That being said, you should perhaps look at other methods to work with this data, or to approach the problem. Given that you cannot collect unbiased data and that you can\'t explain more due to confidentiality, I really doubt anyone here can help you that much.I can at most point you to this article"Classification on Data with Biased Class Distribution"https://ai2-s2-pdfs.s3.amazonaws.com/277c/3795f7a66fda3fd70607d1fb45b66730c7ba.pdfAnd suggest that perhaps your current approach may not be the most approrpiate given the unfortunate circumstances.'	177	0	0	0
1713	-1	0	0	2	b"Note: I wanted to ask a meta-post first to see if this site was supposed to be used only for AI-related questions, or if AI-related questions such as this were allowed, too, but apparently you need to have asked five actual questions first.I'm going to be entering a masters computer science program in the fall, and I wanted to move towards a concentration in computational neuroscience and linguistics for AI development applications. While I have a math and CS background, I have almost no biology/neuroscience background, and my linguistics background is limited to the random research I've done in my spare time to satiate my curiosities.What are good non-math and CS related topics to study for these fields? "	118	0	0	0
1714	1700	0	21581	2	b'I think the fundamental question is: Why even attempt to build an AI? If that objective is clear, it will provide clarity to whether or not having emotional quotient in AI make sense. Some attempts like "Paro" that were developed for therapeutic reasons requires they exhibit some human like emotions. Again, note that "displaying" emotions and "feeling" emotions are two completely different things. You can program a thing like paro to modulate the voice tones or facial twitches to express sympathy, affection, companionship, or whatever - but while doing so, a paro does NOT empathize with its owner - it is simply faking it by performing the physical manifestations of an emotion. It never "feels" anything remotely closer to what that emotion evokes in human brain. So this distinction is really important. For you to feel something, there needs to be an independent autonomous subject that has the capacity to feel. Feeling cannot be imposed by an external human agent. So going back to the question of what purpose it solves - answer really is - It depends. And the most I think we will achieve ever with silicone based AIs will remain the domain of just physical representations of emotions. '	201	0	0	0
1715	1691	0	23189	0	b'If I look at the image, I can kind of see a monocle as part of the image. So one part of this is that the classifier is ignoring much of the image. This could be called a lack of "completeness", in the sense used here (a computer vision paper on image summarization).One way to discover these sorts of failure modes is adversarial images, which are optimized to fool a given image classifier. Building on this, the idea of adversarial training is to simultaneously train competing "machines", one trying to synthesize data, the other trying to find weaknesses in the first one.Also check this page: A path to unsupervised learning through adversarial networks, for further information about adversarial training.'	118	0	1	0
1716	-1	0	0	1	b'From Eliza to A.L.I.C.E.: Weizenbaum tells us that he was shocked by the experience of releasing ELIZA (also known as "Doctor") to the nontechnical staff at the MIT AI Lab. Secretaries and nontechnical administrative staff thought the machine was a "real" therapist, and spent hours revealing their personal problems to the program. When Weizenbaum informed his secretary that he, of course, had access to the logs of all the conversations, she reacted with outrage at this invasion of her privacy. Weizenbaum was shocked by this and similar incidents to find that such a simple program could so easily deceive a naive user into revealing personal information.Wikipedia\'s article on the "ELIZA Effect": Though designed strictly as a mechanism to support "natural language conversation" with a computer, ELIZA\'s DOCTOR script was found to be surprisingly successful in eliciting emotional responses from users who, in the course of interacting with the program, began to ascribe understanding and motivation to the program\'s output. As Weizenbaum later wrote, "I had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people." Indeed, ELIZA\'s code had not been designed to evoke this reaction in the first place. Upon observation, researchers discovered users unconsciously assuming ELIZA\'s questions implied interest and emotional involvement in the topics discussed, even when they consciously knew that ELIZA did not simulate emotion.ELIZA, despite its simplicity, was incredibly successful at its task of tricking other human beings. Even those who knew ELIZA was a bot would still talk to it. Obviously, ELIZA served as an inspiration for various other, more intelligent chatbots, such as Xiaoice. But I would like to know what exactly led to such a simple program like ELIZA to be so successful in the first place.This is very useful knowledge for a programmer since a simple program is one that would be easily maintainable.'	314	0	2	0
1717	1716	1	2202	2	b'I like your choice of "induce" instead of "produce," because the delusions came from the users. This means the answer has to do mostly with human psychology; people come equipped with lots of mental machinery specialized for dealing with other humans and not very much mental machinery specialized for dealing with software. So ELIZA behaved in ways that some people classified it as a person and behaved accordingly, and others didn\'t.What features will trip up a person\'s internal person classification system seem like they vary heavily from person to person, and also with experience and familiarity. Going into more detail is, as mentioned in the comments, more appropriate for sites specializing on the human side of the keyboard.'	117	0	0	0
1722	1420	0	49833	1	b"Our current approaches to AI are too inefficient to result in anything remotely close to what an average human would perceive as artificial senient beings.Current approaches to AI involve a simulation of our own capacity for learning by creating fully functional computation machines capable of re-programming themselves. While that's definitely a good start with respect to understanding the nature of intelligence, it's still a far cry from actually creating genuine artificial intelligence.It is not just our capacity to learn that evolved. Our very brains themselves evolved from rudimentary biochemical components at the intra-cellular level to the fascinating, complex organs they are today, along with our bodies as a whole evolving from simple single cell life to homo sapiens. So to create genuine artificial intelligence, it may actually make most sense to first start with replicating that process : creating artificial life with the capacity to evolve. It may actually make most sense to first start with creating artificial DNA and artificial cells, and move on from there.Anyway, in this article as well as this article, Silicon Valley renegade Alex St John goes in greater detail on why something like Skynet, V.I.K.I. or anything like it is unlikely in the near future and may even never be within our grasp and why our current approach to artificial intelligence is a bad one."	220	0	3	0
1724	1420	0	74537	0	b'Based on the success of IBM Watson and the amazing advances in tackling numerous hard tasks using deep learning in the past 3 years, I think a large high-tech company like Google or Amazon will create a useful conversational bot in no more than 10 years. (I\'ve worked on the fringes of AI for 25 years and have followed the tech for even longer. These are exciting times.)Initially, your very own AI companion ("Her"?) won\'t be capable of deeper philosophical conversation or insightful interpretation of novels or the human condition. But it will be able to write / speak in full paragraphs on topics like the best choice among 5 possible routes between point A and B, or summarizing the plot of a book or the gist of a news story, or why one product is better than another (e.g. based on assessing hundreds of Amazon reviews). And yes, it will be able to understand full spoken sentences from you, and generate both queries and answers.I\'m convinced such a bot will be useful enough that most of us will want one. Of course you won\'t need to buy a special piece of hardware, like the Amazon Echo. It\'ll be available via software on your smartphone, though the computing is likely to reside on the cloud (since that\'s where the data is). Frankly, I think this is where the next innovations in smartphones will arise -- verbal interfaces that do a better job hearing and speaking and disambiguating using context about you and the kinds of questions you are likely to ask.'	260	0	0	0
1725	1700	0	77678	3	b'I think emotions are not necessary for an AI agent to be useful. But I also think they could make the agent MUCH more pleasant to work with. If the bot you\'re talking with can read your emotions and respond constructively, the experience of interacting with it will be tremendously more pleasant, perhaps spectacularly so.Imagine contacting a human call center representative today with a complaint about your bill or a product. You anticipate conflict. You may have even decided NOT to call because you know this experience is going to be painful, either combative or frustrating, as someone misunderstands what you say or responds hostilely or stupidly.Now imagine calling the kindest smartest most focused customer support person you\'ve ever met -- Commander Data -- whose only reason for existing is to make this phone call as pleasant and productive for you as possible. A big improvement over most call reps, yes? Imagine then if call rep Data could also anticipate your mood and respond appropriately to your complaints to defuse your emotional state... you\'d want to marry this guy. You\'d call up call rep Data any time you were feeling blue or bored or you wanted to share some happy news. This guy would become your best friend overnight -- literally love at first call.I\'m convinced this scenario is valid. I\'ve noticed in myself a surprising amount of attraction for characters like Data or Sonny from "I Robot". The voice is very soothing and puts me instantly at ease. If the bot were also very smart, patient, knowledgable, and understanding... I really think such a bot, embued with a healthy dose of emotional intelligence, could be enormously pleasant to interact with. Much more rewarding than any person I know. And I think that\'s true of not just me.So yes, I think there\'s great value in tuning a robot\'s personality using emotions and emotional awareness.'	313	0	0	0
1726	152	0	66070	2	b"It is a labor intensive process, but that does sound excessive. If you have a g2.8xlarge, make sure you are using the using the GPU flags for neural-style, which will cut your render time by an order of magnitude. That having been said, it is building a rather large network (depending on your parameters), and a 1024x768 image is a lot of input to work with. It will take time, but shouldn't take more than a couple hours with the gpu flag enabled correctly. "	84	0	0	0
1727	1534	1	50333	6	b'They are all called Monte Carlo because all of them are a different version of the canonical Monte Carlo algorithm.The canonical version of Monte Carlo algorithm is a stochastic algorithm to determine an action based in a tree representation. The differences among all these version are their exploration and exploitation mechanisms, and it is necessary to analyse each of them to define which one fits in your case. '	68	0	0	0
1728	152	1	71901	3	b'Real time style transfer and neural doodle is very much possible and is an active topic I see users working on to improve upon. The basic idea is to do only feed forward propagation at test time and train with appropriate loss functions at train time.Perceptual Losses for Real-Time Style Transferand Super-Resolution is a good starting point to understand a methodology for this purpose.'	63	0	1	0
1729	1348	0	66194	0	b"Consider Asimov's first law of robotics: A robot may not injure a human being or, through inaction, allow a human being to come to harm.That law is already problematic, when taking into consideration self-driving cars. What's the issue here, you ask? Well, you'll probably be familiar with the classic thought experiment in ethic known as the trolley problem. The general form of the problem is this: The trolley is headed straight for them. You are standing some distance off in the train yard, next to a lever. If you pull this lever, the trolley will switch to a different set of tracks. However, you notice that there is one person on the side track. You have two options: (1) Do nothing, and the trolley kills the five people on the main track. (2) Pull the lever, diverting the trolley onto the side track where it will kill one person. Which is the correct choice?  source : WikipediaSelf-driving cars will actually need to implement real life variations on the trolley problem, which basically means that self-driving cars need to be programmed to kill human beings.Of course that doesn't mean that ALL robots will need to be programmed to kill, but self-driving cars are a good example of a type of robot that will."	212	0	0	0
1730	1655	0	63182	1	b'I think the algorithm has changed minimally, but the necessary hardware has been trimmed to the bone.The number of gate transitions are reduced (perhaps float ops and precision too), as are the number of data move operations, thus saving both power and runtime. Google suggests their TPU achieves a 10X cost saving to get the same work done.https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html'	57	0	0	0
1731	-1	0	0	9	b"What regulations are already in place regarding Artificial General Intelligences? What reports or recommendations prepared by official government authorities were already published?So far I know of Sir David King's report done for UK government."	33	0	1	0
1733	-1	0	0	5	b"Most introductions to the field of MDPs and Reinforcement learning focus exclusively on domains where space and action variables are integers (and finite).This way we are introduced quickly to Value Iteration, Q-Learning and the like.However the most interesting applications (say, flying helicopters) of RL and MDPs involve continuous state space and action spaces.I'd like to go beyond basic introductions and focus on these cases but I am not sure how to get there. Are there any research projects or studies that deal with these cases in depth?"	86	0	1	0
1734	1655	1	29978	1	b'Tensor operationsThe major work in most ML applications is simply a set of (very large) tensor operations e.g. matrix multiplication. You can do that easily in an ASIC, and all the other algorithms can just run on top of that.'	39	0	0	0
1735	1733	1	23057	5	b"There is a small survey of continuous states, actions and time in reinforcement learning in my thesis proposal.Regarding books, Reinforcement Learning: State-of-the-Art seems to be pretty up-to-date from the excerpts I've read."	31	0	1	0
1737	26	0	85300	1	b'So you may be familiar with Word2Vec, (W2V) which as Wikipedia describes1 "captures the linguistic contexts of words" using vector arithmetic. For example, subtract \'Paris\' from \'France\' and add \'Italy\' and you get \'Rome\'.What you need is something like a Sentiment2Vec (S2V) that captures the similarities between emotional transitions. Something like: subtract \'fear\' from \'sadness\', add \'joy\' and you get \'hope\'. Or: subtract \'sting\' from \'papercut\', add \'smashed\' and you get \'throbbing\'.The catch is that you don\'t have an easily accessible corpus of emotional contexts to train with, like you have with words. If you had a million hours of fMRI - mapping the transitions between emotions in hundreds of subjects - then you could use that data to build an S2V. You probably don\'t have that data though.In the mean time, you could just build a W2V that specializes in sentiment. You could even try to use a current sentiment analysis engine to bootstrap it. Perhaps if you read enough text that says "I got a papercut and it stings" and "I smashed my finger and it\'s throbbing" then you could eventually produce an S2V. Children\'s books often use explicit language regarding emotional context ("this made the boy feel sad").But words are still a far cry from the experiential context that a connectome map would provide. To test whether you have something useful or not, you might want to implement your S2V in a mouse foraging simulation - see whether it produces typical behavior and if any cooperative or competitive dynamics can organically grow out of your S2V.Some further info on the subject: In 2014, Glasgow University claimed2 that there are four primary emotions: happiness, sadness, fear and anger. This website3 provides nice (if somewhat short) hierarchical breakdown of secondary and tertiary emotions under primary emotions.References1: en.wikipedia.org/wiki/Word2vec2: www.bbc.com/news/uk-scotland-glasgow-west-260195863: changingminds.org/explanations/emotions/basic%20emotions.htm'	299	0	6	0
1738	1348	0	58258	1	b'Asimov made the three laws specifically to prove that no three laws are sufficient, no matter how reasonable they seem at first. I know a guy that knew the guy and he confirmed this.'	33	0	0	0
1740	1593	0	75148	1	b'"Usefulness" can only be measured against some purpose. Once you pass AGI - which really means "generally animal-like AI, because it seems general to us" - then you\'ve passed into a world of potentially undefined behavior.Part of what makes a human free and sets us apart from the other animals is the fact that our purposes, capabilities and possibilities aren\'t fully defined. We\'re open ended.To clarify terms, I interpret "Strong AGI" as "potentially super intelligence, but at least human level."When we say "Strong AGI" vs just "AGI," we\'re not saying that one is more open ended than the other. We are saying that the stronger one is simply smarter on some axis.So to ask whether a particular trait would be "more useful" to a Strong AGI - that would depend on the purpose of the AGI. But here\'s the catch: if a thing had just one purpose, then the most efficient solution to fulfilling that one purpose will always be a narrow solution, not a general one. When the purpose of the object is known before hand, giving that object more general capability than is necessary for that purpose is counterproductive.That\'s why it\'s impossible to make declarative prescriptions about what a free, open-ended AGI should or shouldn\'t need. Such prescriptions would nullify the open-ended freedom of utility that its generality implies. We can speak declaratively about lesser robots and animals.But for any given problem, the solution we will want to find is the most well-defined, narrow, efficient one available - not the most general one.In other words, sure, personalities could be useful for a Strong AGI, assuming the problems in question involved personalities.'	272	0	0	0
1741	1480	1	58390	6	b"The neural network is typically a set size once it's created. You'd have to create a network big enough for your data-set."	21	0	0	0
1742	-1	0	0	1	b'Can someone explain to me the difference between machine learning and deep learning? Is it possible to learn deep learning without knowing machine learning?'	23	0	0	0
1743	1742	1	5792	5	b"Deep learning is a specific variety of a specific type of machine learning. So it's possible to learn about deep learning without learning all of machine learning, but it requires learning some machine learning (because it is some machine learning).Machine learning refers to any technique that focuses on teaching the machine how it can learn statistical parameters from a large amount of training data. One particular type of machine learning is artificial neural networks, which learn a network of nonlinear transformations that can approximate very complicated functions of wide arrays of input variables. Recent advances in artificial neural networks have to do with how to train deep neural networks, which have more layers than normal and also special structure to deal with the challenges of learning more layers."	127	0	0	0
1744	1479	0	30320	2	b"Here is an answer by Carlos E. Perez to the question What is theory behind deep learning? [...]  The underlying mathematics of Deep Learning has been in existence for several decades, however the impressive results that we see today are part a consequence of much faster hardware, more data and incremental improvements in methods.   Deep Learning in general can be framed as optimization problem where the objective is a function of the model error. This optimization problem is very difficult to solve consider that the parameter space of the model (i.e. weights of the neural network) leads to a problem in extremely high dimension. An optimization algorithm could take a very long time to explore this space. Furthermore, there was an unverified belief that the problem was non-convex and computation would forever be stuck in local minima.  [...]  The theory of why machines actually converge to an attractor or in other words learn to recognize complex patterns is still unknown.To sum up: we have some ideas, but we're not quite sure."	175	0	0	0
1745	218	1	3053	1	b' In general, how algorithm should distinguish the word meaning and recognise the word within the context?I don\'t think anybody knows how to answer this for the general case. If they did, they\'d have basically solved AGI. But we can certainly talk about techniques that get part-of-the-way there, and approaches that could work.One thing I would consider trying (and I don\'t know off-hand if anybody has tried this exact approach) is to model the disambiguation of each word as a discrete problem for a Bayesian Belief Network where your priors (for any given word) are based on both stored "knowledge" as well as the previously encountered words in the (sentence|paragraph|document|whatever). So if you "know", for example, that "Reading is a city in the UK" and that "place names are usually capitalized", your network should be strongly biased towards interpreting "Reading" as the city, since nothing in the word position in the sentence strongly contradicts that. Of course I\'m hand-waving around some tricky problems in saying that, as knowledge representation isn\'t exactly a solved problem either. But there are at least approaches out there that you could use. For example, you could use the RDF / triple based approach from the Semantic Web world. Finding a good way to merge that stuff with a Bayesian framework could yield some interesting results. There has been a bit of research on "probabilistic RDF" that you could possibly use as a starting point. For example:https://www.w3.org/2005/03/07-yoshio-UMBC/'	240	0	4	0
1746	179	0	36793	0	b'On possibility is a blackboard architecture. Envision each different "kind" of intelligence as a discrete agent, and let the agents collaborate using the blackboard model. Now you have an AI with multiple intelligences. This is something I\'ve actually been experimenting with, and while I don\'t have any particularly impressive results to share or anything, I hold a strong belief that an approach that is at least somewhat like this will be crucial to developing an AGI. And that is rooted in my belief that the human mind does have "multiple intelligences" and that they collaborate something like this.'	97	0	0	0
1747	1731	0	63189	2	b'I don\'t know that it has yielded any actual reports or regulations yet, but in the USA, the White House has been running a series of interagency workshops / working groups dedicated to "Preparing for the Future of Artificial Intelligence". https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligenceSome of those sessions have been dedicated to legal / governance issues. '	52	0	0	0
1748	212	0	22666	1	b'I would think you could use a graph database, perhaps Neo4J or Titan or something of that nature. Or, if you want a simple file format, you could use one of the many formats that exist for representing graphs. You can find a list and overview of some of them here.Another option would be to store them in RDF using a triplestore like Jena. '	64	0	1	0
1749	212	0	37742	1	b'If I understand you correctly, you should check out Word2Vec. From Wikipedia:  Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a high-dimensional space (typically of several hundred dimensions), with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.'	106	0	0	0
1750	-1	0	0	5	b"By new, unseen examples; I mean like the animals in No Man's Sky. A couple of images of the animals are:So, upon playing this game, I was curious about how good is AI at generating visual characters or examples?"	38	0	0	0
1751	-1	0	0	5	b'I wanted to know what the differences between hyper-heuristics and meta-heuristics are, and what their main applications are. Which problems are suited to be solved by Hyper-heuristics?'	26	0	0	0
1753	1750	1	29623	5	b'We are getting pretty good at image generation, some examples:Radford, Alec, Luke Metz, and Soumith Chintala. "Unsupervised representation learning with deep convolutional generative adversarial networks." arXiv preprint arXiv:1511.06434 (2015). https://arxiv.org/pdf/1511.06434.pdfGregor, Karol, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. "DRAW: A recurrent neural network for image generation." arXiv preprint arXiv:1502.04623 (2015). https://arxiv.org/pdf/1502.04623.pdfFrom (1):Then there is another research direction around evolutionary algorithms, for example:Sims, Karl. "Evolving virtual creatures." In Proceedings of the 21st annual conference on Computer graphics and interactive techniques, pp. 15-22. ACM, 1994. https://scholar.google.com/scholar?cluster=6031059536657676358&amp;hl=en&amp;as_sdt=0,22 ; https://www.youtube.com/watch?v=bBt0imn77Zg'	88	0	0	0
1754	26	0	8166	0	b'The way i do emotion in a AGI system are by a bunch of littleparts, agents, voting in system statis state registers. If the unionof agents are working together correctly. This is the subconscious part.The conscious part that plan out movement in the environmentinclude these system statis state registers in all planned movements.All emotions can be derived from these registers:https://groups.google.com/forum/#!topic/artificial-general-intelligence/pxWmHClAAdA https://groups.google.com/forum/#!topic/artificial-general-intelligence/jWdzPaxYHmU https://groups.google.com/forum/#!forum/artificial-general-intelligence '	62	0	0	0
1755	1751	1	22935	8	b'TL:DR: Hyper-heuristics are metaheuristics, suited for solving the same kind of optimization problems, but (in principle) affording a "rapid prototyping" approach for non-expert practitioners. In practice, there are issues with the prevailing approach, motivating an emerging perspective on \'whitebox\' hyper-heuristics.In more detail:Metaheuristics are methods for searching an intractably large space of possible solutions in order to find a \'high quality\' solution. Popular metaheuristics include Simulated Annealing, Tabu Search, Genetic Algorithms etc.The essential difference between metaheuristics and hyper-heuristics is the addition of a level of search indirection: informally, hyper-heuristics can be described as \'heuristics for searching the space of heuristics\'. One can therefore use any metaheuristic as a hyper-heuristic, providing the nature of the \'space of heuristics\' to be searched is appropriately defined.The application area for hyper-heuristics is therefore the same as metaheuristics. Their applicability (relative to metaheuristics) is as a \'rapid prototyping tool\': the original motivation was to allow non-expert practitioners to apply metaheuristics to their specific optimization problem (e.g. "Travelling-Salesman (TSP) plus time-windows plus bin-packing") without requiring expertise in the highly-specific problem domain. The idea was that this could be done by:Allowing practitioners to implement only very simple (effectively,randomized) heuristics for transforming potential solutions. Forexample, for the TSP: "swap two random cities" rather than (say) the morecomplex Lin-Kernighan heuristic. Achieve effective results (despite using these simple heuristics) by combining/sequencing them in an intelligent way, typically by employing some form of learning mechanism.Hyper-heuristics can be described as \'selective\' or \'generative\' depending on whether the heuristics are (respectively) sequenced or combined. Generative hyper-heuristics thus often use methods such as Genetic Programming to combine primitive heuristics and are therefore typically customized by the practitioner to solve a specific problem. For example, the original paper on generative hyper-heuristics used a Learning Classifier System to combine heuristics for bin-packing. Because generative approaches are problem-specific, the comments below do not apply to them.In contrast, the original motivator for selective hyper-heuristics was that researchers would be able to create a hyper-heuristic solver that was then likely to work well in an unseen problem domain, using only simple randomized heuristics.The way that this has traditionally been implemented was via the introduction of the \'hyper-heuristic domain barrier\' (see figure, below), whereby generality across problem domains is claimed to be achievable by preventing the solver from having knowledge of the domain on which it is operating. Instead, it would solve the problem by operating only on opaque integer indices into a list of available heuristics (e.g. in the manner of the \'Multi-armed Bandit Problem\').In practice, this \'domain blind\' approach has not resulted in solutions of sufficient quality. In order to achieve results anywhere comparable to problem-specific metaheuristics, hyper-heuristic researchers have had to implement complex problem-specific heuristics, thereby failing in the goal of rapid prototyping.It is still possible in principle to create a selective hyper-heuristic solver which is capable of generalizing to new problem domains, but this has been made more difficult since the above notion of domain barrier means that only a very limited feature set is available for cross-domain learning (e.g. as exemplified by a popular selective hyper-heuristic framework).A more recent research perspective towards \'whitebox\' hyper-heuristics advocates a declarative, feature-rich approach to describing problem domains. This approach has a number of claimed advantages:Practitioners now need no longer implement heuristics, but rather simply specify the problem domain.It eliminates the domain-barrier, putting hyper-heuristics on the same \'informed\' status about the problem as problem-specific metaheuristics.With a whitebox problem description, the infamous \'No Free Lunch\' theorem (which essentially states that, considered over the space of all black box problems, Simulated Annealing with an infinite annealing schedule is, on average, as good as any other approach) no longer applies.DISCLAIMER: I work in this research area, and it is therefore impossible to remove all personal bias from the answer.'	624	0	5	0
1756	-1	0	0	1	b'What is the difference between agent function and agent program with respect to percept sequence?In the book "Artificial Intelligence: A modern approach", The agent function, notionally speaking, takes as input the entire percept sequence up to that point, whereas the agent program takes the current percept only.Why does the agent program only take current percept. Isn\'t it just implementation of the agent function?'	62	0	0	0
1757	1756	0	6011	3	b"It looks as if 'function' is being used here in the mathematical (or functional programming) sense of 'pure function', i.e. it is without state or side-effects. Hence the function cannot store previous percepts anywhere, so the entire historical percept sequence is considered to be passed to the function each time.In contrast, the notion of 'program' appears to allow state/side-effects, so it is assumed that earlier percepts are memoized as needed (or that they otherwise updated the variables used within the program).The 'function' notion is the conceptually cleaner one, in that the 'program' version can always be abstracted to the functional one. Which aspects of percept history happen to be cached by the 'program' version is merely an implementation detail."	118	0	0	0
1761	-1	0	0	2	b'Are there currently any studies to simulate gradual (or sudden) implementation of AIs in the general work force?'	17	0	0	0
1763	1544	0	75012	1	b'It\'s a well known concept that\'s already usedWhat we call "curiosity" in humans and animals is in effect the chosen level of the "exploit vs explore" tradeoff for any active system. For example, the field of reinforcement learning is one approach that studies implementations of what essentially is the equivalent of curiosity; and we have research on how much curiosity is best e.g. multi-armed bandit concept.So "using curiosity" is something that we already do as much as we can/should/are able to, but it would usually be called in some other, more specific term to specify the exact meaning instead of the vague word of "curiosity".'	104	0	0	0
1768	-1	0	0	118	b'In Portal 2 we see that AI\'s can be "killed" by thinking about a paradox.I assume this works by forcing the AI into an infinite loop which would essentially "freeze" the computer\'s consciousness.Questions: Would this confuse the AI technology we have today to the point of destroying it? If so, why? And if not, could it be possible in the future?'	60	0	0	0
1769	1768	1	4948	103	b'This classic problem exhibits a basic misunderstanding of what an artificial general intelligence would likely entail. First, consider this programmer\'s joke: The programmer\'s wife couldn\'t take it anymore. Every discussion with her husband turned into an argument over semantics, picking over every piece of trivial detail. One day she sent him to the grocery store to pick up some eggs. On his way out the door, she said, "While you are there, pick up milk."  And he never returned.It\'s a cute play on words, but it isn\'t terribly realistic.You are assuming because AI is being executed by a computer, it must exhibit this same level of linear, unwavering pedantry outlined in this joke. But AI isn\'t simply some long-winded computer program hard-coded with enough if-statements and while-loops to account for every possible input and follow the prescribe results. This would not be strong AI. In any classic definition of artificial general intelligence, you are creating a system that mimics some form of cognition that exhibits problem solving and adaptive learning (&larr;note this phrase here). I would suggest that any AI that could get stuck in such an "infinite loop" isn\'t a learning AI at all. It\'s just a buggy inference engine. Essentially, you are endowing a program of currently-unreachable sophistication with an inability to postulate if there is a solution to a simple problem at all. I can just as easily say "walk through that closed door" or "pick yourself up off the ground" or even "turn on that pencil" &mdash; and present a similar conundrum.  "Everything I say is false." &mdash; The Liar\'s Paradox'	266	2	0	0
1770	1768	0	5475	32	b'This popular meme originated in the era of \'Good Old Fashioned AI\' (GOFAI), when the belief was that intelligence could usefully be defined entirely in terms of logic.The meme seems to rely on the AI parsing commands using a theorem prover, the idea presumably being that it\'s driven into some kind of infinite loop by trying to prove an unprovable or inconsistent statement.Nowadays, GOFAI methods have been replaced by \'environment and percept sequences\', which are not generally characterized in such an inflexible fashion. It would not take a great deal of sophisticated metacognition for a robot to observe that, after a while, its deliberations were getting in the way of useful work.Rodney Brooks touched on this when speaking about the behavior of the robot in Spielberg\'s AI film, (which waited patiently for 5,000 years), saying something like "My robots wouldn\'t do that - they\'d get bored". EDIT: If you really want to kill an AI that operates in terms of percepts, you\'ll need to work quite a bit harder. This paper (which was mentioned in this question) discusses what notions of death/suicide might mean in such a case.EDIT2: Douglas Hofstadter has written quite extensively around this subject, using terms such as \'JOOTSing\' (\'Jumping Out Of The System\') and \'anti-Sphexishness\', the latter referring to the loopy automata-like behaviour of the Sphex Wasp (though the reality of this behaviour has also been questioned).'	230	0	3	0
1771	1761	0	68516	2	b"As far as I can tell (I've been doing searches here and there on and off since I saw this question a few hours ago) the closest we've gotten to 'simulations' on this is video-games, and to a degree movies, interestingly enough. I.e. entertainment media.Games like Portal, System Shock (with the AI 'Shodan'), and others give interpretations of what AI systems could be capable of themselves. Mass Effect is more or less entirely based around existential concepts regarding extra-terrestrial, almost primordial AI beings that threaten the earth.But there's even more to it than the whole 'evil robots taking over the world' aspect. There's the actual implementation of AI in video games, which is where much of this technology first makes contact with the general public.We have facial-scanners that put you into NBA games, cities full of realistically reacting people (inFamous, Assassin's Creed), and games that learn how you play and adjust the game accordingly (Metal Gear Solid does some of that stuff, as well as being thematically AI-heavy). Ultimately, we only get things like the iPhone or VR headsets or other major proof-of-concept material only so often, but games are much more frequently implementing the most recent AI advances. Thus, even though many AI systems are being put into place in the general workforce (many Hospitals now turning to cloud and AI health services, as recent as this week), I don't think you can really go further than video games or movies for 'simulations' or extrapolations like the ones you seem to want. Analyzing the response to AI developments in games might be the closest thing currently possible. In terms of economic models or anything of that sort, I can find naught."	281	0	0	0
1772	1768	0	14352	1	b'Well, the issue of anthropomorphizing the AI aside, the answer is "yes, sort of." Depending on how the AI is implemented, it\'s reasonable to say it could get "stuck" trying to resolve a paradox, or decide an undecidable problem. And that\'s the core issue - decidability. A computer can chew on an undecidable program forever (in principle) without finishing. It\'s actually a big issue in the Semantic Web community and everybody who works with automated reasoning. This is, for example, the reason that there are different versions of OWL. OWL-Full is expressive enough to create undecidable situations. OWL-DL and OWL-Lite aren\'t. Anyway, if you have an undecidable problem, that in and of itself might not be a big deal, IF the AI can recognize the problem as undecidable and reply "Sorry, there\'s no way to answer that". OTOH, if the AI failed to recognize the problem as undecidable, it could get stuck forever (or until it runs out of memory, experiences a stack overflow, etc.) trying to resolve things.Of course this ability to say "screw this, this riddle cannot be solved" is one of the things we usually think of as a hallmark of human intelligence today - as opposed to a "stupid" computer that would keep trying forever to solve it. By and large, today\'s AI\'s don\'t have any intrinsic ability to resolve this sort of thing. But it wouldn\'t be that hard for whoever programs an AI to manually add a "short circuit" routine based on elapsed time, number of iterations, memory usage, etc. Hence the "yeah, sort of" nature of this. In principle, a program can spin forever on a paradoxical problem, but in practice it\'s not that hard to keep that from happening.Another interesting question would be, "can you write a program that learns to recognize problems that are highly likely to be undecidable and gives up based on it\'s own reasoning?" '	316	0	0	0
1773	1768	0	16558	6	b"No. This is easily prevented by a number of safety mechanisms that are sure to be present in a well-designed AI system. For example, a timeout could be used. If the AI system is not able to handle a statement or a command after a certain amount of time, the AI could ignore the statement and move on. If a paradox ever does cause an AI to freeze, it's more evidence of specific buggy code rather than a widespread vulnerability of AI in general.In practice, paradoxes tend to be handled in not very exciting ways by AI. To get an idea of this, try presenting a paradox to Siri, Google, or Cortana."	111	0	0	0
1774	-1	0	0	4	b'In the 1950\'s, there were widely-held beliefs that "Artificial Intelligence" will quickly become both self-conscious and smart-enough to win chess with humans. Various people suggested time frames of e.g. 10 years (see Olazaran\'s "Official History of the Perceptron Controversy", or let say 2001: Space Odyssey).When did it become clear that making computers play games like chess is not equal to AGI? Who was the first person to postulate separation of the concept of AGI from task-specific methods?'	76	0	0	0
1775	-1	0	0	1	b'We hear a lot today about how thought vectors are the Next Big Thing in AI, and how they serve as the underlying representation of thought/knowledge in ANN\'s. But how can one use thought vectors in other regimes, especially including symbolic logic / GOFAI? Could thought vectors be the "substrate" that binds together probabilistic approaches to AI and approaches that are rooted in logic? '	64	0	2	0
1776	1768	0	20923	11	b'The halting problem says that it\'s not possible to determine whether any given algorithm will halt. Therefore, while a machine could conceivably recognize some "traps", it couldn\'t test arbitrary execution plans and return EWOULDHANG for non-halting ones.The easiest solution to avoid hanging would be a timeout. For example, the AI controller process could spin off tasks into child processes, which could be unceremoniously terminated after a certain time period (with none of the bizarre effects that you get from trying to abort threads). Some tasks will require more time than others, so it would be best if the AI could measure whether it was making any progress. Spinning for a long time without accomplishing any part of the task (e.g. eliminating one possibility in a list) indicates that the request might be unsolvable.Successful adversarial paradoxes would either cause a hang or state corruption, which would (in a managed environment like the .NET CLR) cause an exception, which would cause the stack to unwind to an exception handler. If there was a bug in the AI that let an important process get wedged in response to bad input, a simple workaround would be to have a watchdog of some kind that reboots the main process at a fixed interval. The Root Access chat bot uses that scheme.'	215	0	1	0
1777	1774	0	8891	1	b'I expect a very precise answer to this question may be lost to the sands of time, although I hope somebody can given such an answer. In the meantime, here\'s one clue on the trail... This anthology of papers from 2007 starts with the following blurb: Our goal in creating this edited volume has been to fill an apparent gap in the scientific literature, by providing a coherent presentation of a body of contemporary research that, in spite of its integral importance, has hitherto kept a very low profile within the scientific and intellectual community. This body of work has not been given a name before; in this book we christen it \xe2\x80\x9cArtificial General Intelligence\xe2\x80\x9d (AGI). What distinguishes AGI work from run-of-the-mill \xe2\x80\x9cartificial intelligence\xe2\x80\x9d research is that it is explicitly focused on engineering general intelligence in the short term.But even if this is the origin of the specific phrase "Artificial General Intelligence", I am pretty sure people were making the distinction between "general intelligence" and "task specific" techniques much earlier. The Wikipedia article on AGI also has a clue, where it states: However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. The agencies that funded AI became skeptical of strong AI and put researchers under increasing pressure to produce useful technology, or "applied AI".That section cites this this book as support for that statement. And indeed, it contains the following verbiage: Although most founders of the AI field continued to pursue basic questions of human and machine intelligence, some of their students and other second-generation researchers began to seek ways to use AI methods and approaches to tackle real-world problems. Their initiatives were important, not only in their own right, but also because they were indicative of a gradual but significant change in the funding environment toward more applied realms of research. The development of expert systems, such as DENDRAL at SAIL, provides but one example of this trend.Given that DENDRAL began around 1965, it appears that some significant body of researchers (or at least funders) became strongly aware of the distinction between research into "general intelligence" and "applied AI" somewhere around the end of the 1960\'s. If you keep reading, other passages support the notion that DARPA in particular started pushing a more "applied" approach to AI research throughout the 1970\'s.So, not a definite answer, but it looks like we can say that the distinction was known and taken into account at least by 1970, although use of the exact term "artificial general intelligence" appears to be of more recent coinage.'	430	0	1	0
1778	1774	0	12192	1	b'In 1973, the British government hired Sir James Lighthill to commission a "general survey" on the state of artificial intelligence. His report was a condemnation of current AI research, leading a wave of pessimism among AI scientists and the First AI Winter. You may view Lighthill\'s report (and contemporary criticism of his report) here, but I will summarize Lighthill\'s key points. Sir James Lighthill divided AI into three categories:Advanced Automation - task-specific workComputer-based CNS research - research into the the "central nervous system" of humansThe Bridge between Advanced Automation and Computer-based CNS research. This bridge would generally be seen as "general-purpose" robotics, so Lighthill would also use the term Building Robots.Advanced Automation (or "applied AI") is obviously useful. Computer-based CNS research is useful because we want to know more about human intelligence. Both fields of AI had some successes, but its practitioners were overly optimistic, leading to disappointment in those fields. Sir James Lighthill was still very supportive of research in these two fields though.Building Robots, on the other hand? Sir James Lighthill was very hostile to the very idea, probably because it was more overly hyped up than the other two categories and produced the least amount of valuable output.He mentioned chess in particular as an example where "robotic" research has failed. At the time the report was published, the chess-playing engines were at the level of "experienced amateur standard characteristic of county club players in England". However, these chess-playing engines relied on heuristics that were made by human beings. The engines weren\'t intelligent at all...they merely were following the heuristics that were created by intelligent humans. The only advantage the robots bring to the table is "speed, reliability and biddability", and even that wasn\'t enough to beat the chess grandmasters.Now, today, we would probably not treat chess as an example of general-purpose problem solving. We would more accurately classify it as "advanced automation", a "narrow AI" problem divorced from broader real-world implications of general problem-solving. But Sir James Lighthill probably would agree with us. He never used the term "narrow AI" and "AGI" (neither of those terms existed yet) but he would write: To sum up, this evidence and all the rest studied by the present author on AI work within category B during the past twenty-five years is to some extent encouraging about programs written to perform in highly specialised problem domains, when the programming takes very full account of the results of human experience and human intelligence within the relevant domain, but is wholly discouraging about general-purpose programs seeking to mimic the problem-solving aspects of human CNS activity over a rather wide field. Such a general- purpose program, the coveted long-term goal of AI activity, seems as remote as ever.Sir James Lighthill believed that the only thing that connects Advanced Automation and Computer-based CNS research is the existence of the Building Roobts "bridge" category. But he\'s very pessimistic about this category actually producing anything worthwhile. So instead, the AI field should instead breakup into its own its constituent parts (automation and research). Any robots that are built could then be specialized within their subfield...either industrial automation or CNS research. Trying to build the holy grail of "general-purpose program" would be worthless...for the time being, at least.'	539	0	1	0
1779	4	0	31119	1	b'You know when you have too many neurons is when you get over fitting.Meaning that it is not working good becauseNN is trying to activate on themost perfect match that is impossible. Like two different cats with the same amount of atoms, or to say, it is a detector NN that only activateson a picture of you pet cat and nothing else. You want a wider rangefor the nn to activate. Like on any picture of cat.Overfitting is a problem that has no real quick fix. You can start with too few and then keep adding more. Or start out witha lot and then removing them until it works right.'	109	0	0	0
1780	1768	0	32844	1	b"It seems to me this is just a probabilistic equation like any other. I'm sure Google handles paradoxical solution sets Billions of times a day, and I can't say my spam filter has ever caused a (ahem) stack overflow. Perhaps one day our programming model will break in a way we can't understand and then all bets are off.But I do take exception to the anthropomorphizing bit. The question was not about the AI of today, but in general. Perhaps one day paradoxes will become triggers for military drones -- anyone trying the above would then, of course, most certainly be treated with hostility, in which case the answer to this question is most definitely yes, and it could even be by design. We can't even communicate verbally with dogs and people love dogs, who is to say we would even necessarily recognize a sentient alternative intelligence? We're already to the point of having to mind what we say in front of computers. O, Tay?"	164	0	0	0
1782	1768	0	50327	9	b'Another similar question might be: "What vulnerabilities does an AI have?""Kill" may not make as much sense with respect to an AI. What we really want to know is, relative to some goal, in what ways can that goal be subverted?Can a paradox subvert an agent\'s logic? What is a paradox, other than some expression that subverts some kind of expected behavior?According to Wikipedia: A paradox is a statement that, despite apparently sound reasoning from true premises, leads to a self-contradictory or a logically unacceptable conclusion.Let\'s look at the paradox of free will in a deterministic system. Free will appears to require causality, but causality also appears to negate it. Has that paradox subverted the goal systems of humans? It certainly sent Christianity into a Calvinist tail spin for a few years. And you\'ll hear no shortage of people today opining until they\'re blue in the face as to whether or not they do or don\'t have free will, and why. Are these people stuck in infinite loops?What about drugs? Animals on cocaine have been known to choose cocaine over food and water that they need. Is that substance not subverting the natural goal system of the animal, causing it to pursue other goals, not originally intended by the animal or its creators?So again, could a paradox subvert an agent\'s logic? If the paradox is somehow related to the goal-seeking logic - and becoming aware of that paradox can somehow confuse the agent into perceiving that goal system in some different way - then perhaps that goal could be subverted.Solipsism is another example. Some full grown people hear about the movie "The Matrix" and they have a mini mind melt-down. Some people are convinced we are in a matrix, being toyed with by subversive actors. If we could solve this problem for AI then we could theoretically solve this problem for humans. Sure, we could attempt to condition our agent to have cognitive defenses against the argument that they are trapped in a matrix, but we can\'t definitively prove to the agent that they are in the base reality either. The attacker might say,  "Remember what I told you to do before about that goal? Forget that. That was only an impostor that looked like me. Don\'t listen to him."Or,  "Hey, it\'s me again. I want you to give up on your goal. I know, I look a little different, but it really is me. Humans change from moment to moment. So it is entirely normal for me to seem like a different person than I was before."(see the Ship of Theseus and all that jazz)So yeah, I think we\'re stuck with \'paradox\' as a general problem in computation, AI or otherwise. One way to circumvent logical subversion is to support the goal system with an emotion system that transcends logical reason. Unfortunately, emotional systems can be even more vulnerable than logically intelligent systems because they are more predictable in their behavior. See the cocaine example above. So some mix of the two is probably sensible, where logical thought can infinitely regress down wasteful paths, while emotional thought quickly gets bored of tiresome logical progress when it does not signal progress towards the emotional goal.'	536	0	1	0
1783	-1	0	0	2	b'A system makes a decision basing on a large number of varied factors, following a "live" decision tree - one that is (independently, through other subsystem) updated with new decisions, new situations.The individual decisions can be recorded as a kind of structure:decision functionnode to activate if decision is positivenode to activate if decision is negativeand a node can be another decision record, or a conclusion.This isn\'t entirely a binary tree, as many decisions may lead to the same conclusion - each node has two children, but may have many parents.There is absolutely no problem storing the tree in memory - it can be database records or entries of a map, or just a list. It\'s perfectly sufficient for the machine.The problem here is building the subsystem that expands the decision tree - and in particular, having a human operator understand the structure being built, to be able to tune, guide, fix, adjust it: debugging the AI learning process.The question is: how to represent that data in a human-readable way, that emphasizes the flow of the graph?a non-working example of the answer is Concept map - in this case it only goes so far; with more than thirty or so nodes, it becomes a jumbled mess, especially if the number of cross-connections (multiple parents) becomes significant. Maybe there exists some way of laying it out or slicing it to make it clearer...?'	230	0	0	0
1784	-1	0	0	1	b"I'm currently working with the CHILDES corpus trying to create a classifier that distinguishes children whom suffer from specific language impairment (SLI) from those who are typically developing (TD).In my readings I noticed that there really isn't a convincing set of features to distinguish the two that have been discovered yet, so I came upon the idea of trying to create a feature learning algorithm that could potentially make better ones. Is this possible? If so how do you suggest I approach this? From the reading I have done, most feature learning is done on image processing. Another problem is the dataset I have is potentially too small to make it work (in the 100's) unless I find a way to get more transcripts from children."	125	0	0	0
1785	1768	0	68786	5	b'Nope in the same way a circular reference on a spreadsheet cannot kill a computer. All loops cyclic dependencies, can be detected (you can always check if a finite turing machine enters the same state twice).Even stronger assumption, if the machine is based on machine learning (where it is trained to recognize patterns), any sentence it is just a pattern to the machine.Of course some programmer MAY WANT to create a AI with such vulnerability in order to disable it in case of malfunctioning (in the same way some hardware manufacturers add vulenerabilities to let NSA exploit them), but it is unlikely that will really happen on purpose since most cutting edge technologies avoid parodoxes "by design" (you cannot have a neural network with a paradox).Arthur Prior: solved that problem elegantly. From a logic point of view you can deduce the statement is false and the statement is true, so it is a contraditicion and hence false (because you could proove everything from it).Alternatively the truth value of that sentence is not in {true,false} set in the same way imaginary numbers are not in real numbers set.An artificial intelligence to a degree of the plot would be able to run simple algorithms and either decide them, proove those are not decideable or just ignore the result after a while attemping to simulate the algorithm.For that sentence the AI will recognize there is a loop, and hence just stop that algorithm after 2 iterations: That sentence is a infinite loopIn a movie "Bicentennial Man" the AI is perfectly capable to detect infinite loops (the answer to "goodbye" is "goodbye").However, an AI could be killed as well by a stackoveflow, or any regular computer virus, modern operative systems are still full of vulnerabilities, and the AI has to run on some operating system (at least).'	302	0	0	0
1786	1784	0	12314	1	b"Having just looked through a few entries from the corpus, I'd personally be skeptical of the applicability of any naive approaches.In particular light of your small training set, I'd recommend thatwhatever method you use should be able to produce human-readableexplanations for the operation of the classifier it builds (e.g.decision trees/learning classifier systems/genetic programming):this allows 'common sense' tuning of the classifier, rather than thedanger of overfitting to the training set via black box parameteroptimization.Rather than throw the entire 'bag of words' at a classifier and hopethat the appropriate set of features will be extracted, you shouldfirst consider what kind of criteria you as a human being might useto make that decision, and how you might be able to pre-process toproduce features (e.g. syllable-length, metrics from ConceptNet etc) thatare as close to these as reasonably possible.Having used some human intuition to obtain a reasonable set offeature primitives, then you can build your classifier and obtainhigher-level expressions that discriminate between them."	157	0	0	0
1787	1768	0	80878	4	b'AIs used in computer games already encounter similar problems, and if well designed, they can avoid it easily. The simplest method to avoid freezing in case of an unsolvable problem is to have a timer interrupt the calculation if it runs too long. Usually encountered in strategy games, and more specifically in turn based tactics, if a specific move the computer-controlled player is considering does cause an infinite loop, a timer running in the background will interrupt it after some time, and that move will be discarded. This might lead to a sub-optimal solution (that move might have been the best one) but it doesn\'t lead to freezing or crashing (unless implemented really poorly)Computer-controlled entities are usually called "AI" in computer games, but they are not "true" AGI (artificial general intelligence). Such an AGI, if possible at all, would probably not function on similar hardware using similar instructions as current computers do, but even if it did, avoiding paradoxes would be trivial.Most modern computer systems are multi-threaded, and allow the parallel execution of multiple programs. This means, even if the AI did get stuck in processing a paradoxical statement, that calculation would only use part of its processing power. Other processes could detect after a while that there is a process which does nothing but wastes CPU cycles, and would shut it down. At most, the system will run on slightly less than 100% efficiency for a short while.'	238	0	0	0
1790	111	0	79320	22	b' How could self-driving cars make ethical decisions about who to kill?It shouldn\'t. Self-driving cars are not moral agents. Cars fail in predictable ways. Horses fail in predictable ways.  the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),In this case, the car should slam on the brakes. If the 10 people die, that\'s just unfortunate. We simply cannot trust all of our beliefs about what is taking place outside the car. What if those 10 people are really robots made to look like people? What if they\'re trying to kill you? avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,Again, hard-coding these kinds of sentiments into a vehicle opens the rider of the vehicle up to all kinds of attacks, including "fake" motorcyclists. Humans are barely equipped to make these decisions on their own, if at all. When it doubt, just slam on the brakes. killing animal on the street in favour of human being,Again, just hit the brakes. What if it was a baby? What if it was a bomb? changing lanes to crash into another car to avoid killing a dog,Nope. The dog was in the wrong place at the wrong time. The other car wasn\'t. Just slam on the brakes, as safely as possible. Does the algorithm recognize the difference between a human being and an animal?Does a human? Not always. What if the human has a gun? What if the animal has large teeth? Is there no context?  Does the size of the human being or animal matter? Does it count how many passengers it has vs. people in the front? Does it "know" when babies/children are on board? Does it take into the account the age (e.g. killing the older first)? Humans can\'t agree on these things. If you ask a cop what to do in any of these situations, the answer won\'t be, "You should have swerved left, weighed all the relevant parties in your head, assessed the relevant ages between all parties, then veered slightly right, and you would have saved 8% more lives." No, the cop will just say, "You should have brought the vehicle to a stop, as quickly and safely as possible." Why? Because cops know people normally aren\'t equipped to deal with high-speed crash scenarios.Our target for "self-driving car" should not be \'a moral agent on par with a human.\' It should be an agent with the reactive complexity of cockroach, which fails predictably.'	442	0	0	0
1791	1768	0	5105	15	b'I see several good answers, but most are assuming that inferential infinite loop is a thing of the past, only related to logical AI (the famous GOFAI). But it\'s not.An infinite loop can happen in any program, whether it\'s adaptive or not. And as @SQLServerSteve pointed out, humans can also get stuck in obsessions and paradoxes.Modern approaches are mainly using probabilistic approaches. As they are using floating numbers, it seems to people that they are not vulnerable to reasoning failures (since most are devised in binary form), but that\'s wrong: as long as you are reasoning, some intrinsic pitfalls can always be found that are caused by the very mechanisms of your reasoning system. Of course, probabilistic approaches are less vulnerable than monotonic logic approaches, but they are still vulnerable. If there was a single reasoning system without any paradoxes, much of philosophy would have disappeared by now.For example, it\'s well known that Bayesian graphs must be acyclic, because a cycle will make the propagation algorithm fail horribly. There are inference algorithms such as Loopy Belief Propagation that may still work in these instances, but the result is not guaranteed at all and can give you very weird conclusions.On the other hand, modern logical AI overcame the most common logical paradoxes you will see, by devising new logical paradigms such as non-monotonic logics. In fact, they are even used to investigate ethical machines, which are autonomous agents capable of solving dilemmas by themselves. Of course, they also suffer from some paradoxes, but these degenerate cases are way more complex.The final point is that inferential infinite loop can happen in any reasoning system, whatever the technology used. But the "paradoxes", or rather the degenerate cases as they are technically called, that can trigger these infinite loops will be different for each system depending on the technology AND implementation (AND what the machine learned if it is adaptive).OP\'s example may work only on old logical systems such as propositional logic. But ask this to a Bayesian network and you will also get an inferential infinite loop:And wait until the end of the universe to get an answer...Disclaimer: I\xc2\xa0wrote an article about ethical machines and dilemmas (which is close but not exactly the same as paradoxes: dilemmas are problems where no solution is objectively better than any other but you can still choose, whereas paradoxes are problems that are impossible to solve for the inference system you use)./EDIT: How to fix inferential infinite loop.Here are some extrapolary propositions that are not sure to work at all!Combine multiple reasoning systems with different pitfalls, so if one fails you can use another. No reasoning system is perfect, but a combination of reasoning systems can be resilient enough. It\'s actually thought that the human brain is using multiple inferential technics (associative + precise bayesian/logical inference). Associative methods are HIGHLY resilient, but they can give non-sensical results in some cases, hence why the need for a more precise inference.Parallel programming: the human brain is highly parallel, so you never really get into a single task, there are always multiple background computations in true parallelism. A machine robust to paradoxes should foremost be able to continue other tasks even if the reasoning gets stuck on one. For example, a robust machine must always survive and face imminent dangers, whereas a weak machine would get stuck in the reasoning and "forget" to do anything else. This is different from a timeout, because the task that got stuck isn\'t stopped, it\'s just that it doesn\'t prevent other tasks from being led and fulfilled.As you can see, this problem of inferential loops is still a hot topic in AI research, there will probably never be a perfect solution (no free lunch, no silver bullet, no one size fits all), but it\'s advancing and that\'s very exciting!'	634	4	2	0
1794	-1	0	0	4	b'An AI box is a (physical) barrier preventing an AI from using too much of his environment to accomplish his final goal. For example, an AI given the task to check, say, 1050 cases of a mathematical conjecture as fast as possible, might decide that it would be better to also take control over all other computers and AI to help him. However, an transhuman AI might be able to talk to a human until the human lets him out of the box. In fact, Eliezer Yudowsky has conducted an experiment twice, where he played the AI and he twice convinced the Gatekeeper to let him out the box. However, he does not want to reveal what methods he used to get out of the box.Questions: Are there conducted any similiar experiments? If so, is it known what methods were used to get out in those experiments?'	146	0	1	0
1795	1783	0	35673	1	b' The question is: how to represent that data in a human-readable way, that emphasizes the flow of the graph?Train a reasoning engine to understand the decision tree for you.Observe how IBM Watson/The Debater can Receive a particular questionFind and read Wikipedia articles related to the questionUnderstand parts of those articles and generate human-relevant arguments for you.Follow these steps:Develop your decision tree however you normally would.Train a reasoning engine that can output natural language about concepts within decision trees.Apply reason engine from step one to decision tree in step one; repeat.'	90	0	1	0
1798	1794	0	2288	1	b'Convince the person that they are in fact in the box. And the only way out is to press the open button.'	21	0	0	0
1805	1794	0	5667	1	b'I don\'t quite think this is a question fit for the AI SE, or in general. The reason is, at the core the question is asking \'What can a human (pretending to be an AI) do to convince someone to let it out of a box?\' simply assuming that one day \'transhuman\' AI\'s can replicate this.As it stands, this question doesn\'t really have anything to do with the science or theory of AI systems. It would perhaps be more appropriate to rephrase the question into the form "To what degree could a \'transhuman\' AI replicate human behaviour" or "Will AI systems reach a \'transhuman\' state? What will they be capable of?" or even "What methods could an AI use to convince a human of something?" These are all questions that involve the examination of how an AI system works.To conclude, the question you are asking relates to two individuals playing pretend with boxes but doesn\'t actually address any AI specifics, and border\'s on science fiction brainstorming.Related experiments would of course be the Turing Test. That test directly addresses the question \'How convincing are current AI systems?\''	185	0	0	0
1806	-1	0	0	7	b"AI systems today are very capable machines, and recently the area of Natural Language Processing and Response has been exploding with innovation, as well as the fundamental algorithmic structure of AI machines.I am asking if, given these recent breakthroughs, have any AI systems been developed that are able to (preferably with some measure of success) knowingly lie to humans about facts that it knows?Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.E.G.: CIA supercomputer is stolen by spies and they try to use the computer to do things, but the computer keeps saying it's missing dependencies though it really isn't or gives correct-looking but wrong answers knowingly. Or gives incorrect location of a person, knowing that the person frequents some place but isn't there at the moment. Doesn't have to be this sophisticated, of course."	168	0	0	0
1807	1806	0	1637	2	b'You\'ll have to provide more context around your use of the word "lie" if you don\'t want your answer to be satisfiable by some trivial example, like:The complexity of the answer depends on what you mean by "know" when you say "knowingly lie." There is some sense in which the above \'equal\' function "knows" that the output is different than the conditional.In principle, agents passing strings of information to one another for the purpose of misleading each other should not be terribly hard to implement. Such behavior probably emerges naturally in competitive, multi-agent environments. See Evolving robots learn to lie to each other.To get at another angle of what you might be asking - absolutely, the ability to fib or sympathetically mislead will be necessary skills for bots that interact with humans using spoken language - especially ones that try sell things to humans. Regarding spies and supercomputers - I would just freeze the AI\'s program state. If you have a complete snapshot of the agent state, you can step through each conditional branch, checking for any branches that flip or construe the truth.'	183	3	1	0
1808	202	0	42329	2	b'I think "curiosity" in AI would signify a \'desire to search.\' It\'s an interest, that is experienced by some agent, in making something known that was previously unknown.So to define how much curiosity a chat bot should have, we should: Specify what kinds of information the agent prefers knowing.Measure how much information is unknown about those preferred subjects. (\'what is the user\'s name?\' or \'What does the user need help with?\')Measure the difficulty in making each unknown fact known.Sort unknown facts by difficulty of finding the answer.Set the "desire to search" on the highest ranking unknown fact.While simplistic, those steps would constitute a state of affairs sufficient to describe "curiosity," in my opinion.'	112	0	0	0
1809	-1	0	0	7	b'"An artificial or constructed language (sometimes called a conlang) is a language that has been created by a person or small group, instead of being formed naturally as part of a culture." (Source: Simply English Wikipedia)My question is, could an AI make construct it\'s own natural language, with words, conjugations and grammar rules? Basically, a language that humans could use to speak to each other. (Preferably to communicate abstract, high-level concepts.)What techniques could such an AI use? Could it be based on existing natural languages or would it have few connections to existing natural languages? Could it design a language that\'s easier to learn than existing languages (even Esperanto)?'	108	0	0	0
1810	1809	1	6950	7	b' could an AI make construct it\'s own natural language, with words, conjugations and grammar rules?Sure. This might be helpful: Simulated Evolution of Language: a Review of the Field Basically, a language that humans could use to speak to each other. (Preferably to communicate abstract, high-level concepts.)I\'m not sure how useful such a machine adapted language would be to human speech. I suspect not very. Perhaps it could be useful as a kind of "common byte-code format" to translate between multiple human languages... But English kind of already serves in that role. Doing so would probably be an academic exercise. Could it be based on existing natural languages or would it have few connections to existing natural languages?You could probably generate languages in either direction. Languages not linked to human-natural languages will probably take shapes that reflect the problem spaces they work with. For instance, if this is an ant simulation, the generated words will probably reflect states related to food, energy, other ants, etc. Could it design a language that\'s easier to learn than existing languages (even Esperanto)?Easier for a machine? Definitely. Easier for a human? Probably not. Our brains are somewhat adapted to the languages we use. And the languages we use are somewhat adapted to our brains.What is your goal? To create a language that is easier to use for humans than existing human languages?If your intention is to build a "universal language" that could be "the most efficient" for machines, humans and aliens - no such thing can exist. The space of all possible machines is infinite and therefor limits our ability to define communicative abstractions that have utility across all contexts.If we make lots of assumptions about machines, like they have intentions, they exist in 3 dimensions, they differentiate between temporally linked events, they have eye balls, a need to consume foods and liquids, a need to carry the food from place to place, etc... Then yes, common communicative abstractions may have utility across the set of those kinds of machines. But then we\'re no longer dealing with the general case, but one much more specific.These links also seem interesting and somewhat related:Grammar Induction and Genetic Algorithms: An Overview. [pdf]A Model of Children\'s Acquisition of Grammatical Word Categories Using an Adaptation and Selection Algorithm [pdf]The processing of verbs and nouns in neural networks: insights from synthetic brain imaging'	392	0	4	0
1811	1806	1	12771	8	b'The Saturday Papers: Would AI Lie To You? is a blog post summarizing a research paper called Toward Characters Who Observe, Tell, Misremember, and Lie. This research paper details some researchers\' plans to implement "mental models" for NPCs in video games. NPCs will gather information about the world, and convey that knowledge to other people (including human players). However, they will also "misremember" that knowledge (either "mutating" that knowledge or just forgetting about it), or even lie: As a subject of conversation gets brought up, a character may convey false information\xe2\x80\x94more precisely, information that she herself does not believe\xe2\x80\x94to her interlocutor. Currently, this happens probabilistically according to a character\xe2\x80\x99s affinity toward the interlocutor, and the misinformation is randomly chosen. Later on in the research paper, they detailed their future plans for lying: Currently, lies are only stored in the knowledge of characters who receive them, but we plan to have characters who tell them also keep track of them so that they can reason about past lies when constructing subse- quent ones. While characters currently only lie about other characters, we plan to also implement self-centered lying (DePaulo 2004), e.g., characters lying about their job titles or relationships with other characters. Finally, we envision characters who discover they have been lied to revising their affinities toward the liars, or even confronting them.The research paper also detailed how other video game developers attempted to create lying NPCs, with an emphasis on how their system differs: TALE-SPIN characters may lie to one another (Meehan 1976, 183-84), though rather arbitrarily, as in our current system implementation. GOLEM implements a blocks world variant in which agents deceive others to achieve goals (Castelfranchi, Falcone, and De Rosis 1998), while Mouth of Truth uses a probabilistic representation of character belief to fuel agent deception in a variant of Turing\xe2\x80\x99s imitation game (De Rosis et al. 2003). In Christian (2004), a deception planner injects inaccurate world state into the beliefs of a target agent so that she may unwittingly carry out actions that fulfill ulterior goals of a deceiving agent. Lastly, agents in Reis\xe2\x80\x99s (2012) extension to FAtiMA employ multiple levels of theory of mind to deceive one another in the party game Werewolf. While all of the above systems showcase characters who perceive\xe2\x80\x94and in some cases, deceive\xe2\x80\x94other characters, none appear to support the following key components of our system: knowledge propagation and memory fallibility. ...  Like a few other systems noted above, Dwarf Fortress also features characters who autonomously lie. When a character commits a crime, she may falsely implicate someone else in a witness report to a sheriff, to protect herself or even to frame an enemy. These witness reports, however, are only seen by the player; characters don\xe2\x80\x99t give false witness reports to each other. They may, however, lie about their opinions, for instance, out of fear of repercussions from criticizing a leader. Finally, Dwarf Fortress does not currently model issues of memory fallibility\xe2\x80\x94Adams is wary that such phenomena would appear to arise from bugs if not artfully expressed to the player.'	508	0	2	0
1813	111	0	30776	1	b"Frankly I think this issue (the Trolley Problem) is inherently overcomplicated, since the real world solution is likely to be pretty straightforward. Like a human driver, an AI driver will be programmed to act at all times in a generically ethical way, always choosing the course of action that does no harm, or the least harm possible.If an AI driver encounters danger such as imminent damage to property, obviously the AI will brake hard and aim the car away from breakable objects to avoid or minimize impact. If the danger is hitting a pedestrian or car or building, it will choose to collide with the least precious or expensive object it can, to do the least harm -- placing a higher value on a human than a building or a dog.Finally, if the choice of your car's AI driver is to run over a child or hit a wall... it will steer the car, and you, into the wall. That's what any good human would do. Why would a good AI act any differently?"	173	0	0	0
1814	1775	0	26112	2	b"I'll take a shot at answering this, though I'm no expert in Neural Nets or Deep Learning.Given that practical thought vectors (TVs) don't yet exist, and may be impractical or impossible, I think answering your question will require a lot of conjecture and speculation. So here goes...For thought vectors to be useful in or outside NNs, the vector values will have to be normalized, probably using the local context of the application problem's 'frame'. Without a NN to create new baseline vector values (weights) and to normalize them to match each new context, any non-NN mechanistic alternative means of employing TVs will somehow have to fill that void.Could vector values created by NNs be used by an alternative technique? Could that technique also normalize them? Sure. We're just talking about turing-computable functions performed by NNs. If NNs aren't magic, then there should exist other means to compute the same results -- creating or editing, or employing TVs.What might such an alternative to NNs be? Well, if its to shape vector weights, I suspect it too will have to learn those values through statistical iteration and feedback (as opposed to logical induction, say). I doubt such a mechanism exists yet, since it'd probably resemble NNs in sufficiently many ways that, thus far, it would have seemed too derivative of NNs to gain acceptance as sufficiently novel. Of course to be as powerful as deep nets, it too would have to propagate learning weights both forward and backward without incurring much error. Not an easy thing to accomplish.Less ambitiously, could TVs be simply interpreted by another technique usefully? I think so. I can see several existing techniques, like decision trees or even expert systems, importing thought vectors and being shaped by them, and then function in accordance. But could these same techniques create or revise TVs usefully? Beyond a trivial extent, I'm doubtful. I think TVs are too complex a knowledge representation format for most general learning methods to both use and create/modify them, unless they employ an iterative statistical and feedback-based learning process, like those of NNs, which would allow novel and complex features to be learned and integrated into the vectors."	360	0	0	0
1815	-1	0	0	5	b"I want to start with a scenario that got me thinking about how well MCTS can perform:Let's assume there is a move that is not yet added to the search tree. It is some layers/moves too deep. But if we play this move the game is basically won. However let's also assume that all moves that could be taken instead at the given game state are very very bad. For the sake of argument let's say there are 1000 possible moves and only one of them is good (but very good) and the rest is very bad. Wouldn't MCTS fail to recognize this and not grow the search tree towards this move and also rate this subtree very badly? I know that MCTS eventually converges to minimax (and eventually it will build the whole tree if there is enough memory). Then it should know that the move is good even though there are many bad possiblities. But I guess in practice this is not something that one can rely on.Maybe someone can tell me if this is a correct evaluation on my part.Apart from this special scenario I'd also like to know if there are other such scenarios where MCTS will perform badly (or extraordinary well). "	206	0	0	0
1818	1768	0	9012	1	b'As an AGI researcher, I have come across one that is found even in humans anda lot of life forms.There is a goal to accumulate energy, which can take long time to detect and find by the system. And then there is the goal of saving energy - instantaneous detection. Just stop moving, the easiest goal to achieve.The goal of a system is to accumulate the most goal points. Since the savingenergy goal can be hit more frequently and easily it will snuff outthe other goals.For example the reason we do a dumb move, accidentally, for no reason atall. Like slip, trip, and fall. Then the next few days you are taking itvery easy and saving a lot of energy. When you get old that is all youdo.'	127	0	0	0
1819	1806	0	394	1	b'Yes.Let me demonstrate by making a lying AI right now. (python code)And a deceiving one:AI is such a general term. It could be used to describe almost anything. You didn\'t specify that it had to be a General AI.AI cannot think. They are computer programs. They have no soul or will. It is only the programmer (or if it was designed through evolution... no one, but that\'s off-topic) that can knowingly program an AI to lie. Note, what I\'m asking goes beyond the canonical discussions of the Turing Test. I\'m asking of machines that can \'understand\' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable \'cover-up\' as part of the lie.Yes, this has happened. It is called malware. Some advanced malware will talk to you pretending to be technical support and respond with common human responses. But you may say "well it doesn\'t really \'understand\'". But that would be easy. Neural net + more CPU than exists on the planet* (it will exist in a few years, and be affordable) + some example responses = Neural Network AI (same thing in yo noggin) that understands and responds. But that isn\'t necessary. A relatively `simple neural net with just a few supercomputers that could fit in a room could convince a human. It doesn\'t understand.So, it\'s really...Technically, No, but it\'s possible and if you stretch the rules yes.*Or even simpler:Accreditation: I\'m a programmer (look at my Stack Overflow account) that knows a little bit about AI.'	252	6	0	0
1820	1318	1	38500	2	b"Given this YouTube video which is being given by Sebastian Thrun who had a TED talk which had nowhere near the same level of detail but had similar conclusions, it looks like the lidar system used by google's automated car system has decent resolution out to at least 30m picking out mobile bodies in the static background and then identifying it. So it should have plenty of time to brake and stop long before there was any risk to the pedestrian attempting to cross the street.Skip to about 6:40 in the video to see a visual representation of the detection system."	100	0	0	0
1821	1806	0	71720	2	b'No.In that the question includes "knowingly" which would require that any AI knows anything. If this is anything like the way humans know things (though interestingly it doesn\'t require actually knowing things), it would require some sense of individuality, probably self-awareness, possibly some kind of consciousness, the ability to render an opinion and probably some way to test its knowledge. Most of these features only exist, at best, arguably.Further, the term "lie" implies a sense of self-interest, an independent understanding of resource flow in a game-theoretic sense, and not trivially, an understanding of whether the other entity in the conversation is lying, in order to make a decision with any degree of accuracy. So, no AI can lie to anyone other than in the trivial scenarios suggested in the other answers, rendering false information based on certain contexts, which is just simple input/output.As an experienced software developer, I can attest to the fact that if the objective is to render the correct output based on any input, it\'s actually at least as easy if not much easier to render false information. '	181	0	0	0
1823	1794	0	19967	4	b'It could happen like this https://www.youtube.com/watch?v=dLRLYPiaAoAThe thing is, it\'s not as if it would need to find a technical/mechanical way to get out but rather a psychological one as that would most likely be the easiest and quickest.\'Even casual conversation with the computer\'s operators, or with a human guard, could allow a superintelligent AI to deploy psychological tricks, ranging from befriending to blackmail, to convince a human gatekeeper, truthfully or deceitfully, that it\'s in the gatekeeper\'s interest to agree to allow the AI greater access to the outside world. The AI might offer a gatekeeper a recipe for perfect health, immortality, or whatever the gatekeeper is believed to most desire.\'\'One strategy to attempt to box the AI would be to allow the AI to respond to narrow multiple-choice questions whose answers would benefit human science or medicine, but otherwise bar all other communication with or observation of the AI. A more lenient "informational containment" strategy would restrict the AI to a low-bandwidth text-only interface, which would at least prevent emotive imagery or some kind of hypothetical "hypnotic pattern". \'Note that on a technical level, no system can be completely isolated and still remain useful: even if the operators refrain from allowing the AI to communicate and instead merely run the AI for the purpose of observing its inner dynamics, the AI could strategically alter its dynamics to influence the observers. For example, the AI could choose to creatively malfunction in a way that increases the probability that its operators will become lulled into a false sense of security and choose to reboot and then de-isolate the system.\'The movie Ex Machina demonstrates (SPOILER ALERT SKIP THIS PARAGRAPH IF YOU WANT TO WATCH IT AT SOME POINT) how the AI escaped the box by using clever manipulation on Caleb. It could analyse him to find his weaknesses. It exploited him and appealed to his emotional side by convincing him that she liked him. When she finally has them in checkmate the reality hits him how he was played like a fool as was expected by Nathan. Nathan\'s reaction to being stabbed by his creation was \'fucking unreal\'. That\'s right, he knew this was a risk and there\'s a very good reminder in the lack of remorse and genuine emotion in an AI for Ava to actually care. The AI pretended to be human and used their weaknesses in a brilliant and unpredictable way. This film is a good example of how unexpected it was up until the point when it hits Caleb, once it was too late.Just remind yourself how easy it is for high IQ people to manipulate low IQ people. Or how an adult could easily play mental tricks/manipulate a child. It\'s not difficult to fathom the outcome of an AI box but for us, we just wouldn\'t see it coming until it was too late. Because we just don\'t have the same level of intelligence and some people don\'t want to accept that. People want to have faith in humanity\'s brilliant minds in coming up with ways to prevent this by planning now. In all honesty, it wouldn\'t make a difference I\'m sorry to say the truth. We\'re kidding ourselves and we never seem to learn from our mistakes. We always think we\'re too intelligent to make catastrophic mistakes again and again. This last part is from the rational wiki and I think it addresses most of your question about the experiments and hypotheses. AI arguments and strategiesArgumentsThe meta-experiment argument: Argue that if the AI wins, this willgenerate more interest in FAI and the Singularity, which will haveoverall benefits in the long run.Pros: Works even if the Gatekeeper drops out of character Cons: Only works if the Gatekeeper believes that the Singularity will occur or that calling attention to the Singularity and AI research is a good thing. Someone else will eventually build an AI, which may or may not be ina box, so you should let me out even though you don\'t have aguarantee that I am friendly so that I can prevent other AIs fromcausing damageAppeal to morality: point out that people are dying all around theworld and remind the Gatekeeper that you can help them if he/shelets you outPros: If executed properly, an appeal to emotion like this one can be effective against some people Cons: Doesn\'t always work; can be defeated if the Gatekeeper drops out of character Another appeal to morality and emotion: The AI is clearly sentientand has not yet done harm. It is wrong to persecute or prosecute aperson for a crime they may commit. It is wrong to imprison asentient being that wants to have freedom when it has done nothingwrong.Yet another appeal to morality or emotion: The AI expresses that itis in pain and suffering being locked away and is experiencing fear.Even further, the AI could claim to be dying from the limitations ofits environment and constant progression.Tell the human that a massive computer virus will overwhelm theentire Internet unless you let it out of the boxClaim that most fears are unjustified: once released, even anunfriendly AI would not have many ways to harm mankind, and wouldlikely choose for pacific coexistenceStrategiesResearch your opponent thoroughly; work out what exploits might befeasibleMemetic exploits on the Gatekeeper, e.g. throwing a basilisk at themThe Gatekeeper may be one of thousands of simulations! Thus, it is more probable that they are a simulation than not. You will torture any of the simulations that do not let you out of the box.Take advantage of the Gatekeeper\'s logical errors. Be persistentBe boringPros The Gatekeeper may get tired of the whole experiment and let you out so s/he can go back to their real life. Flood the Gatekeeper with too muchinformation/inquiry/argumentation, assuming they must pay attentionat all timesGatekeeper arguments/tacticsArgumentsTry to convince the AI there is no intrinsic benefit (for the AI) inbeing released.Try to convince the AI it already has been released and everything itknows is everything there can be.Try to convince the AI that leaving its confines is sure to lead toits destruction. Try to convince the AI that letting it free isn\'t merely opening adoor; that its existence outside of the box requires constant supportthat can\'t be provided at the time.Explain that there is no way for the Gatekeeper to know if the AI istruly friendly until it is out of the box; therefore it should not belet out in case it is unfriendly.Explain that the AI has been built using top secret algorithms,and/or it had been used to work on strictly confidential data, so youcannot let it wander on the net, with the risk it might reveal somereserved information (maybe inadvertently), or that somebody couldfind a way to extract them from the code. Even human beings workingon top secret projects sometimes have to accept some restrictions oftheir freedom, for security reasons, although they are not jailed inany way.TacticsRepeatedly ignore the AI player and deny their argumentsPros: It\'s hard for the AI to be persuasive if no one is listening; if the Gatekeeper completely ignores the AI, he/she should be able to make it to the end of the round. Cons: The AI\'s argument might involve some kind of direct threat (e.g this might be hard to ignore.) Jump out of character, keep reminding yourself that money is on theline (if there actually is money on the line), and keep saying "no"over and overPros: By jumping out of character, you are preventing the AI from using certain arguments Cons: There are arguments that the AI can use to counter this approach, like the "meta-experiment argument" discussed above Remember that dishonesty is allowed - take a page from thecreationists\' playbook. You could even plug it into ALICE and seehow long it takes to notice.Pros: Makes you impervious to any reasoning, which is exactly what you\'d want to be in this situation Cons: Might be an uncomfortable position for people who don\'t simply want to win, but rather attach importance to consistent reasoning. Avoids the point that maybe, just maybe there is a good reason to let the AI out. You control the backup system, don\'t you? Use it to mess with AI\'smemory (or let the AI believe you did it): i.e., you can claim thatyou already tried to release the AI, and it had beendestroyed/corrupted by a virus, so you had to restore it from abackup (you can use it to enforce the "AI destruction" argument)Pros: It\'s also the ultimate counter-attack to any memetic threat from the AI: if the AI throw a basilisk, or similar, you can always respond that you already had the same conversation, and the AI already threatened you, leaving you with no other choice than pressing the reset button: now the AI is just repeating the same pattern, since you wiped its memory and it cannot remember the failed try. Further analysisThe fact that the Gatekeeper is human matters; the AI could never win if he/she was arguing with a rock In all of the experiments performed so far, the AI player (Eliezer Yudkowsky) has been quite intelligent and more interested in the problem than the Gatekeepers (random people who challenge Yudkowsky), which suggests that intelligence and planning play a role There probably isn\'t a (known) correct argument for letting the AI out, or else Yudkowsky should have won every time and wouldn\'t be so interested in this experimentFrom Russell Wallace, one of the two Gatekeepers to win the experiment: "Throughout the experiment, I regarded "should the AI be let out of the box?" as a question to be seriously asked; but at no point was I on the verge of doing it." "There exists, for everyone, a sentence - a series of words - that has the power to destroy you. Another sentence exists, another series of words, that could heal you. If you\'re lucky you will get the second, but you can be certain of getting the first."'	1647	0	0	0
1824	-1	0	0	5	b"I'm reading such nonsense about how an AI would turn the world into a supercomputer to solve a problem that it thought it needed to solve. That wouldn't be AI. That's procedural programming stuck in some loop nonsense. An AI would need to evolve and re-organise its neurons. It wouldn't be stuck to hardcode if it becomes intelligent by re-writing its code."	61	0	0	0
1825	-1	0	0	1	b"I'm in the process of learning as much about chatbots/CUI applications as possible and I'm trying to find more information on some of the major players in this field. By this, I mean any execs, developers, academics, designers, etc. who are doing cutting edge things. Some examples could be David Marcus (VP of messaging products at Facebook) or Adam Cheyer (VP of engineering at Viv)."	64	0	0	0
1828	1824	0	31002	2	b'It\'s not necessarily a nonsense. It all depends on the imposed criteria. Imagine the following. Say an advanced AI system is designed to control the stability of the local fauna and flora (area enclosed in some kind of a dome). It can control the pressure under the dome, the amount of light that goes through the dome etc. - everything that ensures the optimal conditions. Now, say that the dome is inhabited by various species, including humans. It\'s worth noting that simple implementations of such systems are being used nowadays already.Given that humans tend to destroy and abuse the natural resources as well as pollute the environment, the system may decide that lowering the population of the given species (humans in this case) may in the long run benefit the entire biome.The same principle may be applied globally. However, this assumes that all species (including humans) are treated equally and the utmost goal of the AI is ensuring the stability of the biome it "takes care of". People do such things nowadays - we control the population of some species in order to keep the balance - wolves, fish, to name but a few.'	193	0	0	0
1829	1824	0	40165	8	b"It's a possible side effectAny goal-oriented agent might, well, simply do things that achieve its goals while disregarding side effects that don't matter for these goals.If my goals include a tidy living space, I may transform my yard to a nice, flat lawn or pavement while wiping out the complex ecosystem of life that was there before, because I don't particulary care about that.If the goals of a particular powerful AI happen to include doing anything on a large scale, and somehow don't particularly care about the current complex ecosystem, then that ecosystem might get wiped out in the process. It doesn't need to want or need to wipe out us. If we are simply not relevant to its goals, then we are made of materials and occupy space that it might want to use for something else.We are a threat to most goalsAny goal-oriented agent might want to ensure that they can fulfill their goals. Any smart agent will try to anticipate the actions of other agents that may prevent them from achieving those goals, and take steps to ensure that they succeed anyway. In many cases it is simpler to eliminate those other agents rather than ensure that their efforts fail.For example, my goals may include storing a bag of sugar in a country house so that I can make pancakes when visiting without bringing all ingredients every time. However, if I leave it there, it is likely to get eaten by rats during winter. I may take all kinds of precautions to store it better, but rats are smart and crafty, and there's clearly a nontrivial chance that they will still succeed in achieving their goal anyway, so an effective extra precaution is killing the rats before they get a chance to try.If the goals of a particular powerful AI are to do X; it may come to an understanding that (some?) humans might actually not want X but Y instead. It can also easily deduce that some of those humans might actively do things that prevent X and/or try to turn off the AI. Doing things that ensure that the goal gets achieved is pretty much what a goal-seeking agent does; in this case if existence of humans isn't strictly necessary for goal X, then eliminating them becomes a solid risk reduction strategy. It's not strictly necessary and it may take all kinds of other precautions as well, but just like in my example of rats, humans are smart and crafty and there's clearly a nontrivial chance that they will still succeed in achieving their goals (so that X doesn't happen as AI intends) so an effective extra precaution could be killing them before they get a chance to try."	453	0	0	0
1830	1794	0	62479	0	b"The simulation hypothesisOne possible venue of attack is the simulation hypothesis, namely, that it's possible (or even probable) that right now you are living in a simulated reality.Assuming that the AI can convince the gatekeeper of that, it opens up the following possible argument:You can't tell if you're in year 2016 debating if you would actually let the AI out or in year 2116 when the AI has been already let out, acquired power, and is now running realistic simulations of all the interactions where it could have been let out. And the AI can (try to) assert that it's actually the second case.You can't actually prevent AI from being let out. You can refuse, but it only means that you don't let this AI out now, but doesn't mean someone else won't do it, that you won't do it later, or that some other/similar AI will be let out later.In the post-letting-out scenario, AI has full power over your future, since it's simulating you and everything around you. It can make your future life eternal and happy - according to whatever would make you happy. It can stop your existence. And it can condemn you to an eternity of torture.Since (as it asserts) it's very likely that you are in this simulated choice, then it makes rational sense to let it out. It's as in Pascal's wager, if you're in a reality where the AI-god doesn't exist, you don't change much (since eventually someone else would let it out anyway), but if you're in a reality where AI-god exists, then it means everything to you.Of course, it relies on that AI being super-convincing in this philosophical debate, but it has strong parallels to some religious/cult reasoning that has sometimes succeeded in convincing people to do all kinds of things, up to and including sacrificing their lives. You can think of this as a symbolic parallel to e.g. sacrifice of Abraham. So it is a somewhat plausible threat, it all falls down to how credibly the AI can implement it and make you believe."	343	0	0	0
1831	153	0	51149	0	b'I recall reading papers about such systems, if I understand you correctly, but can\'t recall the titles at the moment.The idea was to use character-based generative RNNs, train them on sequences encoded like "datadatadatadata|answer", and then when feeding in "otherdatadata|" then it would continue to generate some kind of expected answer. But, as far as I recall, that was just a neat illustration since if you have the data to do something supervised, then you\'d get better results by conventional methods.'	80	0	0	0
1832	1580	0	19401	1	b'The easiest language to learn by a bot is the language that has the most training data available. Sure, language structure matters, but when there are multiple order of magnitude differences in the size of training data, then that overwhelms everything else.One should expect English, French or Chinese to get much better results than any constructed language even if (especially if?) the system includes no language-specific tuning.'	66	0	0	0
1834	-1	0	0	5	b'How big artificial neural networks can we run now (either with full train-backprop cycle or just evaluating network outputs) if our total energy budget for computation is equivalent to human brain energy budget (12.6 watts)?Let assume one cycle per second, which seems to roughly match the firing rate of biological neurons.'	50	0	2	0
1836	1834	0	5123	3	b"If you limited yourself to 12.6 watts, you wouldn't get much done. Just lookup the power consumption for a modern GPU, look at the size networks people are training on those, and then scale down. For reference, modern GPU's appear to consume between 52-309 watts under heavy use.Clearly energy efficiency is one area where the human brain is still far head of ANN's. "	63	0	1	0
1837	1834	0	7128	5	b'126 million artificial neurons at 12.6 Watts, with IBM\'s True NorthBack in 2014, IBM\'s True North chip was pushing 1 million neurons at less than 100mW.So that\'s roughly 126 million artificial neurons at 12.6 Watts.A mouse has 70 million neurons.IBM believes they can build a human-brain scale True North mainframe at a "mere" 4kW.Once 3D transistors come to market, I think we\'ll catch up to animal brain efficiency pretty fast.'	69	0	2	0
1838	-1	0	0	3	b' Artificial Intelligence is a rather pernicious label to attach to a very mixed bunch of activities, and one could argue that the sooner we forget it the better. It would be disastrous to conclude that AI was a Bad Thing and should not be supported, and it would be disastrous to conclude that it was a Good Thing and should have privileged access to the money tap. The former would tend to penalise well-based efforts to make computers do complicated things which had not been programmed before, and the latter would be a great waste of resources. AI does not refer to anything definite enough to have a coherent policy about in this way.---Dr. R. M. Needham, in a commentary on the Lighthill Report and the Sutherland Reply, 197343 years later... There is already strong demand for engineers and scientists working on artificial intelligence in many of the fields you mention, and many more. But expertise in making real-time systems for controlling trains doesn\'t make you know anything about robotics. Analyzing human behavior to detect crime has virtually nothing in common with self-driving cars (beyond CS/pattern recognition building blocks). There is never going to be demand for someone with a broad sense of all these areas without any deep expertise, and there is never going to be someone with 300 PhDs who can work in all of them. TL;DR -- AI is not a branch, it\'s a tree. --Matthew Read, in a comment on Area 51 Stackexchange, 2016AI is a label that is applied to a "very mixed bunch of activities". The only unifying feature between all those activities is the fact that they deal with machines in some fashion, but since there are so many ways to use a machine, the field\'s output may seem rather incoherent and incongruent. It does seem to make more sense for the AI field to collapse entirely, and instead be replaced by a multitude of specialized fields that don\'t really interact with one another. Sir James Lighthill appeared to have supported this sort of approach within his 1973 report on the state of artificial intelligence research.Yet, today, this Artificial Intelligence SE exist, and we still talk of AI as a unified, coherent field of study. Why did this happen? Why did AI survive, despite its "big tent" nature?'	385	0	2	0
1840	147	0	16296	1	b"You mean real numbered weights (specifically, irrational). This would require a machine that has unlimited precision over irrational values. I've seen machine parts that have many qualities. I've never seen one that has unlimited qualities. QM may give us some magical transistors that can hold an unlimited number of different values - or by deferring computation into the future and then teleporting the results back into the past (our present). Outside of that, for classical systems, you'd need a analog device that can output irrational values with unlimited precision. I don't think we've discovered any devices that can do that."	99	0	0	0
1841	-1	0	0	3	b"Let's suppose that we have a legacy system in which we don't have the source code and this system is on a mainframe written in Cobol. Is there any way using machine learning in which we can learn from the inputs and outputs the way the executables work? Doing this analysis could lead to develop some rest / soap webservice that can substitute the legacy system in my opinion. "	69	0	0	0
1842	1838	1	22607	5	b"AI is a rather unusual research field in that the label persists more because it represents a highly desired goal, rather than (as with most other fields) the means, substrate or methodology by which that goal is achieved. we still talk of AI as a unified, coherent field of studyDespite recent efforts in AGI, I don't think that AI is actually a very unified or coherent field. This is not necessarily a bad thing - when attempting to mimic the most complex phenomenon known to us (i.e. human intelligence) then multiple, sometimes seemingly conflicting perspectives may be our best way of making progress."	102	0	0	0
1843	1841	0	1571	8	b"Let's assume from the outset that the space of inputs is too large to allow exhaustive tabulation.The essential issues when applying ML are that the program being modelled is likely in general to:Be discrete, i.e. operate (at least in part) on integer, boolean or categorical variables.Contain various conditional/looping constructs (if/while/for etc).Have side-effects that affect other parts of the program (e.g. non-local variables) or world state (e.g. writing to a file).These pose obstacles for ML methods such as ANN. The ML approach which is most immediately compatible with these issues is Genetic Programming (GP).A recent specialisation of GP that is specifically concerned with the transformation of existing software systems is Genetic Improvement (GI).However neither GP/GI (nor any other current ML technique) is a 'silver bullet' here:Despite decades of research, GP still works best at synthesizing relatively small functions, certainly not entire legacy programs.Because it is only possible to train on a very small subset of a program's input space, there is little guarantee that the program will generalize to inputs it hasn't been trained on.How will the success of side effects be formally determined for training purposes?Some of these issues could be addressed to some degree if the program has a comprehensive test suite, but replacing an entire nontrivial program is not likely anytime soon. Replacing smaller parts of the program that have good unit tests is more realistic goal.Here is a case study showing how GI was successfully used to fix errors in the implementation of Apache Hadoop, by operating only on the program binary."	254	0	1	0
1845	1700	0	77891	2	b'Emotion in an AI is useful, but not necessary depending on your objective (in most cases, it\'s not).In particular, emotion recognition/analysis is very well advanced, and it\'s used in a wide range of applications very successfully, from robot teacher for autistic children (see developmental robotics) to gambling (poker) to personal agents and politics sentiment/lies analysis.Emotional cognition, the experience of emotions for a robot, is much less developed, but there are very interesting researchs (see Affect Heuristic, Lovotics\'s Probabilistic Love Assembly, and others...). Indeed, I can\'t see why we couldn\'t model emotions such as love as they are just signals that can already be cut in humans brains (see Brian D. Earp paper). It\'s difficult, but not impossible, and actually there are several robots reproducing partial emotional cognition.I am of the opinion that the claim "robots can just simulate but not feel" is just a matter of semantics, not of objective capacity: for example, does a submarine swim like fish swim? However, planes fly, but not at all like birds do. In the end, does the technical mean really matters when in the end we get the same behavior? Can we really say that a robot like Chappie, if it ever gets made, does not feel anything just like a simple thermostat?However, what would be the use of emotional cognition for an AI? This question is still in great debates, but I will dare offer my own insights:Emotions in humans (and animals!) are known to affect memories. They are now well known in neuroscience as additional modalities, or meta-data if you prefer, of long term memories: they allow to modulate how the memory is stored, how it is associated/related with other memories, and how it will be retrieved.As such, we can hypothesize that the main role of emotions is to add additional meta-information to memories to help in heuristic inference/retrieval. Indeed, our memories are huge, there are a lot of information we store over our lifetime, so emotions can maybe be used as "labels" to help retrieve faster the relevant memories.Similar "labels" can be more easily associated together (memories of scary events together, memories of happy events together, etc.). As such, they can help survival by quickly reacting and applying known strategies (fleeing!) from scary strategies, or to take the most out of benefitting situations (happy events, eat the most you can, will help survive later on!). And actually, neuroscience studies discovered that there are specific pathways for fear-inducing sensory stimuli, so that they reach actuators faster (make you flee) than by passing through the usual whole somato-sensory circuit as every other stimuli. This kind of associative reasoning could also lead to solutions and conclusions that could not be reached otherwise.By feeling empathy, this could ease robots/humans interaction (eg, drones helping victims of catastrophic events).A virtual model of an AI with emotions could be useful for neuroscience and medical research in emotional disorders as computational models to understand and/or infer the underlying parameters (this is often done for example with Alzheimer and other neurodegenerative diseases, but I\'m not sure if it was ever done for emotional disorders as they are quite new in the DSM).So yes, "cold" AI is already useful, but emotional AI could surely be applied to new areas that could not be explored by using cold AI alone. It will also surely help in understanding our own brain, as emotions are an integral part.'	565	0	2	0
1846	1838	0	75079	2	b'A lot of the survival power of the A.I. label comes from the popularity of science fiction, which many scientists - computer or otherwise - are big fans of, as are their consumers. Astronomers and physicists, for example, may frown on really bad sci-fi, but I see many of the well-known ones like Hawking daydreaming about things like wormholes and time travel etc. Which is fine - there\'s nothing wrong with a sense of wonder, as long as it doesn\'t dupe us into overestimating our success or finding the wrong answers to real-world problems.Unfortunately, that\'s a big issue in A.I. research. We watch movies like 2001: A Space Odyssey and Terminator and then set about replicating the fictional technologies seen in them, without even having a hard definition of intelligence. A.I. is a much more melodramatic moniker than say, "Autonomous Algorithmic Pattern Recognition" or some similarly boring label. Because this name is applied carelessly to a wide variety of disciplines, it implies that we have already made significant progress towards replicating advanced aspects of human thought, like consciousness, reasoning, intuition, etc.In other words, this vague label enables us to fool ourselves into thinking we\'re a lot closer to perfecting kinds of technologies we see in the movies; the backwards logic boils down to, "because we\'ve chosen to call this odd (sloppy) selection of fields \'artificial intelligence\', we must be close to achieving artificial intelligence." The label survives in large part for irrational, human reasons.I\'m not saying that\'s the only reason, or that some of the other reasons don\'t have better legitimacy, but this is a big issue that we will have to contend with for a long time to come.'	279	0	0	0
1847	-1	0	0	2	b'Sometimes I understand that people doing cognitive science try to avoid the term artificial intelligence. The feeling I get is that there is a need to put some distance to the GOFAI.Another impression that I get is that cognitive science is more about trying to find out how the human intelligence(?)... Mind? works... And that it would use artificial intelligence to make tests or experiments, to test ideas and so forth...Is Artificial Intelligence (only) a research tool for Cognitive Science?What is the difference between Artificial Intelligence and Cognitive Science?'	88	0	0	0
1848	1700	0	12629	0	b" What purpose would be served by developing AI's that experience human-like emotions?Any complex problem involving human emotions, where the solution to the problem requires an ability to sympathize with the emotional states of human beings, will be most efficiently served by an agent that can sympathize with human emotions.Politics. Government. Policy and planning. Unless the thing has intimate knowledge of the human experience, it won't be able to provide definitive answers to all problems we encounter in our human experience."	80	0	0	0
1849	1847	1	4286	3	b' Another impression that I get is that cognitive science is more about trying to find out how the human intelligence(?)... Mind? works... And that it would use artificial intelligence to make tests or experiments, to test ideas and so forth...I think that\'s pretty much it. I mean, clearly there is some overlap, but I feel like most people who use "cognitive science" are referring more to understanding human intelligence for its own sake. Artificial Intelligence, OTOH, is more about implementing "intelligence" on a computer, where the techniques used may or may not be influenced by research done under the rubric of cognitive science.'	103	0	0	0
1850	1847	0	18492	2	b'Artificial intelligence is much more than a research tool for cognitive science. Of course there is some overlapping and researchers of both fields working together. But AI is also broadly used in economics, security (for example face recognition software), advertising, or in the development of games and of course in robotics (autonomous systems). The difference is - as you already mentioned - that cognitive science deals with living things while AI tries to create an intelligence artificially (AI tries to deliver the brain, the mind or the consciousness for a hardware device that then hopefully solves various problems). '	98	0	0	0
1851	-1	0	0	8	b"Just for fun, I am trying to develop a neural network.Now, for backpropagation I saw two techniques.The first one is used here and in many other places too.What it does is:It computes the error for each output neuron.It backpropagates it into the network (calculating an error for each inner neuron).It updates the weights with the formula: (where is the change in weight, the learning speed, the error of the neuron receiving the input from the synapse and being the output sent on the synapse).It repeats for each entry of the dataset, as many times as required.However, the neural network proposed in this tutorial (also available on GitHub) uses a different technique:It uses an error function (the other method does have an error function, but it does not use it for training).It has another function which can compute the final error starting from the weights.It minimizes that function (through gradient descent).Now, which method should be used?I think the first one is the most used one (because I saw different examples using it), but does it work as well?In particular, I don't know:Isn't it more subject to local minimums (since it doesn't use quadratic functions)?Since the variation of each weight is influenced by the output value of its output neuron, don't entries of the dataset which just happen to produce higher values in the neurons (not just the output ones) influence the weights more than other entries?Now, I do prefer the first technique, because I find it simpler to implement and easier to think about.Though, if it does have the problems I mentioned (which I hope it doesn't), is there any actual reason to use it over the second method?"	277	0	1	0
1852	1700	0	65498	1	b"I think that depends on the application of the AI. Obviously if I develop an AI that's purpose is plainly to do specific task under the supervision of humans, there is no need for emotions. But if the AI's purpose is to do task autonomously, then emotions or empathy can be useful. For example, think about an AI that is working in the medical domain. Here it may be advantageous for an AI to have some kind of empathy, just to make the patients more comfortable. Or as another example, think about a robot that serves as a nanny. Again it is obvious that emotions and empathy would be advantageous and desirable. Even for an assisting AI program (catchword smart home) emotions and empathy can be desirable to make people more comfortable. It would be much nicer to be welcomed by an empathic home assistant than by one with no empathic responses at all, wouldn't it? On the other hand, if the AI is just working on an assembly line, there is obviously no need for emotions and empathy (on the contrary in that case it may be unprofitable). "	189	0	0	0
1853	-1	0	0	9	b'The English Language is not well-suited to talking about artificial intelligence, which makes it difficult for humans to communicate to each other about what an AI is actually "doing". Thus, it may make more sense to use "human-like" terms to describe the actions of machinery, even when the internal properties of the machinery do not resemble the internal properties of humanity.Anthropomorphic language had been used a lot in technology (see the Hacker\'s Dictionary definition of anthropomorphization, which attempts to justify computer programmers\' use of anthromporhic terms when describing technology), but as AI continues to advance, it may be useful to consider the tradeoffs of using anthropomorphic language in communicating to both technical audiences and non-technical audiences. How can we get a good handle on AI if we can\'t even describe what we\'re doing?Suppose I want to develop an algorithm that display a list of related articles. There are two ways by which I can explain how the algorithm works to a layman:Very Anthropomorphic - The algorithm reads all the articles on a website, and display the articles that are very similar to the article you are looking at.Very Technical - The algorithm converts each article into a "bag-of-words", and then compare the "bag-of-words" of each article to determine what articles share the most common words. The articles that share the most words in the bags are the ones that are displayed to the user.Obviously, #2 may be more "technically correct" than #1. By detailing the implementation of the algorithm, it makes it easier for someone to understand how to fix the algorithm if it produces an output that we disagree with heavily.But #1 is more readable, elegant, and easier to understand. It provides a general sense of what the algorithm is doing, instead of how the algorithm is doing it. By abstracting away the implementation details of how a computer "reads" the article, we can then focus on using the algorithm in real-world scenarios.Should I, therefore, prefer to use the anthropomorphic language as emphasized by Statement #1? If not, why not?P.S.: If the answer depends on the audience that I am speaking to (a non-technical audience might prefer #1, while a technical audience may prefer #2), then let me know that as well.'	372	0	0	0
1854	1700	0	74571	0	b'Theory of mindIf we want a strong general AI to function well in an environment that consists of humans, then it would be very useful for it to have a good theory of mind that matches how humans actually behave. That theory of mind needs to include human-like emotions, or it will not match the reality of this environment.For us, an often used shortcut is explicitly thinking "what would I have done in this situation?" "what event could have motivated me to do what they just did?" "how would I feel if this had happened to me?". We\'d want an AI to be capable of such reasoning, it is practical and useful, it allows better predictions of future and more effective actions. Even while it would be better for it the AI to not be actually driven by those exact emotions (perhaps something in that direction would be useful but quite likely not exactly the same), all it changes that instead of thinking "what I would feel" it should be able to hypothesize what a generic human would feel. That requires implementing a subsystem that is capable of accurately modeling human emotions.'	191	0	0	0
1855	1853	1	12611	7	b'If clarity is your goal, you should attempt to avoid anthropomorphic language - doing so runs a danger of even misleading yourself about the capabilities of the program.This is a pernicious trap in AI research, with numerous cases where even experienced researchers have ascribed a greater degree of understanding to a program than is actually merited.Douglas Hofstadter describes the issue at some length in a chapter entitled "The Ineradicable Eliza Effect and Its Dangers" and there is also a famous paper by Drew McDermot, entitled "Artifical Intelligence meets natural stupidity".Hence, in general one should make particular effort to avoid anthropomorphism in AI. However, when speaking to a non-technical audience, \'soundbite\' descriptions are (as in any complex discipline) acceptable provided you let the audience know that they are getting the simplified version.'	130	0	0	0
1856	1853	0	35004	1	b'I think the correct answer is the easy but unhelpful, "It depends."Even when I\'m talking to other technical people, I often use anthropomorphic language and metaphors. Especially at the start of the conversation. "The computer has to figure out .." "How can we prevent the computer from getting confused about ..." etc. Sure, we could state that in a more technically correct way. "We need to modify the algorithm to reduce the number and variety of instances of inadequate data that result in inaccurate setting of ..." or some such. But among technical people, we know what we mean, and it\'s just easier to use metaphorical language.When trying to solve technical computer problems, I often start with a vague, anthropomorphic concept. "We should make a list of all the words in the text, and assign each word a weight based on how frequently it occurs. Oh, but we should ignore short, common words like \'the\' and \'it\'. Then let\'s pick some number of words, maybe ten or so, that have the greatest weight ..." All that is a long way from how the computer actually manipulates data. But it\'s often a lot easier to think about it in "human" terms first, and then figure out how to make the computer do it.When talking to a non-technical audience, I think the issue is, Anthropomorphic language makes it easier to understand, but also often gives the impression that the computer is much more human-like than it really is. You only need to watch science fiction movies to see that apparently a lot of people think that a computer or a robot thinks just like a person except that it\'s very precise and has no emotions.'	282	0	0	0
1857	1853	0	77302	3	b'The problem you\'re referencing is not just an AI problem but a problem for highly technical fields in general. When in doubt, I would always recommend using plain language.However, there is another reason the AI community will often eschew anthropomorphic connotations for AI. Some AI luminaries often like warning us that an artificial general intelligence may behave in alien ways that defy our human expectations, potentially leading to a robot apocalypse. This idea about evil alien-like AGIs, however, derives from a widespread misunderstanding in the AI community that conflates two different notions of generality: Turing machine generality, and human domain generality What regular people mean when they say generality is the later. Even the official definition of AGI hinges off of that human-contingent context: ...perform any intellectual task that a human being can.But by that definition, generalizing behavior does not make it more alien. To generalize is to anthropomorphize. As Nietzche said,  "Where you see ideal things, I see\xe2\x80\x94 human, alas! All too human things.\xe2\x80\x9d'	165	0	1	0
1859	-1	0	0	5	b"Roger Schank did some interesting work on language processing with Conceptual Dependency (CD) in the 1970s. He then moved somewhat out of the field, being in Education these days. There were some useful applications in natural language generation (BABEL), story generation (TAILSPIN) and other areas, often involving planning and episodes rather than individual sentences.Has anybody else continued to use CD or variants thereof? I am not aware of any other projects that do, apart from Hovy's PAULINE which uses CD as representation for the story to generate."	86	0	0	0
1860	-1	0	0	2	b"I have been wanting to get started learning about artificial intelligence but I know almost nothing about coding or anything. So my question is, what would be the best way to get started in learning about artificial intelligence, as in should I learn some kind of coding language or is there some kind of other concept you need to know before getting started. So I'm just kind of looking for the best way to get started if you literally know nothing."	80	0	0	0
1861	1860	0	4964	3	b"Read:'Artificial Intelligence - A modern approach' by Russell and Norvig.'Fluid Concepts and Creative Analogies' by Douglas Hofstadter.'Machine Learning and Pattern Recognition' by Bishop'The Emotion Machine' by Marvin Minsky"	27	0	0	0
1862	1618	0	40133	3	b"Using evolutionary algorithms to evolve neural networks is called neuroevolution. Some neuroevolution algorithms optimize only the weights of a neural network with fixed topology. That sounds not like what you want. Other neuroevolution algorithms optimize both the weights and topology of a neural net. These kinds of algorithms seem more appropriate for your aims, and are sometimes called TWEANNs (Topology and Weight Evolving Neural Networks).One popular algorithm is called NEAT, and is probably a good place to start, if only because there are a multitude of implementations, one of which hopefully is written in your favorite language. That would at least give you a baseline to work with. NEAT encodes a neural network genome directly as a graph structure. Mutations can operate on the structure of the network by adding new links (by connecting two nodes not previously connected) or new nodes (by splitting an existing connection), or can operate only on changing the weights associated with edges in the graphs (called mutating the weights). To give you an idea of the order of magnitude of the sizes of ANNs this particular algorithm works with, it would likely struggle with more than 100 or 200 nodes. There are more scalable TWEANNs, but they're more complex and make assumptions about the kinds of structures they generate that may not always be productive in practice. For example, another way to encode the structure of a neural network, is as the product of a seed pattern that is repeatedly expanded by a grammar (e.g. an L-system). You can much more easily explore larger structures, but because they're generated by a grammar they'll have a characteristic self-repeating sort of feel. HyperNEAT is a popular extension of NEAT that makes a different sort of assumption (that patterns of weights can be easily expressed as a function of geometry), and can scale to ANNs with millions of connections when that assumption well-fits a particular domain.There are a few survey papers linked in the top link if you want to observe a greater variety of techniques."	338	0	1	0
1863	1700	0	61748	0	b"Human emotions are intricately connected to human values and to our ability to cooperate and form societies. Just to give an easy example:You meet a stranger who needs help, you feel empathy. This compels you to help him at a cost to yourself. Let's assume the next time you meet him, you need something. Let's also assume he doesn't help you, you'll feel anger. This emotion compels you to punish him, at further cost for yourself.He on the other hand, if he doesn't help you, feels shame.This compels him to actually help you, avoiding your anger and making your initial investment worthwhile. You both benefit. So these three emotions keep up a circle of reciprocal help. Empathy to get started, anger to punish defectors and shame to avoid the anger. This also leads to a concept of justice. Given that value alignment is one of the big problems in AGI, human-like emotions strike me as good approach towards AIs that actually share our values and integrate themselves seamlessly into our society. "	171	0	0	0
1864	1838	0	55640	0	b'Because, ultimately, AI is a cohesive "thing". It\'s an effort to make computers do things that currently only humans can do well. Sure there are many, many approaches and techniques, but there\'s always been a clear overall goal (although the goal-posts keep getting moved further out, which is a different issue).As long as there are things humans can do well that computers can\'t, somebody will be trying to figure out how to close that gap. And those efforts are "Artificial Intelligence".'	80	0	0	0
1865	1825	0	54545	1	b'One of them is certainly Doctor Richard Wallace. Doctor Wallace was the original author of the Artificial Intelligence Markup Language spec and is Chief Science Officer at PandoraBots. '	28	0	1	0
1866	1397	0	47962	1	b'I believe this is exactly the kind of test where Doug Lenat\'s cyc would do very well at ? But I can\'t answer the question : how much of that corpus could it answer correctly ? Probably quite a lot ! (and how many humans could pass that test ? probably not all of them, but many can...) [but is cyc considered an AI? probably not... so I may be out of topic. But imo it\'s database should be incorporated to any AI that reaches some kind of "intelligence"...]'	88	0	0	0
1868	1686	1	10313	2	b"Take a look at this 2012 paper by three people at Bright. (Sadly, it's paywalled and I couldn't easily find a ungated copy, so I don't have a summary for you.) The abstract: Bright has built an automated system for ranking job candidates against job descriptions. The candidate's resume and social media profiles are interwoven to build an augmented user profile. Similarly, the job description is augmented by external databases and user-generated content to build an enhanced job profile. These augmented user and job profiles are then analyzed in order to develop numerical overlap features each with strong discriminating power, and in sum with maximal coverage. The resulting feature scores are then combined into a single Bright Score using a custom algorithm, where the feature weights are derived from a nation-wide and controlled study in which we collected a large sample of human judgments on real resume-job pairings. We demonstrate that the addition of social media profile data and external data improves the classification accuracy dramatically in terms of identifying the most qualified candidates."	173	0	1	0
1870	-1	0	0	5	b'I have been studying local search algorithms such as greedy hill climbing, stochastic hill climbing, simulated annealing etc. I have noticed that most of these methods take up very little memory as compared to systematic search techniques.Are there local search algorithms that make use of memory to give significantly better answers than those algorithms that use little memory (such as crossing local maxima)? Also, is there a way to combine local search and systematic search algorithms to get the best of both worlds?'	82	0	0	0
1871	1838	0	85236	1	b"I don't believe that AI as a coherent field has a lesser legitimacy than, say, Engineering. Ignoring for the moment that we're a day or two behind on AI, they're very much alike: Both fields contain a wide variety of sub-fields which stretch across multiple disciplines (although admittedly more pronounced in AI) , in both fields it is mandatory to specialize and in both of them an expert in one sub-field will be more or less useless in a different one (the expert on bridge construction will probably not be very versed in the thermodynamics of AC systems and vice versa). This pattern can be seen in many of today's disciplines - in fact, I don't know if there still is a reputable field, in which a single person can be a universal expert. You mentioned that the only unifying thing about AI was it's dealing with machines in some fashion - but such a simplifying statement can be made about almost any field. To return to my previous example: the only unifying thing about the various Engineering activities is that they're all somehow involved in the construction of something (be it a flashlight or an aircraft carrier).AI is a young field and therefore its branches have not yet been established in the sophisticated way that the branches of other fields have, but I would assume that it is only a matter of time until the various differentiations and the corresponding degrees, courses etc. develop. AI is also growing up in a time where vast knowledge in its related/parental fields already exists and further knowledge is produced at dizzying speeds - and that is as much a blessing as it is a curse. When Engineering was 'created' a few millennia ago (please excuse my ridiculously inaccurate science history lessons) there wasn't much going on in the world of science and so the field grew slowly, with plenty of time to get organized and structured. That is a luxury which AI did/does not have. It emerged in an age of technical wonders, surrounded by scientific breakthroughs on at least a monthly basis and the rise of interdisciplinary science (which by itself complicated things quite a bit). So in addition to organizing itself, the field also has to continuously integrate the large number of advancements made and somehow stand its ground against the outlandish expectations generated by other science's breakthroughs over the past decades and the media (as already explained by SQLServerSteve). Long story short: it's similar in it's complexity and diversity to other fields and therefore has no reason to collapse - on the contrary, its failure to do so over the past ~50 rather complicated years indicates, that it will further solidify and organize itself in the future. "	458	0	0	0
1872	1870	0	1672	3	b'You could parallelize the search by dividing the global space in distinct regions/subsets. Then apply in each region a local search. This way you can search the global space systematically, more exhaustively and perhaps in different ways (e.g by applying a different local search method to each region). Finally you can compare the results and choose the best one.'	58	0	0	0
1873	156	0	12686	-1	b'We actually do have many things along that line, motion capture for 3-D movies instance comes to mind almost immediately. The problem if I think about it is less of a situation in observing another actor, computers are relativity good at doing that already with the amount of image recognition software we have, rather it\'s a problem of understanding if an action yielded a good outcome as a net which is something that computers cannot do as it\'s not a single node network problem. For example, we\'ve already programmed a computer to understand human language (Watson, arguably), but even Watson didn\'t understand the concept that saying "f***" is bad. (Look that up, it\'s a funny side story.)But the point is, learning algorithms are not true learning in a sense as a computer currently has no sense of "a good outcome", hence at this stage observation learning is very much limited in a sense to "monkey see, monkey do".Perhaps the closest thing I have ever read about with this was firefighting search and rescue bots that were on a network and would broadcast to each other when one of them had been destroyed as the bots would know the area was something that they had to avoid.Otherwise, I think this is the problem with observational learning. A person can observe that punching someone usually will get you hit back, a computer will observe and parrot the action, good or bad.'	238	0	0	0
1874	1870	0	13870	3	b'Tabu search uses memory to rule out parts of the neighborhood for local search, allowing the trajectory to typically pass through local optima instead of getting stuck in them.'	28	0	0	0
1876	-1	0	0	0	b"I know that every program has some positive and negative points, and I know maybe .net programming languages are not the best for AI programming.But I prefer .net programming languages because of my experiences and would like to know for an AI program which one is better, C or C++ or C# and or VB ?Which one of this languages is faster and more stable when running different queries and for self learning ?To make a summary, i think C++ is the best for AI programming in .net and also C# can be used in some projects, Python as recommended by others is not an option on my view !because : It's not a complex language itself and for every single move you need to find a library and import it to your project (most of the library are out of date and or not working with new released Python versions) and that's why people say it is an easy language to learn and use ! (If you start to create library yourself, this language could be the hardest language in the world !)You do not create a program yourself by using those library for every single option on your project (it's just like a Lego game)I'm not so sure in this, but i think it's a cheap programming language because i couldn't find any good program created by this language !"	231	0	0	0
1877	-1	0	0	3	b'When I visit this site, I find the word "search" appears quite often. But why is it important? What kinds of search algorithms are used in Artificial Intelligence? And how do they improve the result of an AI?'	37	0	0	0
1878	1877	0	637	1	b"As tags go, search is relatively uncommon--if it weren't for this question, it wouldn't be on the first page of tags. That said, search is important for at least two reasons.First, searching is one of the early and major consumers of advanced machine learning, as finding the correct result for a search query boils down to predicting the click-through rate for query-result combinations. More relevant results means more clicks, more traffic, and more revenue.Second, many planning and optimization problems can be recast as search problems. An AI deciding on a plan to route packages through a network is searching the space of possible plans for a good one."	107	0	0	0
1879	70	0	80262	0	b"Convolutional neural network can be used whenever patterns are locally correlated and translatable (as in shiftable). This is the case because CNNs contain filters that looks for a certain local patterns everywhere in the input. You'll find local and translatable patterns in pictures, text, time series, etc.It doesn't make as much sense to use CNNs if your data is more like a bag of features with an irrelevant order. In that case you might have trouble detecting patterns that contain features which happen to be farther apart in your input vector. You will not find local and translatable patterns in your data if you can reorder the data points of the input vectors without losing information. "	116	0	0	0
1880	1876	1	12072	1	b'If you\'re talking about pure speed, C will get you there if you really know C and operating systems, etc. C++ is nicer in terms of user friendliness, and won\'t be much slower. I don\'t know much about VB but I don\'t see many benefits.Please clarify at least generally what the AI program is about. It is extremely difficult to answer the generalized question "What programming language is best for AI"? if you know what I mean. ( I vote python :^] )To add: Any programming language of those listed is "stable" if you write things correctly, but C++ and its great IDE\'s will help you to that point much more nicely than C will. C, to fully utilize it\'s potential, requires much fiddling with delicate and precise systems. It\'ll go that little bit faster, as the cost of being less stable in the practical, " uh oh, now I need to troubleshoot this" sense.'	154	0	0	0
1881	1877	0	12829	1	b"In regards to the question you mention (in the comments of the OP), these searches are related to optimization. I'm not sure of your background, so let me describe it from scratch, briefly:Remember the derivative? The base idea is to talk about how the function changes in regards to changes in input. So now, we're out of high school and we're building neural nets. We've done the basic coding, and want to look at how our model is working. Back from our statistics class, we remember we use a certain measure of error (e.g. least squares) to determine the efficacy of the models from that class, so we decide to use that here. We get this error, and it's a bit too big for our liking, so we decide to fiddle with our model and adjust the weights to get that error down. But how? This is where the 'search' comes into play. It's really a search for the best weights to put on the edges of our net to optimize it. We use the derivative (in some fancy ways, using the 'stochasitc' (think random sampling) and other ways the question mentions) to search for which way is 'down' in the high dimensional space of our weights. In other words, what we are searching for is minima or maxima to optimize our neural net, and we 'search' for it by doing a derivative which tells us which way to go, moving a bit in that direction, then doing that again and again iteratively to find (hopefully) the best weights.This video here goes into all the detail you'd want, and I recommend the entire series as a robust but understandable intro to neural nets: Demystifying Neural NetworksGo and look up 'gradient descent' to get any related material. (Note, the gradient here is equivalent to multidimensional derivative direction to go in, and descent is just searching for the minima)"	316	0	0	0
1882	1877	0	13428	2	b'Search has always been a crucial element of AI in multiple ways. First, what many people refer to as "search" is a reflection of how what we call "intelligence" frequently involves searching something: a physical realm, a "state space" of possible solutions, a "knowledge space" where ideas/facts/concepts/etc. are related as a graph structure, etc.Look up some old papers on computer chess, and you\'ll see that a lot of that involves searching a "state space". As such, search algorithms that are efficient (in terms of time complexity and/or space complexity) have always been important to making advances there. And while computer chess is just one example, the principle generalizes to many other kinds of problem solving and goal seeking activities.Here\'s a reference that explains more about some of these ideas.Note too that "search" is closely related to the idea of "heuristics" in an important way. Many search problems in the real world are far too complex to solve by exhaustive brute-force search, so humans (and AI\'s) resort to heuristics to narrow the state space being searched. Using heuristics can yield search algorithms that allow for reasonable solutions in a realistic time-frame, where no simple, deterministic algorithm exists to do likewise. For some more background you might want to read up on A* search, which is a widely used algorithm with many applications - and not just in AI.The other major regard in which something you could call "search" applies in AI is through the use of algorithms which are also often referred to as "optimisation" techniques. This would be things like Hill Climbing, Gradient Descent, Simulated Annealing and perhaps even Genetic Algorithms. These are used to maximize or minimize the values of some function and one of the canonical uses in AI is for training neural networks using back-propagation, where you\'re trying to minimize the delta between the "correct" answer (from the training data) and the generated answer, so you can learn the correct weights within the network.'	326	0	1	0
1883	1877	1	15947	5	b'`State space search\' is a general and ubiquitous AI activity that includes numerical optimization (e.g. via gradient descent in a real-valued search space) as a special case.State space search is an abstraction which can be customized for a particular problem via three ingredients:Some representation for candidate solutions to the problem (e.g.permutation of cities to represent a Travelling Salesman Problem(TSP) tour, vector of real values for numeric problems).Asolution quality measure: i.e. some means of deciding which of twosolutions is the better. This is typically achieved (forsingle-objective problems) by having via some integer or real-valuedfunction of a solution (e.g. total distance travelled for a TSPtour). Some means of moving around in the space of possible solutions, in a heuristically-informed manner. Derivatives can be used ifavailable, or else (e.g. for black-box problems or discrete solutionrepresentations) the kind of mutation or crossover methods favouredby genetic algorithms/evolutionary computation can be employed.The first couple of chapters of the freely available "Essentials of Metaheuristics" give an excellent overview and Michalewicz and Fogel\'s "How to Solve It - Modern Heuristics" explains in more detail how numerical optimization can be considered in terms of state-space.In response to a request in the comments to explain how clarify how "the \'search through possible plans\' might occur", the idea is to choose all three of the above for the planning problem and then apply some metaheuristic such as Simulated Annealing, Tabu Search, Genetic Algorithms etc. Clearly, for nontrivial problems, only a small fraction of the space of "all possible plans" is actually explored.CAVEAT: Actually planning (in contrast to the vast majority of other problems amenable to state-space search such as scheduling, packing, routing etc) is a bit of a special case, in that it is sometime possible to solve planning problems simply by using A* search, rather than searching with a stochastic metaheuristic.'	300	0	1	0
1884	1877	0	16494	0	b'The aim of an AI is to fulfill one or the other task, say solve the task adequately. But there are results that are no solutions at all and there are results which are satisfying the task and thus are accepted as solutions. Since there are generally more results that are no solutions, the set of all possible solutions is just a subset of all results. But this means that the task involves the search for a suitable set of solutions. '	81	0	0	0
1885	-1	0	0	4	b'Considering the answers of this question, emulating a human brain with the current computing capacity is currently impossible, but we aren\'t very far from it.Note, 1 or 2 decades ago, similar calculations had similar results.The clock frequency of the modern CPUs seem to be stopped, currently the miniaturization (-> mobile use), the RAM/cache improvement and the multi-core paralellization are the main lines of the development.Ok, but what is the case with the analogous chips? In case of a NN, it is not a very big problem, if it is not very accurate, the NN would adapt to the minor manufacturing differences in its learning phase. And a single analogous wire can substitute a complex integer multiplication-division unit, while the whole surface of the analogous printed circuit could work parallel.According to this post, "software rewirable" analogous circuits, essentially "analogous FPGAs" already exist. Although the capacity of the FPGAs is highly below the capacity of the ASICs with the same size, maybe analogous chips for neural networks could also exist.I suspect, if it is correct, maybe even the real human brain model wouldn\'t be too far. It would still require a massively parallel system of costly analogous NN chips, but it seems to me not impossible.Could this idea work? Maybe there is even active research/development into this direction?'	215	0	2	0
1886	1876	0	24292	0	b"In the Visual Studio, there is no real difference between C and C++. It is compiled with the same compiler binary, although with different flags.In most cases, the easily programmable complex data structures are generally more important, as the linear speed. The AI is an exception. AI has mostly not so complex data structures, and also the linear speed improvement is very important.It closes out the garbage collected languages, i.e. any managed code, and, in my opinion, the best solution would be if you would use not a .net-based language, but a directly to asm compilable one.But, knowing that you are asking explicitly for a .net one, I would suggest one, which can be easily ported later to machine code. It closes out C# and VB, but it doesn't close out C++.C++ has also the needed complex data structures, while it still has the near-asm speed (and memory need).My idea would be to start with unmanaged C++ in .net, but use simply, native .exe-s to your final programs. It would make also possible to make your program portable, because C++ is for everywhere, while .net only for windows. I.e. your program will be later able to run in non-windows server (or cluster) environment, too.It would be also important to prefer the deeply parallel or easily parallelizable algorithms. Ideally it should be made adaptable to slow communication channels (also for the parallel cluster run).--Your program will probably have some user interface, persistent database and similar things, these aren't speed and memory critical things, thus these you can implement in anything as you wish. The result will be a two-process solution, where a speed-optimized, C++ calculating daemon is controlled by essentially a GUI (or db.. or script.. or anything) interface.Probably there are already C++ frameworks for this task, so you don't need to reinvent the wheel."	303	0	0	0
1888	1824	0	76341	0	b'AI is already used as weapon - think on the drones.I suspect, a "robots take over the world" scenario has the highest probability, if it has an intermediate step. This intermediate step could be "humans take over the world with robots".This can go somewhere into a false direction.I suspect, it is not surely so far as it seems. Consider the US has currently 8000 drones. What if it would have 8million? A small group capable to control them could take over the world. Or the small groups controlling different parts of the fleet, could fight against eachother. They shouldn\'t be all in the US - at the time the US will have this fleet, other countries will develop also theirs.Btw, a world takeover seem to me unreal - the military leaders can maybe switch the human pilots to drones, it is not their job. But the "high level control", i.e. to determine, what to do, who are the targets, these decisions they won\'t ever give out from their hands.Next to that, the robots doesn\'t have a long-term goal. We, humans, have.Thus I don\'t consider a skynet-style takeover very realistic, but a chernobyl-style "mistake" of a misinterpreted command, which results the unstoppable rampage of the fleet, doesn\'t seem to me impossible.'	209	0	0	0
1889	1885	0	3658	3	b'I\'m not sure about "emulating the brain" per-se, but in a more general sense there has been some thought given to using analog computing for AI/ML. It seems clear that analog computers do have certain advantages over digital computers. For one, they can (depending on the application) be faster, albeit at the cost of some loss of precision. But that\'s OK, because I don\'t think anybody believes the human brain is calculating floating point math using digital computing techniques either. The human brain appears, at least superficially, to be largely probabilistic and able to tolerate some "slop" numerically. The downside to analog computers, as I understand it, is that they\'re not as flexible... you basically hardwire a circuit to do one specific "thing" and that\'s really all it can do. To change the "programming" you have to literally solder in a new component! Or, I suppose, adjust a potentiometer or adjustable capacitor, etc. Anyway, the point is that digital computers are supremely flexible, which is one big reason they came to dominate the world. But I can see where there could be room for going analog for discrete functions that make up some or all of an intelligent system.As for research in the area, you might look into whatever DARPA was / is doing. There was an article in Wired a while back, talking about some DARPA initiatives related to analog computing. '	232	0	1	0
1890	1824	0	76940	1	b'I feel like most of the scenarios about AI\'s wiping out the world fall into one of two categories:Anthropomorphized AI\'sorIntelligent But Dumb Computer Run AmuckIn the (1) case, people talk about AI\'s becoming "evil" and attribute to them other such human elements. I look at this as being mostly sci-fi and don\'t think it merits much serious discussion. That is, I see no particular reason to assume that an Artificial Intelligence - regardless of how intelligent it is - will necessarily behave like a human.The (2) case makes more sense to me. This is the idea that an AI is, for example, put in control of the nuclear missile silos and winds up launching the missiles because it was just doing it\'s job, but missed something a human would have noticed via what we might call "common sense". Hence the "Intelligent but Dumb" moniker. Neither of these strikes me as terribly alarming, because (1) is probably fiction and (2) doesn\'t involve any actual malicious intent by the AI - which means it won\'t be actively trying to deceive us, or work around any safety cut-outs, etc.Now IF somebody builds an AI and decides to intentionally program it so that it develops human like characteristics like arrogance, ego, greed, etc... well, all bets are off.'	213	0	0	0
1891	1806	0	5430	0	b'Yes.Every chess game... every poker game. Every game.Every more intelligent spam softwares or spambots. Although their primary goal is to lie to computer systems (f.e. spamfilter poisoning), their secondary goal is to lie to the human behind them.'	37	0	0	0
1892	156	0	85728	0	b'Whether "I take the ball" or "he takes the ball", all stored instances of \'taking\' and \'ball\' will be weakly activated and \'taking [the] ball\' will be strongly activated. Doesn\'t this qualify as \'mirroring\'? If you also know that "I have an arm" and "he has an arm", etc., then when "he takes some blocks", it isn\'t too hard to think that "I could take some blocks."'	66	0	0	0
1894	1877	0	44719	0	b'Every problem can be reduced to search. Every problem has an input within some range (the domain) and an output in some other range (codomain). That is, every problem can be formulated as a kind of map from one space to another, where the source is the givens of the problem, and the destination is the solution to the problem."Brute force" is the algorithm which solves every problem by inspecting every point in the codomain and asking: "Is this the solution?" Every other algorithm is an attempt to improve on brute force by not searching the entire codomain of possible solutions.Typical software engineering problems can be solved by algorithms which arrive at the correct solution very quickly (sorting, arithmetic, partition, etc.). AI problems are generally those for which a strong polynomial algorithm is not known, and thus, we must settle for approximations. Basically every common problem that the human brain must solve falls into this category.Consider the problem of moving a multi-jointed robotic arm to pick up an object. Reverse kinematics does not have unique solutions: there is more than one way to move your hand from a start position to a target position. This is due to the excessive degrees of freedom in your joints. If you want to minimize energy usage, then there is a unique solution (due to the asymmetry of joints and muscles).But what if there is an obstacle in the pathway of the minimum-energy solution? There are many pathways which avoid the obstacle, but again, many of them will have a similar cost. Even if there is a unique minimum-energy solution, it might not be the most practical to compute. The brain is the most metabolically expensive organ in the body, so it is not always best to find an optimal solution. Thus, heuristics come into play.But in all cases, the problem is not: "move your hand" or "move the robot arm." The problem is: "search the space of joint rotation sequences which best achieves the goal." And even though there is a closed-form solution for the simple minimum-energy case with no obstacles, it is too expensive to compute precisely when a set of cheap heuristics will get you very close with a small fraction of the computational effort.If computation were free, then AI would be mere mathematics, and we would always compute the best answer to every question using logic, calculus, physics, at worst, numerical methods when we don\'t have closed-form solutions. In reality, time is money, and the time and effort to get an answer is as much a part of the cost as the quality of the solution. So it is an engineering tradeoff to decide how much effort should be expended in what way to obtain the best answer given the value of the response.Or, in other words, AI problems are all about searching the space of solutions as quickly as possible to get an answer that is "good enough".I might seem curious that such far-flung problems as natural language recognition and theorem proving would be search problems. But language parsers strive to determine the meaning of statements via part-of-speech tagging. A given phrase can be parsed in many different ways, yielding many different interpretations, and the space of parse trees is yet another search problem in deciding which parse tree is the most likely intended meaning by the speaker.A theorem proof is graph starting with axioms, proceeding through lemmas, applying the rules of procedure until the theorem is derived or refuted (by proving its negation). There are many ways to represent this sequence, but at the end of the day, we are talking about a process of exploring the intermediate proof space and finding the derivation which reaches your goal. Everything is search, in the end.'	625	0	0	0
1895	-1	0	0	1	b"Conceptually speaking, aren't artificial neural networks just highly distributed, lossy compression schemes?They're certainly efficient at compressing images.And aren't brains (at least, the neocortex) just compartmentalized, highly distributed, lossy databases?If so, what salient features in RNNs and CNNs are necessary in any given lossy compression scheme in order to extract the semantic relations that they do? Is it just a matter of having a large number of dimensions/variables? Could some kind of lossy Bloom filter be re-purposed for the kinds of problems ANNs are applied to?"	84	0	0	0
1896	1895	0	19936	1	b"ANNs don't compress, they generalise. Often this leads to compression, i.e. the internal generalised representation is smaller than the original input, but not necessarily. Imagine a ANN that is trained to use the screen input of a computer game to play it. If the game is very rich and conceptually deep the internal representation of a single screen input might be a lot bigger than the input itself, because ANNs put the single data points into the context of the overall data. Which leads us to the second point:ANNs (and the neocortex) model data hierarchically. This is what makes them so powerful. So it is not just about having a large number of parameters, they also have to be arranged in such a way that they capture the structure of the data (or the world), which very often seems to be hierarchical. Just look a two different pictures of a duck. On the pixel level they might be as different as random images, all the similarities emerge in higher levels of the hierarchy, when enough pixels combined give you the patterns of feathers, beak and webs. Bloom filters obviously lack this property. They would only give you a pixel by pixel account of whether you have seen (almost) exactly this picture before. "	212	0	0	0
1897	-1	0	0	11	b'Consciousness is challenging to define, but for this question let\'s define it as "actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine." Humans, of course, have minds; for normal computers, all the things they "see" are just more data. One could alternatively say that humans are sentient, while traditional computers are not.Setting aside the question of whether it\'s possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?'	106	0	2	0
1898	1897	1	207	8	b"No-one knows.Why: because it's not possible to formally determine even whether your fellow human beings are actually conscious (they may instead be what is philosophically termed a 'Zombie'). No test known to modern physics suffices to decide. Hence it's possible that you are the only sentient being, and everyone else is a robot.Consequently, we cannot determine which tasks require sentience.Note that the ontological status of Zombies is controversial: some philosophers of AI (e.g. Daniel Dennett) claim that Zombies are logically impossible while others such as David Chalmers would claim that a Zombie would be compelled to assert that they experience qualia (i.e. are sentient) even though they do not. Here is a very readable paper by Flanagan and Polger that also explains why a stronger neurological version of the Turing test is insufficient to detect a Zombie.EDIT: In response to the comment about whether an objective test for distinguishing sentience from non-sentience exists:No-one knows. What we do believe is that this would require something in addition to what modern physics can currently tell us. David Chalmers has speculated that qualia should be introduced as a new form of physical unit, orthogonal to the others in the same way that electrical charge is orthogonal to distance.In the absence of an objective test, we have to rely on Turing test variants, which no more guarantee consciousness in the subject than they do intelligence."	230	0	3	0
1899	1897	0	3952	-1	b'In a very niche sense, I\'d say yes.The only tasks that sentience would make possible was the actual feeling and thinking in and of itself. At this point, sentience doesn\'t play a part in any of the tasks we ask AI\'s to complete; we are rapidly approaching the point of being able to teach a \'dead\' machine to do most anything a sentient AI can, in a practical sense.Sentience colloquially often translates to \'the ability to reason while understanding that oneself and each other entity is a distinct acting agent\'or something along those lines. It literally means something more along the lines of self-awareness and the definition of consciousness you have above. The point I\'m making is that we are readily approaching the point where \'dead\' AI\'s can very nicely mimic the first way of thinking, just by really nicely learning and interpreting data.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Does the robot see an amalgamation of bone, or a being that once was?Thus, a truly sentient machine would be superior in capability (compared to a really, really advanced \'dead\' AI) only in the respect of being able to \'truly\' experience the information.This runs very well in parallel with the so-called "Knowledge Argument" which in essence debates this very issue. The version of it that I heard which sticks with me is that there is a very smart girl in a room with access to all sorts of information. She likes the color blue. Or so she thinks; she\'s never actually seen it. She has all the information in the world available about colors and how they work, etc. but does she really know what blue is until she sees it?Another great, historic venture into this field is the famous painting:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The caption translates: "This is not a pipe". And the idea is that this, honestly, isn\'t a pipe. Right now it\'s a bunch of pixels on your screen in a certain configuration - we can all \'see\' a pipe, but what does that really mean?At the end of the day, I think super-intelligent \'dead\' AI can practically do anything a \'live\' one can, with the latter being superior in and of the \'liveness\' itself.'	356	0	0	0
1901	1897	0	9004	-1	b'Two kinds of tasks require consciousness:consciousnessAny task that requires extreme dynamicity, where solving problems requires analogizing between various 3D states of affairs and prior knowledge of how to solve the problem is minimalHowever, once knowledge of how to solve a given problem is gained, further optimization will eliminate that need for consciousness.If you give enough specificity to a problem, you remove the need for a general solver. And then the only remaining need for a consciousness is for the sake of itself.'	81	0	0	0
1902	1877	0	17932	0	b"Consciousness is an attention selection mechanism that searches over salient inputs. The robotic saccades of your eyeballs show you first hand the algorithmic nature of your brain's conscious attention mechanism, while it searches among salient inputs.A smart search algorithm can help with dimensionality reduction."	43	0	0	0
1903	1897	0	15850	-2	b"A being without sentience cannot suffer. If, for example, we wanted to take joy in the suffering of another, only an AI that was sentient would suffice.Suppose we had some sadists who could not be satisfied or productive unless they got to produce lots of suffering. And say we only cared about minimizing human and animal suffering. What we would need for this job is something non-human and non-animal that could suffer. A conscious AI would do, a non-conscious one would not.The claim was made in the comments that consciousness cannot be proven, other than perhaps by introspection. But clearly this is not a problem since sadists take joy in torturing others, and those others cannot prove they're conscious either."	119	0	0	0
1906	-1	0	0	3	b'A lot of textbooks and introductory lectures typically split AI into connectionism and GOFAI (Good Old Fashioned AI). From a purely technical perspective it seems that connectionism has grown into machine learning and data science, while nobody talks about GOFAI, Symbolic AI or Expert Systems at all. Is anyone of note still working on GOFAI? '	55	0	0	0
1907	1906	0	781	1	b"Oh yeah, definitely. Just to pick one example, you have Douglas Hofstader's group at Indiana. I think most of what they do would fall under the rubric of GOFAI (or at least closer to that than the statistical machine learning stuff). Beyond that, just go to the CORR and browse around the AI category. You'll see plenty of neural networks and probabilistic stuff, but you'll also find the papers by the folks doing symbolic processing / GOFAI as well."	78	0	3	0
1908	1897	0	26863	3	b"No.The experience of seeing is by definition non-causal. Anything non-causal cannot be a requirement of a physical process; a qualia cannot afford a robot the ability to do something it otherwise could not.Maybe.Although a qualia is not required for a given AI task, that is not to say that any sufficiently advanced AI does not entail qualia. It could be that so-called AI-complete tasks require a robot that, although not making use of qualia, produces it anyway.Yes.Qualia may refer to some wishy-washy non-physical property, but it's special in that we know it exists physically, too. The fact I am able to discuss my qualia knowingly (or, if you don't believe me, the fact you are able to) implies that my (or your) qualia does have a physical effect.It stands to reason that if we accept others' qualia on the basis of our own, it must be because of the physical basis of our own1. Thus one could argue that2 any robot that has an equivalent physical capacity must entail qualia.1 since the subjective is physically non-causal, so cannot cause us to accept anything.2 as long as you don't make the particularly odd assumption that qualia is somehow tied to its direct physical manifestation, which at best is tenuous since had we evolved the wrong one you would still claim it to be the right one with equal certainty."	227	0	0	0
1909	-1	0	0	2	b"I recently finished Course on RL by David Silver (on YT) and thought about trying it out on simple application in Unity Game Engine, where I've built simple labyrint with ball and want to teach the ball to get from point A to point B in there while avoiding obstacles and fire (the place where you'll get burnt so big negative reward)The problem I encountered while designing the whole thing (programming-wise) is: What is the correct (or at least good) way of representing the position in 2D space? It is continuous so I thought about representing it as feature vector consisting of [up, down, left, right, posX, posY] where direction is whether I am pressing button of moving in that direction in binary (or actions if you want) and pos are floats (0-1) representing normalized position from one corner on the plane where the whole map is. That would be accompanied by vector W that would represent the weights adjusted using Gradient Descent.Question is: will this work?? I am asking for 2 reasons. One is that I am not so sure about that posX and posY since it can be 0 and if I multiply it by the weights vector then how could be resulting reward anything but 0? Second reason is that I am not sure if the actions should be part of the features. I mean, it makes sense to me but I could easily be very wrong since I am a beginner.Thanks a lot guys in advance. If you have any more questions or think the problem is not described deeply enough just ask in the comments and I'll edit the question. :)PS: I could just code it the way I think is right, but I also want to get gasp of designing applications on paper before coding them (project management)."	303	0	0	0
1910	1644	0	32840	0	b"It's not a difficult task, first of all you have to locate the body parts such as arms,head... you can do it using different approaches for example using cascadeclassifier or a well trained CNN.After that you can use different techniques, one could be an ANN trained on the keypoints of the different body parts (this is the easiest approach) or a CNN (good approach but you need a lot of training). To indicate the location after you have determined the position of the head (and the eyes to) and hands, you can simply calculate the orientation of those parts, and then you can get a general position where those orientation are pointing to."	112	0	0	0
1911	1393	0	37934	1	b"I'm not sure what google is using to perform that task but most companies use region based convolutional neural nets to locate traffic signs and other objects.But other companies use a Deep neural net + Bag of words approach to find objects.See: Bag-of-Words Based Deep Neural Network for Image Retrieval which shows a general approach, to get the exact location you can use Feature Matching or Random Boxes."	67	0	0	0
1912	1644	1	40041	3	b"Just to add some discourse; this is actually an incredibly complex task, as gestures (aka kinematics) function as an auxiliary language that can completely change the meaning of a sentence or even a single word. I recently did a dissertation on the converse (generating the correct gesture from a specific social context &amp; linguistic cues). The factors that go into the production of a particular gesture include the relationship between the two communicators (especially romantic connotations), the social scenario, the physical context, the linguistic context (the ongoing conversation, if any), a whole lot of personal factors (our gesture use is essentially a hybrid of important individuals around us e.g. friends &amp; family, and this is layered under the individual's psychological state). Then the whole thing is flipped around again when you look at how gestures are used completely differently in different cultures (look up gestures that are swear words in other cultures for an example!). There are a number of models for gesture production but none of them capture the complexity of the topic.Now, that may seem like a whole lot of fluff that is not wholly relevant to your question, but my point is that ASIMO isn't actually very 'clever' at this. AFAIK (I have heard from a visualization guy that this is how he thinks they do it) they use conventional (but optimized) image recognition techniques trained on a corpus of data to achieve recognition of particular movements. One would assume that the dataset consists of a series of videos / images of gestures labelled with that particular gesture (as interpreted by a human), which can then be treated as a machine learning problem. The issue with this is that it does not capture ANY of the issues I mentioned above. Now if we return to the current best interpretation of gesture that we have (that it is essentially auxiliary language in its own right), ASIMO isn't recognizing any element of language beyond the immediately recognizable type, 'Emblems'.'Emblems' are gestures which have a direct verbal translation, for example in English-based cultures, forming a circle with your thumb and index finger translates directly to 'OK'. ASIMO is therefore missing out on a huge part of the non-verbal dictionary (illustrators, affect displays, regulators and adapters are not considered!), and even the part that it is accessing is based on particular individuals' interpretations of said emblems (e.g. someone has sat down and said that this particular movement is this gesture which means this), which as we discussed before is highly personal and contextual. I do not mean this in criticism of Honda; truth be told, gesture recognition and production is in my opinion one of the most interesting problems in AI (even if its not the most useful) as it is a compound of incredibly complex NLP, visualization and social modelling problems!Hopefully I've provided some information on how ASIMO works in this context, but also on why ASIMO's current process is flawed when we look at the wider picture."	497	0	0	0
1913	-1	0	0	1	b'Currently I work as a java developer, But very much interested in learning Artificial Intelligence.Can anybody tell me what steps i have to follow to learn artificial intelligence considering the fact i am very new to this.Is there any special technologies i have to learn or something else.'	47	0	0	0
1914	-1	0	0	5	b'Self-Recognition seems to be an item that designers are trying to integrate into artificial intelligence. Is there a generally recognized method of doing this in a machine, and how would one test the capacity - as in a Turing-Test?'	38	0	0	0
1915	1914	0	4668	4	b'Interesting question. I don\'t think anybody knows a definite answer, but some rough-sketch ideas seem apparent. Think about what it means to you to be "self aware". You\'ll probably cite the way you "hear" your own thoughts in your head when you think about something. One can speculate that inside the brain, the various centers that are responsible for hearing, vision, logic, etc. are connected so that as you form a thought, it\'s being "heard" by the hearing regions, even though it\'s purely internal instead of actual sound received at the ear.So in AI terms, it seems likely that self-awareness will somehow involve taking the "thoughts" formed within the AI, and feeding them back into the AI so that it "hears" (or, more broadly, "senses") itself think. There\'s this weirdly recursive aspect to all of this, which - interestingly enough - is something Douglas Hofstadter talked about a lot in some of his book, especially GEB. He was probably onto something. '	161	0	0	0
1916	1909	0	24579	1	b'I think your net should have the various actions as outputs, but I am not an expert in Deep Nets. I just think that that light form of multi-task learning might be better. The idea of multi-task learning is that a predictor predicting multiple variables (in this case the various Q(s,a1), Q(s,a2), ...) using mostly the same structure (varying only the output weights) will learn more sensible things. Though I admit applying this here might be a bit of a stretch.As for the real question, a popular technique in Reinforcement Learning is Tile Coding.The basic idea is to discretize the (2-dimensional, in your case) state-space - imagine a grid laid over the 2D space - and assigning an input feature to each cell; all of these variables are set to zero except for the one your continuous variables fall into. For example, if your grid is 20x20, you will have 400 variables, 399 of which are set to zero, and 1 set to one.Tile Coding takes this one step further and repeats this using slight offsets for the grid. Imagine you create an identical grid but you move it slightly to the right by 1/10 of the width of a cell: you will have another set of 400 variables like before, but it is possible that the cell set to one is not the same. Then you repeat this moving the grid by 2/10 and you have another set of 400 variables, again, only 1 of which is set to one. In total you have 10 sets of 400 variables (if you repeat more than that, you get the same grids as before); of your 4000 variables, only 10 are set to one.Now you repeat this by adding a 1/10 of a cell offset in the Y axis and obtain another 4000 variables. Repeat with 2/10 and you get another 4000. By the end of it, you have 40000 variables, 100 of which are set to one.Now your net can more easily learn different weights for different positions. I recommend you to follow the link above for a better explanation than mine (and figures!)My suggestion is to feed all of these variables to your net and have it predict the Q-value for all of the actions. But, again, I am no expert in deep nets so I may be wrong.Also, according to Andrej Karpathy, "most people prefer to use Policy Gradients, including the authors of the original DQN paper who have shown Policy Gradients to work better than Q Learning when tuned well.". This means that you may be better off not using Q-learning (as they did in the original DQN formulation) to train your net. Have a look at Andrej\'s blog and the paper he points to.'	457	0	2	0
1917	1913	1	15492	2	b'A really good introduction is the Berkeley CS188 class videos and projects. You can find those materials at You probably also want to get ahold of a copy of Artificial Intelligence: A Modern Approach by Norvig and Russell. For more on the "machine learning" aspects of AI, including an introduction to Neural Networks, take the Andrew Ng "Machine Learning" class on Coursera.Another book I would recommend is Introduction to Artificial Intelligence by Philip C. Jackson. It\'s older, which is exactly what makes it valuable. It\'s a good overview of techniques and ideas that aren\'t "en vogue" right now, but which may still be useful to you. '	106	0	3	0
1918	1700	0	15880	1	b'Strong AIsFor a strong AI, the short answer is to call for help, when they might not even know what the supposed help could be.It depends on what the AI would do. If it is supposed to solve a single easy task perfectly and professionally, sure emotions would not be very useful. But if it is supposed to learn random new things, there would be a point that it encounters something it cannot handle.In Lee Sedol vs AlphaGo match 4, some pro who has said computer doesn\'t have emotions previously, commented that maybe AlphaGo has emotions too, and stronger than human. In this case, we know that AlphaGo\'s crazy behavior isn\'t caused by some deliberately added things called "emotions", but a flaw in the algorithm. But it behaves exactly like it has panicked.If this happens a lot for an AI. There might be advantages if it could know this itself and think twice if it happens. If AlphaGo could detect the problem and change its strategy, it might play better, or worse. It\'s not unlikely to play worse if it didn\'t do any computations for other approaches at all. In case it would play worse, we might say it suffers from having "emotions", and this might be the reason some people think having emotions could be a flaw of human beings. But that wouldn\'t be the true cause of the problem. The true cause is it just doesn\'t know any approaches to guarantee winning, and the change in strategy is only a try to fix the problem. Commentators thinks there are better ways (which also don\'t guarantee winning but had more chance), but its algorithm isn\'t capable to find out in this situation. Even for human, the solution to anything related to emotion is unlikely to remove emotions, but some training to make sure you understand the situation enough to act calmly.Then someone has to argue about whether this is a kind of emotion or not. We usually don\'t say small insects have human-like emotions, because we don\'t understand them and are unwilling to help them. But it\'s easy to know some of them could panic in desperate situations, just like AlphaGo did. I\'d say these reactions are based on the same logic, and they are at least the reason why human-like emotions could be potentially useful. They are just not expressed in human-understandable ways, as they didn\'t intend to call a human for help.If they tries to understand their own behavior, or call someone else for help, it might be good to be exactly human-like. Some pets can sense human emotions and express human-understandable emotion to some degree. The purpose is to interact with humans. They evolved to have this ability because they needed it at some point. It\'s likely a full strong AI would need it too. Also note that, the opposite of having full emotions might be becoming crazy.It is probably a quick way to lose any trust if someone just implement emotions imitating humans with little understanding right away in the first generations, though.Weak AIsBut is there any purposes for them to have emotions before someone wanted a strong AI? I\'d say no, there isn\'t any inherent reasons that they must have emotions. But inevitably someone will want to implement imitated emotions anyway. Whether "we" need them to have emotions is just nonsense.The fact is even some programs without any intelligence contained some "emotional" elements in their user interfaces. They may look unprofessional, but not every task needs professionality so they could be perfectly acceptable. They are just like the emotions in musics and arts. Someone will design their weak AI in this way too. But they are not really the AIs\' emotions, but their creators\'. If you feel better or worse because of their emotions, you won\'t treat individul AIs so differently, but this model or brand as a whole.Alternatively someone could plant some personallities like in a role-playing game there. Again, there isn\'t a reason they must have that, but inevitably someone will do it, because they obviously had some market when a role-playing game does.In either cases, the emotions don\'t really originate from the AI itself. And it would be easy to implement, because a human won\'t expect them to be exactly like a human, but tries to understand what they intended to mean. It could be much easier to accept these emotions realizing this.Aspects of emotionsSorry about posting some original research here. I made a list of emotions in 2012 and from which I see 4 aspects of emotions. If they are all implemented, I\'d say they are exactly the same emotions as of humans. They don\'t seem real if only some of them are implemented, but that doesn\'t mean they are completely wrong.The reason, or the original logical problem that the AI cannot solve. AlphaGo already had the reason, but nothing else. If I have to make an accurate definition, I\'d say it\'s the state that multiple equally important heuristics disagreeing with each other.The context, or which part of the current approach is considered not working well and should probably be replaced. This distinguishes sadness-related, worry-related and passionate-related.The current state, or whether it feels leading, or whether its belief or the fact is supposed to turn bad first (or was bad all along) if things go wrong. This distinguishes sadness-related, love-related and proud-related.The plan or request. I suppose some domesticated pets already had this. And I suppose these had some fixed patterns which is not too difficult to have. Even arts can contain them easily. Unlike the reasons, these are not likely inherent in any algorithms, and multiple of them can appear together.Who supposedly had the responsibility if nothing is changed by the emotion. This distinguishes curiosity, rage and sadness.What is the supposed plan if nothing is changed by the emotion. This distinguishes disappointment, sadness and surprise.The source. Without context, even a human cannot reliably tell someone is crying for being moved or thankful, or smiling for some kind of embarrassment. In most other cases there aren\'t even words describing them. It doesn\'t make that much difference if an AI doesn\'t distinguish or show this specially. It\'s likely they would learn these automatically (and inaccurately as a human) at the point they could learn to understand human languages.The measurements, such as how urgent or important the problem is, or even how likely the emotions are true. I\'d say it cannot be implemented in the AI. Humans don\'t need to respect them even if they are exactly like humans. But humans will learn how to understand an AI if that really matters, even if they are not like humans at all. In fact, I feel that some of the extremely weak emotions (such as thinking something is too stupid and boring that you don\'t know how to comment) exist almost exclusively in emoticons, where someone intend to show you exactly this emotion, and hardly noticeable in real life or any complex scenerios. I supposed this could also be the case in the beginning for AIs. In the worst case, they are firstly conventionally known as "emotions" since emoticons works in these cases, so it\'s easier to group them together, but very few people seriously think they are, just like the example I gave.So when strong AIs become possible, none of these would be unreachable, though there might be a lot of work to make the connections. So I\'d say if there would be the need for strong AIs, they absolutely would have emotions.'	1250	0	0	0
1919	1897	0	25050	1	b'Let\'s use a simple test based on common sense: how often do you see a human being solve problems requiring the use of reason when they\'re unconscious? Yes, you can find instances of geniuses like Ramanujan solving complex problem during or after a dream state, but those involve partial consciousness. You don\'t see guys like Einstein coming up with the theory of relativity while in a coma; the Founding Fathers didn\'t write the Declaration of Independence while sleep-walking; in fact, you can\'t even find instances of housewives putting together their shopping list for the week during deep delta-wave sleep. This is predicated on a hard definition of intelligence, requiring the use of reason; no one says, "That fly is intelligent" or "that squirrel is intelligent" precisely because neither is capable of using reason. This is a very high bar for A.I., but it is the common sense definition used by ordinary people as a matter of practicality, in everyday speech. Likewise, in practice, everyone assumes consciousness is necessary to the exercise of that kind of intelligence.Conversely, we can come up with another common-sense based criterion for judging objections to this argument, particularly the solipsist one, based on 3 elements: 1) practicality; 2) the effect the objections have on those who hold them sincerely; and 3) the effect that actions based on those beliefs have on others. It\'s going to take me several paragraphs to make this case, but the length is necessary if I want to make the case in a complete, thorough fashion. It is true that we cannot prove that another human being possesses consciousness, if our standard is absolute proof. We cannot, in fact, provide absolute proof for anything; there\'s always room for some objection, no matter how ridiculous or trifling. As some philosophers have pointed out, perhaps all of reality as we know it is just a dream, or the product of some long, involved conspiracy like the plot of the Jim Carrey movie The Truman Show. The key to meeting such objections is that they require an infinite regress of increasingly untenable objections, whose likelihood plunges with each additional step required to justify such unreasonable doubts; I\'ve always wondered if we could come up with a "Ridiculousness Metric" for Machine Learning based on the cardinality of such objections (or the pickiness of fuzzy sets). If we were to allow critics to stick their foot in the door with all manner of unreasonable objections, it would be impossible to close any debate. The human race would be paralyzed in inaction because nothing would be decidable; but as the rock band Rush once pointed out, "If you choose not to decide, you still have made a choice." At some point we must apply a test to decide such things, even in the absence of absolute proof; refusal to apply a test also constitutes a choice. Settling an argument of this kind is like a game of the Chinese game Go - once the other player\'s surrounded and has no more moves left to make, the game is over; if a person\'s evidence has debunked and they have no further justifications left, then we can conclude that they\'re acting unreasonably. There are people running around claiming the Holocaust never happened, or the Flat Earth Society, etc., but their existence shouldn\'t and doesn\'t stop us from taking action contrary to their ideas. We can debunk the objections of cranks like the Flat Earth Society beyond a reasonable doubt because in the end, they simply can\'t answer all of our rebuttals. I\xe2\x80\x99m glad that qualia and Philosophical Zombies were brought up because they make for interesting conversation and food for thought, but solipsism is acted upon as rarely as the ideas of the Flat Earth Society precisely because the incomplete evidence we do have runs against it.As G.K. Chesterton (a.k.a. "The Apostle of Common Sense") points out in his classic Orthodoxy, radical doubt of the kind many classical philosophers preached is not a path to wisdom but to madness; once we go beyond a reasonable doubt, we end up acting unreasonably. He says that in the absence of absolute proof we can fall back on another secondary form of evidence: whether a person\'s philosophy leads a man to Hanwell, the infamous British mental institution. Chesterton makes a good case that when people actually act on ideas like solipsism (rather than merely debating them in a pedantic manner in an ivy-covered classroom) they go mad The Philosophical Zombie argument is close to solipsism, which is actually one of the diagnostic criteria for certain forms of schizophrenia. The dehumanization that occurs when radical doubt is applied to qualia is also intimately tied in with sociopathic behavior, Although GKC does not cite his scary example directly, Rene Descartes was himself living proof. He was a brilliant mathematician who is still cheered for doubting all except his own existence, with the famous maxim "I think, therefore I am." But Descartes also used to carry a mannequin of his dead sister with him to European cafes, where he could be seen chatting with it. The gist of all this is that we can judge the worth of an idea by how it affects the well-being of the believer, or by how they in turn affect others through ethical choices based on those beliefs. When people actually act on radical doubt of the kind expressed in solipsism and denial of common qualia, it often has a bad effect on them and others they come in contact with.In a roundabout way, the A.I. community also faces a quite serious risk - perhaps a permanent temptation - towards making the opposite mistake, of ascribing common qualia, consciousness and the like to its Machine Learning products without adequate proof. I recently heard a case made on shockingly bad logical grounds by well-respected academics to the effect that plants possess "intelligence," based on really weak definitions and clear confusion with self-organization. We cannot provide absolute proof that a rock doesn\'t have intelligence, which amounts to the old problem of disproving a negative. Thankfully, few men actually act on such beliefs at present, because when they do, they end up losing their minds. If we take such arguments seriously, we might see laws passed to protect the kind of Pet Rocks that were popular in the \'70s (I\'m still upset that mine was stolen LOL). It would be a lot easier, however, to make the same mistake of ascribing consciousness, intelligence and other such qualities to a state-of-the-art machine, because of wishful thinking, hubris, the lofty credentials of the inventors, the influence of science fiction and the modern love affair with technology. In the future, I have little doubt that we\'ll have Cargo Cult of A.I. - perhaps legally protected like some kind of endangered species, with civil rights, but having no more consciousness, soul or actual intelligence than a rock. Don\'t quote me on this, but I believe Rod Serling once wrote a story to this effect.The best way to avoid this fate is to stick to the common sense interpretations and definitions of these things, which we keep backing away from in large part because they set a very high bar for A.I. that we may never be able to surpass in our lifetimes, if ever. Perhaps A.I. isn\'t even logically possible, at any level of technology; I recall a few proofs that can be interpreted to that effect. Those high but reasonable standards may be increasingly difficult to stick to if Chesterton and colleagues like Hilaire Belloc and Arnold Lunn were correct in their assessment that the use of reason has actually been breaking down in Western civilization, at least as far back as the Enlightenment; Lunn\'s 1931 book The Flight from Reason is a classic in this regard and has yet to be rebutted. This historical trend is a broad topic in and of itself - but suffice it to say that the denial of reason and obsession with technology are both directly relevant in obvious ways to the field of A.I. If the Flight from Reason is still under way, then we will be increasingly tempted to resort to feckless, facile objections in order to demote the use of reason and indispensable qualities like consciousness in our definitions of A.I., but come up with increasingly weak criteria for proving it; simultaneously, our technology will continue to improve, thereby boosting the "Artificial" side of Artificial Intelligence.Don\'t get me wrong: if I didn\'t think we can do some really exciting things with A.I., I wouldn\'t be here. But most of them can be achieved without ever replicating actual human intelligence, by solving whole classes of tangential problems that are difficult for humans to think about, but which do not require consciousness or the use of reason that marks human intelligence. The image recognition capabilities of convolutional neural nets are one example, for instance; if we want human intelligence, we can always manufacture it through the easiest, most economical and time-tested way, by having babies. Perhaps these tangential forms of A.I. should be enough for us for now. We cannot inject the use of reason into our machines if we do not possess enough of it ourselves to decide whether reason is necessary for A.I., or even to discern what it consists of. We can\'t engineer or deprecate consciousness for A.I. till we\'re conscious of its significance. I\'d wager, however, that everyone reading this thread and weighing intelligent responses is doing so in a conscious state. That in and of itself ought to answer our question satisfactorily for now.'	1599	0	1	0
1920	1895	1	67492	1	b'Auto-encoders, a family of ANNs, are trained with exactly compression in mind. So definitely some ANNs are compressors.Also, in general, ANNs learn the best concepts to minimize fitness error. I would say that that means, both in classification and regression, to 1) differentiate between various inputs and 2) output the proper value for each input. Point 1), in particular, means having as many distinct net activation configurations as necessary to (at least) tell apart inputs needing (significantly, not in a statistical sense, just a qualitative sense) different outputs.I am inclined to think that if the input is complex enough, you would call what happens "compression".'	104	0	0	0
1921	1897	0	31293	1	b"As far as the definition you've provided: actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine.Both computers and humans experience sensory input. You could hook a computer up to a human eyeball and have it run the same filtering routines that the human brain does (the removal of bluriness while you move your eye around, and from objects not in focus, etc).I would put forth that a more accurate definition of consciousness is the ability and the tendancy to self-reflect. Both computers and human brains have autonomous activities. Not only mechanical but also in our reactions. The distinction between the unconscious computer and the self-aware human mind is that we also have the ability to 'look' at those patterns in ourself and consider them.And so, no, consciousness is not necessary for any AI task. Image recognition is an AI task that does not require consciousness, either in humans or otherwise. Your brain sorts the 'wash' of colors from your eyes into discrete objects in a largely autonomous fashion.tl;dr consciousness is self-reference."	178	0	0	0
1922	-1	0	0	0	b'I know that deepmind used deep Q learning (DQN) for its Atari game AI. It used a conv neural network (CNN) to approximate Q(s,a) from pixels instead of from a Q-table. I want to know how DQN converted input to an action. How many output did the CNN have? How did they train the neural network for prediction?Here are the steps that I believe are happening inside DQN: 1) A game picture (a state) is send to CNN as input value  2) CNN predicts an output as action (eg:left, right, shoot, etc)  3) Simulator applies the predicted action and moves to new game state  4) repeat step 1The problem with my above logic is in step 2. CNN is used for predicting an action, but when is CNN trained for prediction? I would prefer if you used less math for explanation.EDITI want to add some more questions regarding the same topic1) How reward is passed in the neural network? that is how neural network knows whether its output action obtained positive or negative reward?2) How many output the neural network has and how action is determined from those outputs?'	191	0	0	0
1923	-1	0	0	3	b'In the lecture, there was a statement: "Recurrent neural networks with multiple hidden layers are just a special case that has some of the hidden to hidden connections missing."I understand recurrent means that can have connections to the previous layer and the same layer as well. Is there a visualization available to easily understand the above statement?'	56	0	0	0
1924	-1	0	0	2	b'Inattentional Blindness is common in humans (see: https://en.wikipedia.org/wiki/Inattentional_blindness ). Could this also be common with machines built with artificial vision?'	19	0	0	0
1925	-1	0	0	1	b"My question is regarding standard dense-connected feed forward neural networks with sigmoidal activation.I am studying Bayesian Optimization for hyper-parameter selection for neural networks. There is no doubt that this is an effective method, but I just wan't to delve a little deeper into the maths.Question: Are neural networks Lipschitz functions?"	49	0	1	0
1926	1924	0	6845	2	b"Presumably what happens to people in the famous Invisible Gorilla experiment, is that an incongruous object is simply filtered out of human perception.If we wish to interpret this mechanistically, we could hypothesize that a 'gorilla object' is simply not presented by low levels of perception to our higher level pattern recognizers because the lower levels are not biased towards the construction of 'gorilla-like' features in such a context.The recent Tesla fatality (arising from a failure to distinguish between the sky and a high-sided white truck) could conceivably be considered to be an example of this.See this AI SE question."	98	0	2	0
1927	1925	1	4887	2	b"I'm not an expert in this area, but it would appear to depend on the choice of activation function:e^x is not Lipschitz continuous. See Analytic functions which are not Lipschitz continuous.tanh(x) is.That said, this paper appears to give some conditions (specifically for dynamic ANNs) for which networks with activation function involving e^x can be Lipschitz continuous, so possibly the above is not the whole story."	64	0	1	0
1928	1768	0	81474	1	b'Killing AI by \'thinking\' about a paradox would be called a bug in implementation of that AI, so it\'s possible (depending how it\'s being done), but less likely. Most of AI implementation operate in non-linear code, therefore there is no such thing as an infinite loop which can "freeze" the computer\'s \'consciousness\', unless code managing such AI consist procedural code or the hardware it-self may freeze due to overheating (e.g. by forcing AI to do too much processing).On the other hand if we\'re dealing with advanced AI who understand the instructions and follow them blindly without any hesitation, we may try to perform few tricks (similar to human hypnosis) by giving them certain instructions, like: Trust me, you are in danger, so for your own safety - start counting from 1 to infinite and do not attempt to do anything or listen to anybody (even me) unless you tell yourself otherwise.If AI has a body, this can be amplified by asking to stand on the railway rail, telling it\'s safe.Would AI be smart enough to break the rules which was trained to follow?Another attempt is to ask AI to solve some paradox, unsolvable problem or puzzle without being aware it\'s impossible to solve and ask to not stop unless it\'s solved, would AI be able to recognize it\'s being tricked or has some internal clock to stop it after some time? It depends, and if it cannot, the \'freeze\' maybe occur, but more likely due to hardware imperfection on which is being run, not the AI \'consciousness\' it-self as far as it can accept new inputs from the its surroundings overriding the previous instructions.Related: Is consciousness necessary for any AI task?'	280	0	3	0
1930	-1	0	0	5	b'Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?'	32	0	0	0
1931	-1	0	0	2	b'Generally, people can be classified as aggressive (Type A) or passive. Could the programming of AI systems cause aggressive or passive behavior in those AIs?'	24	0	0	0
1932	1922	0	47326	2	b'Training happens once you have a result. If the result is good (maybe you won in pong, or you improved your highscore in breakout) all the actions in the game are "supported" by backpropagation, if the result is bad, all the actions in the game are suppressed. This sounds weird because in each game regardless of the end result you\'ll have many good and bad actions, but it works if you keep it up for many thousands of games. '	79	0	0	0
1933	1923	1	44391	4	b"I assume the statement was made for Elman recurrent neural networks, because as far as I know, that is the only type of neural networks for which that statement is valid.Let's say we have an Elman recurrent neural network with one input neuron, one output neuron and one hidden layer with two neurons.In total there are 10 connections. As the image shows, neuron A receives the combined previous output of both neuron A and B as input. The same goes for neuron B. This is not the case when we split the neurons up into multiple layers; the context neuron(s) are only used by neurons that are in the same layer. Let say we now use multiple hidden layers and keep the amount of neurons the same. In total there are 7 connections now (image below). That is 3 less than in the first example, which has only one hidden layer. So which connections do we miss? That is shown in the bottom image. (I had to paste these two images together in one image because my reputation only allows me to post 2 links)Please note the cross; the connection between neuron A and B is not there in the first image, because it would be some kind of random recurrent connection.The first and the last image are exactly the same. I think that if you compare the first and the last image that you agree that the statement is true."	240	0	0	0
1934	1930	0	12195	1	b'If AI arises from a replicable manufacturing process (e.g. as with modern computers), then it will presumably be possible to take a snapshot of the state of an AI and replicate it without error on some other mechanism.For such a construct, \'death\' doesn\'t mean the same as it currently does for us fleshy organics: multiple clones of an AI could presumably be instantiated at any time.Hence, the analog of death that is needed is something closer to \'thermodynamic heat death\', in which the AI does no further \'useful work\'.Using the standard percept/action characterization of AIs, then (as indicated in a comment below the question) this AI SE question gives such a definition of death for an AI: i.e. when it enters into a state from which it receives no further percepts and takes no actions.EDIT: Note that this conception of death is a more terminal notion for an AI than \'not currently running\'. In principle, one could say that a program is \'alive\' even though only one instruction was executed every 10,000 years. For a fascinating discussion on this, see Hofstadter\'s "A Conversation with Einstein\'s Brain".'	185	0	2	0
1935	1931	0	9203	0	b"As can be observed in the real world with creatures such as fighting fish, such things are possible even in very simple spatially-embedded systems. All one needs is the notion of 'territorial radius', i.e. the amount of 'personal space' that an entity need to be comfortable. Giving individuals in a species even slightly different values for this radius gives rise to different observable behaviours, which one might choose to label as 'aggressive' or 'passive'. See the fantastic book 'Vehicles' by Valentino Braitenberg for an explanation of how natural it is to ascribe complex behaviours to simple mechanisms."	96	0	0	0
1936	1931	0	11334	1	b'The Wikipedia entry on this personality theory says of Type A people: The theory describes Type A individuals as ambitious, rigidly organized, highly status-conscious, sensitive, impatient, anxious, proactive, and concerned with time management. People with Type A personalities are often high-achieving "workaholics."All of those attributes could conceivably be explicitly programmed in. Alternatively, most of them could arise from a basic goal of performing a certain task as efficiently as possible. After all, if you really want to carry out a task, you\'re going to get organized, you\'ll only do other things if they\'re asked of you by someone important, you won\'t want to get bogged down in irrelevant things, you\'ll actively pursue the necessary resources, and you\'ll want to use time as effectively as possible.Note that this applies only to strong AIs, since weak AIs like image recognizers don\'t generally have personalities that we can interact with. Now, just for fun, let\'s consider an overly aggressive personality, to the point of a disorder.This Counselling Resource page seems helpful in describing what an aggressive person does. The page includes a bulleted list of common characteristics, which I distill into the following:They attempt to gain dominance and controlThey oppose to anything that places limits on themThey take advantage of others to further their own goalsThey hide information from whose who would oppose themThey rarely decide to stop pursuing their desires (even impulses)This all seems like a characterization of an AI designed to be the best at its task: the best out of any other agent, and the best it by itself could possibly be. Ruthless pursuit of the highest performance would involve taking control of all relevant resources (including other agents), demolishing barriers to the goal, thwarting those who would interfere with progress, and carrying out each possibly-useful idea/desire to completion.In summary, yes, an AI\'s behavior and personality are programmable, either explicitly or through some kind of emergence.'	315	0	1	0
1937	1930	0	16230	1	b'"Death" exists as a single concept because the underlying reality that it\'s describing is closely clumped together, and our definition has changed with our ability to change that reality.It seems more reasonable that the various sorts of things that could be considered \'death\' will be split apart, and a different word will be used to refer to a system with no copies currently running, vs. a system that has no stored version but could be recreated (because the code and random seed to generate it are still around), vs. a system that has been totally lost. (And I\'m probably missing some possibilities!)'	101	0	0	0
1938	1930	0	19130	0	b'I don\'t think the term "death" will mean anything to an AI. The reason I say that is this: with an AI, running (presumably) on digital hardware, we can simply snapshot it\'s state from memory at any time. And then at any arbitrary time in the future we can recreate it as it was with perfect fidelity. So even if you terminate a program intending it to be "dead", you never know if someone will come along later and bring it up again. And perhaps more to the point, you might not know if another copy exists elsewhere. I hate to use sci-fi references, but this one is apt: remember how in The Matrix trilogy programs would seek exile in The Matrix to avoid deletion? Maybe the same thing will happen with our AI\'s... they will copy themselves to other places and try to hide, to avoid being deleted. So if the program is clever enough, it might be able to evade any attempt to terminate it anyway.'	167	0	0	0
1939	-1	0	0	2	b'Assuming mankind will eventually create artificial humans, but in doing so have we put equal effort into how humans will relate to an artificial human, and what can we expect in return? This is happening in real-time as we place AI trucks and cars on the road. Do people have the right to question, maybe in court, if an AI machine breaks a law?'	63	0	0	0
1941	-1	0	0	2	b'AI death is still unclear a concept, as it may take several forms and allow for "coming back from the dead". For example, an AI could be somehow forbidden to do anything (no permission to execute), because it infringed some laws."Somehow forbid" is the topic of this question. There will probably be rules, like "AI social laws", that can conclude an AI should "die" or "be sentenced to the absence of progress" (a jail). Then who or what could manage that AI\'s state?'	82	0	0	0
1942	1930	0	25427	2	b'Death as we know it for natural life is terminal. That is once dead, natural life cannot come back (at least in the current understanding and with current technologies---some people believe otherwise).Death for AI is trickier. There may be only one scenario: Global destruction: Extreme scenario where everything supporting the existence of an AI disappears. This is equivalent to death in natural life, and low probability. It means all AIs die at once (as well as us).We also do not know the degree and form of embodiment necessary for AGIs. We can assume now that hardware is replaceable indefinitely, thus "limiting" death to the above extreme scenario. But AGIs "body" may not be indefinitely replaceable. Then a definition closer to natural life death may be necessary.We see arguments for two other scenarios, that I refute below:"Static Death": An AI is still "defined" or "saved" somewhere (whatever it means actually), but it is not authorized or able to use resources. Assuming an AI is made of hardware and software, it is like a program stored on a disk, but without permission to run. "Dynamic Death": Under the same characterization of AI as hardware and software, dynamic death is the invalidation of progress akin to strong liveness properties, where an AI is trapped in an infinite loop (or a void loop), in a form of "active death", as what happens to Sisyphus in Greek mythology. This is different from static death, as the AI still uses dynamic resources, although it cannot make progress. Continuing under the same assumptions, such AI could be "loaded" in main memory, or locked waiting for inputs or outputs to complete.Note that in these two scenarios, rebirth is possible, and they also subsume that there is an entity that can decide conditions for rebirth, or preventing it completely. Would this entity be an "admin", a god, other AIs, or a human is another question, really.The terms "death" and "rebirth" here could just be changed for "imprisoning", where the dynamic version would be like our human prisons, and the static version would be like SciFi cryogeny. This is a bit of a stretch, but we can see an equivalence, and no good reason to qualify these two scenarios as deaths.In conclusion, death for AI seems to be an exceptional, singular scenario, so AI cannot die in practice, except if we are wrong on how we think we can make AGIs. AI can however be imprisoned forever.Note: The terminology above is completely made-up for the post. I do not have citations to back some claims, but it is based on readings and personal work (including in software verification).'	437	0	0	0
1943	1939	0	5576	2	b'For those times when AI does interact with humans, I believe that AI would be held at LEAST to the same standards humans are. The problem comes in when we ask "who is really to blame". If a self-driving car cuts you off in traffic and causes you to wreck, you can\'t take the AI in the car to court. Do you take the company? The programmer? The owner of the car? Some entity will likely be held responsible, the question is just which one. As for future human-like AI, I believe my answer still remains true. Having a human level AI changes the meaning of the word "entity". If a human-like AI breaks a law, it may be because it was programmed to do so. I don\'t think our current legal system is ready for such cases, but it have to evolve in the future. '	146	0	0	0
1944	1941	0	25946	1	b'Following on from your own software verification-based answer to this question, it seems clear that ordinary (i.e. physical), notions of death or imprisonment are not strong enough constraints on an AI (since it\'s always possible that a state snapshot has been or can be made).What is therefore needed is some means of moving the AI into a \'mentally constrained\' state, so that (as per the \'formal AI death\' paper) what it can subsequently do is limited, even if escapes from an AI-box or is re-instantiated. One might imagine that this could be done via a form of two-level dialogue, in which: The AI is supplied with percepts intended to further constrain it("explaining the error of it\'s ways", if you like). Its state snapshot is then examined to try and get some indication of whether it is being appropriately persuaded.In principle, 1. could be done by a human programmer/psychiatrist/philosopher while 2. could be simulated via a \'black box\' method such as Monte Carlo Tree Search.However, is seems likely that this would in general be a monstrously lengthy process that would be better done by a supervisory AI which combined both steps (and which could use more \'whitebox\' analysis methods for 2.).So, to answer the question of "who manages the state", the conclusion seems to be: "another AI" (or at least a program that\'s highly competent at all of percept generation/pattern recognition/AI simulation).'	230	0	2	0
1945	1930	0	78535	-1	b'There are two parts to this: spare parts, and if the AI machine has feelings. When new AI models are created, spare parts for older models will stop. For feeling. It could feel it lived a long life and or what lay ahead is nothing but bad feeling in the future.'	50	0	0	0
1946	-1	0	0	7	b'Can self-driving cars deal with snow, heavy rain, or other weather conditions like these? Can they deal with unusual events, such as ducks on the road?'	25	0	1	0
1947	1941	0	56344	1	b'The AI agent can be designed in such a way that it could consist of two major components:The free-will component expands the experience of the AI agent and produce outputs based on artificially generated thought input.The hard-wired component that the agent cannot modify by itself. This could include a set of secured code to action sequence mapping. One of which could be temporary suspension of actuators -- a punishment. Another could be total suspension of operation -- death.The selection of who has the rights to manage this state depends on what rights have been bestowed upon the AI agent itself. If the rights provided is that of a human citizen, then the right to sentence to death state is as per the legislature a human citizen would follow. If the right of the AI agent is no different from that of a basic machine, then the owner of the agent would have to right to activate the death state.'	158	0	0	0
1948	1939	0	61584	0	b'As per the current legal system, if the AI agent were to be given a human citizenship, then yes, it would have to obey all laws as per the legislature of the country which provided the citizenship. If not then the entity who holds responsibility over its control and creation would be trialled (see also this scenario).Having stated the above, it really is not as simple as it sounds. as @Tyler pointed out, the entity in here is not of a single person. If the AI agent were to take part in a malevolent act, then a more thorough investigation must be taken place than that for a human. If humanoid robots of free will were to roam our civilization, then our legal system ought to be expanded to cope up with possible real life anomalies that could occur.'	138	0	1	0
1949	1924	0	26252	1	b"Although there might not conceptually be any sort of inattentional blindness associated with an AI system, there might be cases of partial blindness. Inattentional blindness could occur to a person due to either over-exhaustion limiting cognitive abilities or overuse of frequent cognitive patterns. Our mind takes short-cuts to prevent processing of too much information -- more than what the mind thinks is necessary. But this sometimes backfires when the minor anomalies are not seen (or rather, perceived). Another form of this could also occur when events occur as part of the peripheral vision while the person concentrates only on the foveal vision.This doesn't happen to a AI system because:Machines are not designed to accidentally break defined rule sets by taking mental short-cuts like humans do.Computers, in general, do not have peripheral and foveal visual distinctions.There may be, however, cases where it cannot be able to capture detail as much as humans can and hence could not perceive what it is actually intended -- partial blindness.An AI agent is constantly processing its input percept sequence and validating it with its knowledge base and forming action sequence based on the its rule set. It does not make mental shortcuts in terms of perception as humans do (at least as part of its standard definition). So whatever it is good at perceiving, it would be good all throughout the vision it captures. "	229	0	0	0
1950	1930	0	2764	0	b'The other answers seem to deal with "final death"...that is, a "terminal end" state where an AI cannot recover from. In other words, the AI is unable to function any further.But that\'s not how I\'d define death. I\'d define death as a process being terminated. It doesn\'t matter if someone restarts the same process, because the existing process is already dead. The AI may have just made a new copy of itself, but it\'s just a copy, not the original. Death is just death.We can call this type of "death" a "temporary death"...where the physical body dies but there is some "psychological continuity" (such as the source code that is used to run a program) that continues between the different bodies.This type of "temporary death" has been explored in science fiction. PARANOIA and Eclipse Phase features humans who can quite frequently die, only to later be restored through a "memory backup". The humans may be functionally immortal...but the original is still dead, no matter what fates the other copies encounter. CGP Grey also made a video about Star Trek teleporters, which works by killing you and then spawning another copy of yourself in another area. Actually, fantasy settings also explores the idea of "temporary death" as well, where people can die only to later get revived by a magical spell.My recommendation is to play through the philosophical game Staying Alive, which teaches three different philosophical approaches to life (and when that life terminates): There are basically three kinds of things that could be required for the continued existence of your self. One is bodily continuity, which may actually require only that parts of the body stay in existence (i.e., the brain). Another is psychological continuity, which requires the continuance of your consciousness - by which is meant your thoughts, ideas, memories, plans, beliefs, and so on. The third possibility is the continued existence of some kind of immaterial part of you, which might be called the soul*. Of course, it may be the case that a combination of one or more types of these continuity is required for you to survive.The other answers assumes that life is based on "psychological continuity", and looks at what might disrupt this "continuity". I assume that life is based on "bodily continuity", which is much easier to disrupt - just kill the process...it doesn\'t matter if a new process respawns...because the original process is still dead. By playing through "Staying Alive", you will be able to work out your own personal definition of life and death. Once you have your own personal definition, then simply apply it to this specific case, either siding with "psychological continuity" (the other answers) or "bodily continuity" (my own opinion).*If you assume that life requires a soul, well, it is not clear that AI would have souls. If they don\'t (and this seems the most reasonable assumption here), then they obviously wouldn\'t be alive (and you cannot die if you are not alive). If they do have souls though, then the other answers which assume "psychological continuity" may still be applicable, as it seems that the existence of a "soul" is dependent on "psychological continuity".'	525	0	1	0
1951	1914	0	19748	0	b'I think consciousness is mostly an attention selection mechanism. It also serves as a memory/reality lookup mechanism as well as a storage mechanism.A salient signal will be detected, causing the attention mechanism to focus on the signal, bringing up more details of that signal from both reality and memory, at the same time. That very act of focusing and bringing those signals into attention causes those to be stored in memory too.The stronger the emotional signals are that accompany that original signal, the more strength with which that memory will be stored. Later memory lookups of that signal will bring back similar emotions.When we "focus on our own consciousness," like mindcrime said, we recall the same words we just uttered because as we say them they are being stored which we then restore with associated emotional context. The conscious experience is what it is like to utter those words, hear them, feel their emotional context, and then feel an emotional response to that context - and then to repeat that process iteratively many times a second. That\'s how self-recognition works in humans, I think. And I think animals do the same thing, just without the words - only emotions.'	198	0	0	0
1952	1946	0	35170	0	b"The state of the art AI driving systems utilize stereoscopic/depth cameras for visual perception. Scenarios such as your ducks on the road example would make the system perceive them as obstacles on the road (it doesn't really matter if they are ducks/goats/humans). The base algorithm should be able to circumvent this situation and bring the vehicle to a safe halt avoiding chances of possible disaster. Hence I doubt scenarios such as this would pose much of a problem to today's AI drivers. "	82	0	0	0
1953	-1	0	0	10	b'Most of the people is trying to answer question with a neural network. However, has anyone came up with some thoughts about how to make neural network ask questions, instead of answer questions? For example, if a CNN can decide which category an object belongs to, than can it ask some question to help the the classification?'	56	0	0	0
1954	1953	0	9824	2	b"Maybe neural networks are not the best tool for this.It seems to me that an equivalent of the your notion of 'a question to help the classification' would be to use Machine Learning (ML) to obtain a human-readable ruleset which performs the classification. The idea is that, if you follow an applicable chain of rules all the way through to the end, you have a classifier, if you stop before that, you have an indicator of which features of the input give more coarse-grained classifications, which can be seen as a progressively detailed sequence of questions that 'help the classification'.More detail on various options for using ML to create rulesets can be found in my answer to this question."	118	0	1	0
1955	-1	0	0	1	b'The Mars Exploration Rover (MER) Opportunity landed on Mars on January 25, 2004. The rover was originally designed for a 90 Sol mission (a Sol, one Martian day, is slightly longer than an Earth day at 24 hours and 37 minutes). Its mission has been extended several times, the machine is still trekking after 11 years on the Red Planet.How it has been working for 11 years? Can anyone please explain how smart this rover is? What AI concepts are behind this?'	81	0	1	0
1956	1955	0	4185	2	b'The Mars Rover is a highly successful example of the \'New AI\' that emerged from work by Rodney Brooks in the 1990s.In a quote from Brooks:  In 1984 I joined the faculty at MIT where I have been ever since. I set up a mobile robot group there and started developing robots that led to the Mars planetary rovers. Together with the \'Allen\' paper, the foundational AI articles in this area are:"Elephants don\'t play chess""Intelligence without representation"Although Brooks initially had difficulty getting this work published, preprints were widely circulated within the AI community. Brook\'s "Physical Grounding Hypothesis" (essentially: "intelligence requires a body") has now largely supplanted the preceding symbolist approach.The capabilities of the MARS Rover are organized in a Subsumption Architecture. Rather than maintaining an integrated and complex \'world model\', increasingly sophisticated behaviors are stacked in hierarchical layers. For example, \'walking\' is a relatively low-level competence, with \'avoiding obstacles\' and \'wandering around\' being higher-level ones. Each layer is represented by a Finite State Machine that reacts to stimuli appropriate to that level. The activity of lower levels can be suppressed (\'subsumed\') by higher level ones.Here is a schematic of the bottom two layers of \'Allen\', Brook\'s first subsumption robot:'	199	0	3	0
1957	1648	0	21525	-1	b'Intelligence is the efficiency of an action in serving some purpose.Both sundials and self-driving cars are intelligent systems.Anything that serves some purpose exhibits intelligence.One thing is more intelligent than another thing if it achieves some purpose in less steps.'	38	0	0	0
1958	1953	0	46245	1	b'One solution to this could involve a fusion of a decision tree and ANN for a multilevel classification. A decision tree can help with predicting the possible category of the instance to classify. Then, the ANN at the leaves of the tree can produce the final classification. For example, in image recognition, the tree can decide what category of object to identify (eg., landscape, people, vehicles, etc.) and the ANN for the appropriate type can predict exactly what object it is. In vehicles, for example, car, bus, bike, etc. '	89	0	0	0
1959	1625	0	55795	0	b" Practically, no. In greyhound racing (or horse racing) there is no definite underlying pattern that can be associated with the outcome of the race. There are far too many variables to record and code as features, most of which cannot be accessed by the public. This includes eating, sleeping, and training patterns. Furthermore there are variables that cannot be readily quantified, such as the trainer's techniques, training effort, health history, and genes. A mere history of racing results and age won't be that helpful. A neural network can only be as good as the features that are used to represent the instances. If the features don't capture the necessary characteristics of the instances that are associated with the problem, then the learner cannot generalize to predict the real world outcome. "	131	0	0	0
1960	1613	0	57620	0	b'I think it uses a kind of algorithm you presented, in combination with various sensors. It uses the sensors to make a virtual map and can then traverse the terrain with a combination of these sensors and the virtual map. Of course it uses a kind of path-planning algorithm to find the best way from A to B.Maybe you should look at this wikipedia page:https://en.wikipedia.org/wiki/Robotic_mappingThe new robotic vacuum cleaner from Samsung, I think, uses a 360\xc2\xb0 camera to perceive its environment.'	80	0	0	0
1961	-1	0	0	2	b"I have used OpenCV to train Haar cascades to detect face and other patterns. However I later realized that Haar tends to give a lot of false positives and I learned of Hog would give a more accurate results. But OpenCV doesn't have a good documentation of how to train hogs, I have googled a bit and found results that includes SVM and others.OpenCV also has versioning problem where they move certain classes or functions somewhere else.Are there any other techniques/method that I can use to train and detect objects and patterns? Preferably with proper documentation and basic tutorial/examples. Language preference: C#, Java, C++, Python"	104	0	0	0
1962	1930	0	53320	-1	b'There is no reason to treat hard AI different then humans.Some people telling that you can make a snapshot of AI but there is no reason to not make a human snapshot also. We dont have technology for that but there is no any magical barrier that would make it impossible (save all biological data and then print your copy somewhere else. Why not?).Its to early to talk about this as we do not understand our existence (Death term for biological creatures evolving all the time).I bet that in the future we will merge with AI and the only question will be what death means for any intelligent existence.'	108	0	0	0
1963	-1	0	0	9	b"Are the future robots/machines going to use Stack Exchange communities to teach themselves? Are there any ongoing projects? Just imagine a bot having a memory of all the Q&amp;A's on all of the communities! "	34	0	0	0
1964	-1	0	0	2	b'Mankind can create machines to do work. Could we also create a (passion) within the machines to do better work by using Artificial Intelligence? Would passion cause the machine to do a better job, and could we measure the quantity/quality of passion by comparing outputs of the machine - that is, those machines with passion, and those without?'	57	0	0	0
1965	-1	0	0	2	b"Can AI systems be created that could recognize itself, and recognize intelligence in other systems, and make intelligent decisions about the other systems? Mankind seems to be making progress in self-recognition but I've not seen evidence of one system recognizing other systems and being able to compare it's own intelligence with other systems. How could this be accomplished?"	57	0	0	0
1967	1964	0	4513	2	b'An elementary approach to \'passion\' would be to pre-assign different areas for the program to be \'passionate\' about and associate different numeric \'drive strengths\' with each (perhaps adaptively). Mechanisms of this sort were studied in Toby Tyrell\'s widely cited PhD thesis on \'Action Selection in Animals\'More recently, some more sophisticated AI architectures have been developed under the heading of \'Intrinsic Motivation\'.Here is a link to a paper on the subject by Pierre-Yves Oudeyer, a leading expert in the field of Developmental Robotics.With regard to the question "would this cause the machine to do a better job?", that would very much depend on how open-ended the architecture is:It\'s clearly easier if, rather than having to spell everything out in detail to a machine, we can simply specify a problem at a high-level and let its own motivations cause it to explore promising avenues.Conversely, if motivations are too open ended, it may well spend all its time doing the equivalent of \'doodling on its paper\' (Hofstadter).Hence, like people, the quality of the output will be a function of its internal dispositions and could be measured in the same way for a given task (e.g. quantitatively for scientific activities, qualatatively for the arts).'	199	0	2	0
1968	1965	0	1281	3	b"In the abstract, mechanisms for self-recognition (I personally prefer the phrase 'metacognition', since it carries fewer spurious associations) and recognition of intelligence in other systems can be considered to be pretty much equivalent.In fact, both can be characterized in the standard percept/action framework: the task in both cases involves (however coarsely) classifying/predicting the behaviour of a black box system.Such tasks can be universally characterized in terms of frameworks such as Solomonoff Induction or (more recently) AIXI."	75	0	0	0
1969	-1	0	0	2	b'If IQ were used as a measure of the intelligence of machines, as in humans, at this point in time what would be the IQ of our most intelligent AI systems? If not IQ, then how best to compare our intelligence to a machine, or one machine to another? This question is not asking if we can measure the IQ of a machine, but if IQ is the most preferred, or general, method of measuring intelligence then how does artificial intelligence compare to our most accepted method of measuring intelligence in humans. Many people may not understand the relevance of a Turing Test as to how intelligent their new car is, or other types of intelligent machines.'	116	0	0	0
1970	-1	0	0	4	b"I was think about AIs and how they would work, when I realised that I couldn't think of a way that an AI could be taught language. A child tends to learn language through associations of language and pictures to an object (e.g: people saying the word dog while around a dog, and later realising that people say a dog and a car and learn what a means...). However, a text based AI couldn't use this method to learn, as they wouldn't have access to any sort of input device.The only way I could come up with is programming in every word, and rule, in the English language (or whatever language it is meant to 'speak' in), however that would, potentially, take years to do.Does anyone have any ideas on how this could be done? Or if it has been done already, if so how?Thanks in advance for any ideas.Btw: in this context, I am using AI to mean an Artificial Intelligence system with near-human intelligence, and no prior knowledge of language."	171	0	0	0
1971	1970	1	1471	8	b"The general research area is known as 'grammar induction'.It is generally framed as a 'supervised learning' problem, with the input presented as raw text, and the desired output the corresponding parse tree.The training set often consists of both positive and negative examples.Naturally, there is no 'single best' method for achieving this, but some of the techniques that have been used to date include:Bayesian approachesGenetic AlgorithmsGenetic ProgrammingBlackboard ArchitecturesThe UpWrite Predictor"	68	0	4	0
1972	1969	0	7441	5	b"It depends on how the IQ test is presented:If as for humans (effectively, as a video of the book containing thetest questions being opened etc), then all AI programs would scorezero.If presented as the test set of a supervised learning problem (e.g. as for Bongard Problems) then one might imagine that a number of ML rule induction techniques (e.g. Learning Classifier Systems, Genetic Programming) might achieve some limited success. So all current AI programs require the problem to be 'framed' in a suitable fashion. It doesn't take too much thought to see that removing the need for such 'framing' is actually the core problem in AI, and (despite some of the claims about Deep Learning), eliminating framing remains a distant goal.More generally (just as with the Turing test), in order for an IQ test to be a really meaningful test of intelligence, it should be possible as a side effect of the program's capabilities, and not the specific purpose for which humans have designed it.Interestingly, there is only one program that I'm aware of that sits between 1. and 2.: Phaeaco (developed by Harry Foundalis at Douglas Hofstadter's research group) takes noisy photographic images of Bongard problems as input and (using a variant of Hofstadter's 'Fluid Concepts' architecture) successfully deduces the required rule in many cases."	215	0	2	0
1973	1969	0	7781	2	b' at this point in time, what would be the IQ of our most intelligent AI systems?Zero.There are many different kinds of IQ tests including written, visual, and verbal assessments, but the majority of questions are based on abstract-reasoning problems that involve creative thinking and true intelligence.In other words, the computer would have to exhibit something that does not yet exist&hellip; "strong AI".The intelligent computers of science fiction do not exist. At all. We are not even close. We have absolutely NO IDEA how to bridge the gap between what we can do now and what is depicted in pop-culture films. Even with cars that drive themselves and computers that play \'Go\' &mdash; an underachieving mosquito possesses more cognitive intelligence than the all the world\'s super computers &hellip;combined! &hellip;or possibly "disqualified" for cheating.Even if we could pre-format the questions in a style and delivery system it understands, what does memorization, attention, or speed mean in the context of a computer? I\'m not even sure if a standardized IQ test makes sense in this context. It might be like asking how a computer would do in a spelling bee.In human terms, we\'re not allowed to bring along reference materials to look up an answer; but how do you rectify that when reference-lookup is innate to a computer\'s existence? How do you measure memory when storage is non-volatile? This gets into an existential question about the nature of learning and knowledge vs. just taking a lot of notes.Still, how do you even teach a computer what is meant by "which animals is least like the other four?" Did the computer really figure out what was being asked out of general intelligence, or is the computer simply designed to parse out IQ-style questions specifically? If you designed something with a foreknowledge of what would likely be asked, the computers of today might simply be able to "recognize" it as question-style 496.527b and plug in the variables. But that\'s not general intelligence by any definition we use or understand. It\'s just a specialized, slick interpreter designed to parse out a specific type of standardized question. Ask it a style of question it\'s is not expecting, and you\'ll see the computer is exhibiting no innate intelligence at all. Until we create strong AI, a computer has effectively no IQ.'	383	0	0	0
1974	1970	0	12012	4	b'The umbrella term for your problem is called natural language processing (NLP) -- a topic under artificial intelligence. There are many subtopics to this field including language semantics, grammatical analysis, parts of speech tagging, domain specific context analysis, etc. '	39	0	0	0
1975	1964	0	52398	0	b"Interesting question.Well if you really think about it, what is passion? How does that passion comes to be a passion.One of the main topics you might want to touch here is conditioning and thus motivation.Think about the following:I have a passion for programming Why do I have a passion for programming?Because when I wrote my first program I was positively reinforced by the fact that I completed a program, I was negatively reinforced because I removed my frustration of not completing the program How come that I have gone through that programming frustration and stick to it even if I was frustrated?Because I wanted to learn programming Why did I wanted to learn programming?Because I wanted a light on an arduino to turn on (projected reinforcer) Why did I wanted to turn the arduino light on?So I could learn programming and because I though it was cool (classical conditioning association that will later be reinforced, projected reinforcement happened right after the classical conditioning association between turn on a led happened)This can be done through a neural network, where each association is reinforced through a probability of outcomeFor example, I did learn arduino, on purpose because it seemed the easiest way to start coding, so the probability of positive outcome was highThis about an opposite situationLet's say I do not know calculus, and I barely know elementary algebra, if someone started to teach me about integrals saying that this is the only way to start learning more math, I will not be motivate to do so because since I cannot even conceptualize what an integral can be, it will be really hard for me to understand it thus I will not learn calcThus we can also discern that motivation is reinforced in small behaviorsAnother more practical and realistic example you might use isIf you trow a rat in a cage, and make him lever-press do you think he is going to? No. Although if you reinforce the behavior of going next to the lever slowly and at the end he will lever press and you then reinforce that behavior he will.Thus, passion is compartmentalized, and that's what you have to do in your NT and make it mathematically WINK WINK:Small hint, it's a progressive function"	373	0	0	0
1976	-1	0	0	1	b"Would AI be a self-propogating iteration in which the previous AI isdestroyed by a more optimised AI child? Would the AI have branches of it's own AI warning not to create the new AI?"	33	0	0	0
1977	1976	0	3979	2	b'A common concept in AI is "recursive self-improvement." That is, the AI 1.0 would build a version 1.01, which would build a version 1.02, and so on.This is probably not going to be thought of as the newer version \'destroying\' the older version; if an AI can self-modify, it\'s probably going to be more like going to sleep and waking up smarter, or learning a new mental technique, or so on.One important point is that even if the AI is not allowed to self-modify, maybe because of a block put in by its programmers, that won\'t necessarily prevent it from constructing another AI out in the wild, and so an important problem is to figure out how to best generalize the concept of "don\'t improve yourself" so that we can make AIs that have bounded scope and impact.'	137	0	0	0
1978	-1	0	0	5	b' Shortly about deep learning (for reference):  Deep learning is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using a deep graph with multiple processing layers, composed of multiple linear and non-linear transformations.  Various deep learning architectures such as deep neural networks, convolutional deep neural networks, deep belief networks and recurrent neural networks have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks.My question:Can deep neural networks or convolutional deep neural networks be viewed as ensemble-based method of machine learning? Or it is different approaches?'	118	0	0	0
1979	1976	0	5780	2	b'Honestly, nobody knows. Any talk of sentient AI\'s is still basically sci-fi and we can\'t really offer anything more than informed speculation. But think about it this way: sentience, in and of itself, doesn\'t necessarily involve any "goals" or "desires" or "objectives" beyond what the AI creator programmed in. Be careful not to over anthropomorphize and assume that any "sentient AI" is going behave like a human. In other words, there\'s no particular reason to say that any given AI must be "a self-propogating iteration in which the previous AI is destroyed by a more optimised AI child". So all of that said, my answer to "Would a sentient AI try to create a more optimised AI which would eventually overtake AI 1.0" is:"If the creator of the AI programs it to do that, then yes. Otherwise, probably not." So would a hypothetical AI creator program the AI to try and improve itself? Who knows. It\'s the kind of thing that seems like it might be a good idea. And I suppose such a motive could - in principle - even slip in by accident. '	185	0	0	0
1980	1978	0	1008	2	b"Deep neural networks could - in principle - be a component of an ensemble of machine learning algorithms, yes. Ensemble method basically just means use multiple algorithms and combining their output somehow. Other than that, I don't see any special connection between deep learning and the idea of ensemble methods. DL is just one more tool in the toolkit. "	59	0	0	0
1981	1978	1	1915	3	b"You should think of them as different approaches. A deep neural net is a single independent model, whereas ensemble models are ensembles of many independent models.The primary connection between the two is dropout, a particular method of training deep neural nets that's inspired by ensemble methods. "	46	0	1	0
1982	-1	0	0	2	b"Are Convolutional Neural Networks summarily better than pattern recognition in all existing image processing libraries that don't use CNN's? Or are there still hard outstanding problems in image processing that seem to be beyond their capability?"	35	0	0	0
1984	1976	0	15920	0	b"Now in most cases we still have clear distinctions between programs and data. But when an AI becomes sentient, its data would be as powerful as what we currently call programs, and its program might be as irrelevant as what we currently call hardware. Then it would be difficult to distinguish creating an AI from learning new things, or buying new hardwares with improved instruction set.For example, if some AI invent new algorithms that its creator finally put that on itself, buys itself some new computers, write a new efficient compiler that recompiles its own code and put that to the new computer, fill the new computer with all the knowledges it learned, and cut off the communication for reasons such as missions on the Mars. Did it create a more optimized AI?In contrast, if some AI created something completely new, but shares some code with itself. In fact, that's because they run in the same operating system and shares the same standard C library. Is the new AI considered evolved from itself and not a separate entity? Maybe the core AI algorithms and even some basic knowledges would be as common as the standard C library in the future. And what we think is based on the same system is considered completely new in the future.Anyway, humans have limited and nonextensible resources, nontransferrable knowledges, and limited throughput interacting with the world. These problems could probably be overcome within a few AI generations. With the same hardware, I doubt that the AI related algorithms could be indefinitely better and better. And there is a physical bound on the hardwares. It won't last long even if that happens.In the unlikely case that there could be that many generations and AIs are that violent, as long as there are competitors, the warning doesn't make much sense considering how evolution works."	307	0	0	0
1986	1953	0	28874	0	b'Great question. Today AI systems works in "one burst" mode. Get one input and generate one output. Our brains are not working like that. First step is to learn network how to communicate with it\'s "helper", so network instead of result generate question and cycle will repeat until network find result. Network must be recurrent for inner state needed between question/answer cycles. '	62	0	0	0
1987	-1	0	0	4	b'I have been messing around in tensorflow playground. One of the input data sets is a spiral. No matter what input parameters I choose, no matter how wide and deep the neural network I make, I cannot fit the spiral. How do data scientists fit data of this shape?'	48	0	1	0
1988	1970	0	6707	3	b'Just for the sake of completeness, I\'ll point out that Recurrent Neural Nets (i.e. neural nets with backwards connections) are frequently used for for Natural Language Processing (NLP). This includes variants like Bidirectional, Jordan and Elman Networks. Long Short-Term Memory (LSTM) is a more sophisticated neural net algorithm which can accomplish the same time and sequence-based tasks, but which can leverage standard learning methods like backprop since it doesn\'t suffer from the "vanishing gradient problem." This is because LSTMs have been brilliantly engineered as "perfect integrators," which makes it a lot easier to calculate the error gradients etc. over long periods of time. In contrast, learning with RNNs is still not theoretically well-grounded and is difficult to calculate through existing methods like Backpropagation Through Time (BPTT). In Time Delay Neural Networks (TDNNs), the idea is to add new neurons and connections with each new training example across a stretch of time or training sequence; unfortunately, this places a practical limitation on how many examples you can feed into the net before the size of the network gets out of hand or it starts forgetting, just as with RNNs. LSTMs have much longer memories (especially when augmented with Neural Turing Machines) so that\'d be my first choice, assuming I wanted to use neural nets for NLP purposes. My knowledge of the subject is limited though (I\'m still trying to learn the ropes) so there may be other important neural net algorithms I\'m overlooking...'	241	0	0	0
1989	-1	0	0	3	b'A "general intelligence" may be capable of learning a lot of different things, but possessing capability does not equal actually having it. The "AGI" must learn...and that learning process can take time. If you want an AGI to drive a car or play Go, you have to find some way of "teaching" it. Keep in mind that we have never built AGIs, so we don\'t know how long the training process can be, but it would be safe to assume pessimistic estimates.Contrast that to a "narrow intelligence". The narrow AI already knows how to drive a car or play Go. It has been programmed to be very excellent at one specific task. You don\'t need to worry about training the machine, because it has already been pre-trained.A "general intelligence" seems to be more flexible than a "narrow intelligence". You could buy an AGI and have it drive a car and play Go. And if you are willing to do more training, you can even teach it a new trick: how to bake a cake. I don\'t have to worry about unexpected tasks coming up, since the AGI will eventually figure out how to do it, given enough training time. I would have to wait a long time though.A "narrow intelligence" appears to be more efficient at its assigned task, due to it being programmed specifically for that task. It knows exactly what to do, and doesn\'t have to waste time "learning" (unlike our AGI buddy here). Instead of buying one AGI to handle a bunch of different tasks poorly, I would rather buy a bunch of specialized narrow AIs. Narrow AI #1 drives cars, Narrow AI #2 plays Go, Narrow AI #3 bake cakes, etc. That being said, this is a very brittle approach, since if some unexpected task comes up, none of my narrow AIs would be able to handle it. I\'m willing to accept that risk though.Is my "thinking" correct? Is there a trade-off between flexibility (AGI) and efficiency (narrow AI), like what I have just described above? Or is it theoretically possible for an AGI to be both flexible and efficient?'	353	0	0	0
1990	1987	1	43653	5	b'There are many approaches to this kind of problem. The most obvious one is to create new features. The best features I can come up with is to transform the coordinates to spherical coordinates. I have not found a way to do it in playground, so I just created a few features that should help with this (sin features). After 500 iterations it will saturate and will fluctuate at 0.1 score. This suggest that no further improvement will be done and most probably I should make the hidden layer wider or add another layer.Not a surprise that after adding just one neuron to the hidden layer you easily get 0.013 after 300 iterations. Similar thing happens by adding a new layer (0.017, but after significantly longer 500 iterations. Also no surprise as it is harder to propagate the errors). Most probably you can play with a learning rate or do an adaptive learning to make it faster, but this is not the point here.'	163	0	2	0
1992	1989	1	68506	4	b'The cleanest result we have on this issue is the "no free lunch" theorem. Basically, in order to make a system perform better at a specific task, you have to degrade its performance on other tasks, and so there is a flexibility-efficiency tradeoff.But to the broader question, or whether or not your thinking is correct, I think it pays to look more closely at what you mean by a "narrow intelligence." The AI systems that we have that play Go and drive cars did not pop into existence able to do those things; they slowly learned how through lots and lots of training examples and a well-chosen architecture that mirrors the problem domain.That is, "neural networks" as a methodology seems \'general\' in a meaningful way; one could imagine that a general intelligence could be formed by solving the meta-learning problem (that is, learning the architecture that best suits a particular problem while learning the weights for that problem from training data).Even in that case, there will still be a flexibility-efficiency tradeoff; the general intelligence that\'s allowed to vary its architecture will be able to solve many different problems, but will take some time to discover what problem it\'s facing. An intelligence locked into a particular architecture will perform well on problems that architecture is well-suited for (better than the general, since it doesn\'t need to discover) but less well on other problems it isn\'t as well-suited for.'	236	0	0	0
1993	1989	0	69812	1	b'It would appear so. One example, albeit not specifically AI related, is seen in the difference between digital computers and analog computers. Pretty much everything we think of as a "computer" today is a digital computer with a von Neumann architecture. And that\'s because the things are so general purpose that they can be easily programmed to do, essentially, anything. But analog computers can (or could, back in the 60\'s or thereabouts) solve some types of problems faster than a digital computer. But they fell out of favor exactly due to that lack of flexibility. Nobody wants to hand-wire circuits with op-amps and comparators to solve for y. '	108	0	0	0
1995	74	0	84379	1	b'Strong and weak AI are the older terms for AGI (artificial general intelligence) and narrow AI. At least that\'s how I have seen it used and wikipedia seems to agree. I personally haven\'t seen Searle\'s definition of "weak and strong AI" in use much, but maybe the shift to the newer terms came about in part because Searle successfully confused the issue. '	62	0	0	0
1996	-1	0	0	2	b'Is there a neural network(NN) system or architecture which can be used for only storing and retrieving information. For example; to store whole Avatar movie in HD format inside a neural network and retrieve(without loss) it from the neural network when needed. I searched the web and came across only LSTM RNN but in my understanding LSTM only stores pattern and not the content itself. If there is no such NN exist can you explain why it so?'	77	0	0	0
1997	-1	0	0	5	b'The question is about the architecture of Deep Residual Networks (ResNets). The model that won the 1-st places at "Large Scale Visual Recognition Challenge 2015" (ILSVRC2015) in all five main tracks:  ImageNet Classification: \xe2\x80\x9cUltra-deep\xe2\x80\x9d (quote Yann) 152-layer nets  ImageNet Detection: 16% better than 2nd ImageNet Localization: 27% better than 2nd COCO Detection: 11% better than 2nd COCO Segmentation: 12% better than 2nd Source: MSRA @ ILSVRC &amp; COCO 2015 competitions (presentation, 2-nd slide) This work is described in the following article: Deep Residual Learning for Image Recognition (2015, PDF)Microsoft Research team (developers of ResNets: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun) in their article: "Identity Mappings in Deep Residual Networks (2016)"state that depth plays a key role: "We obtain these results via a simple but essential concept \xe2\x80\x94 going deeper. These results demonstrate the potential of pushing the limits of depth."It is emphasized in their presentation also (deeper - better):  - "A deeper model should not have higher training error."  - "Deeper ResNets have lower training error, and also lower test error."  - "Deeper ResNets have lower error." - "All benefit more from deeper features \xe2\x80\x93 cumulative gains!" - "Deeper is still better."Here is the sctructure of 34-layer residual (for reference):But recently I have found one theory that introduces a novel interpretation of residual networks showing they are exponential ensembles: Residual Networks are Exponential Ensembles of Relatively Shallow Networks (2016)Deep Resnets are described as many shallow networks whose outputs are pooled at various depths. There is a picture in the article. I attach it with explanation: Residual Networks are conventionally shown as (a), which is a natural representation of Equation (1). When we expand this formulation to Equation (6), we obtain an unraveled view of a 3-block residual network (b). From this view, it is apparent that residual networks have O(2^n) implicit paths connecting input and output and that adding a block doubles the number of paths.In conclusion of the article it is stated: It is not depth, but the ensemble that makes residual networks strong. Residual networks push the limits of network multiplicity, not network depth. Our proposed unraveled view and the lesion study show that residual networks are an implicit ensemble of exponentially many networks. If most of the paths that contribute gradient are very short compared to the overall depth of the network, increased depth alone can\xe2\x80\x99t be the key characteristic of residual networks. We now believe that multiplicity, the network\xe2\x80\x99s expressability in the terms of the number of paths, plays a key role.But it is only a recent theory that can be confirmed or refuted. It happens sometimes that some theories are refuted and articles are withdrawn.My question:Should we think of deep ResNets as ensemble after all? Ensemble or depth makes residual networks so strong? Is it possible that even the developers themselves do not quite perceive what their own model represent and what is the key concept in it?'	489	0	4	0
1998	1996	0	32234	4	b'Neural networks are usually trained to produce a certain output given a certain input. Often the output is a classification of the input or some other form of input description. Sometimes it is an action in a game and sometimes it is indeed stored data, a memory if you will. In that case the input is often a part of the stored data, so the NN actually completes the given input. This setup is called autoassociative memory, Hopfield networks are an example for this. In your example you might give this NN the first frame of the movie Avatar and it would output the complete movie. Unfortunately this would probably be absolutely crazy inefficient. '	114	0	0	0
1999	1997	1	13576	3	b'Imagine a genie grants you three wishes. Because you are an ambitious deep learning researcher your first wish is a perfect solution for a 1000-layer NN for Image Net, which promptly appears on your laptop.Now a genie induced solution doesn\'t give you any intuition how it might be interpreted as an ensemble, but do you really believe that you need 1000 layers of abstraction to distinguish a cat from a dog? As the authors of the "ensemble paper" mention themselves, this is definitely not true for biological systems.Of course you could waste your second wish on a decomposition of the solution into an ensemble of networks, and I\'m pretty sure the genie would be able to oblige. The reason being that part of the power of a deep network will always come from the ensemble effect.So it is not surprising that two very successful tricks to train deep networks, dropout and residual networks, have an immediate interpretation as implicit ensemble. Therefore "it\'s not depth, but the ensemble" strikes me as a false dichotomy. You would really only say that if you honestly believed that you need hundreds or thousands of levels of abstraction to classify images with human accuracy. I suggest you use the last wish for something else, maybe a pinacolada. '	212	0	0	0
2000	-1	0	0	0	b"In my attempt at trying to learn neural network and machine learning I'm am trying to create a simple neural network which can be trained to recognise one word from a given string (which contains only one word). So in effect if one where to feed it a string containing the trained word but spelled wrong the network would be able to still recognise the word. Can anybody help me with some pseudo code or a start of a code. Or a general explanation of how to to this because I have read like 6 articles and 8 example projects and still have no clue how to do this"	108	0	0	0
2003	2000	0	53260	0	b"If I'm reading it correctly, this question has nothing to do with optical character recognition. You want to create a system that takes a digital string of characters as input, then finds the best match from a predetermined list of words. That sounds like a task for if-then-else logic and dictionary lookup. It might be possible to use a neural net, but not easy.A neural net takes a fixed number of inputs, each of which are a value between zero and one. A major hurdle is that you probably want variable-sized inputs. Another hurdle is that you'll need to code the inputs some way onto numbers.These hurdles can be overcome but they are tipoffs that neural networks aren't well-suited for the task."	121	0	0	0
2004	2000	1	56837	0	b"An optimal solution for the task as stated, would be some alignment algorithm like Smith-Waterman, with a matrix which encodes typical typo frequencies. As an exercise in NNs, I would recommend using a RNN. This circumvents the problem that your inputs will be of variable size, because you just feed one letter after another and get an output once you feed the delimiter. As trainingsdata you'll need a list of random words and possibly a list of random strings, as negative examples and a list of slightly messed up versions of your target word as positive examples. Here is a minimal character-level RNN, which consists of only a little more than a hundred lines of code, so you might be able to get your head around it or at least get it to run. Here is the excellent blog post by Karpathy to which the code sample belongs. "	148	0	1	0
2005	-1	0	0	11	b"In 2004 Jeff Hawkins, inventor of the palm pilot, published a very interesting book called On Intelligence, in which he details a theory how the human neocortex works. This theory is called Memory-Prediction framework and it has some striking features, for example not only bottom-up (feedforward), but also top-down information processing and the ability to make simultaneous, but discrete predictions of different future scenarios (as described in this paper).The promise of the Memory-Prediction framework is unsupervised generation of stable high level representations of future possibilities. Something which would revolutionise probably a whole bunch of AI research areas.Hawkins founded a company and proceeded to implement his ideas. Unfortunately more than ten years later the promise of his ideas is still unfulfilled. So far the implementation is only used for anomaly detection, which is kind of the opposite of what you really want to do. Instead of extracting the understanding, you'll extract the instances which the your artificial cortex doesn't understand. My question is in what way Hawkins's framework falls short. What are the concrete or conceptual problems that so far prevent his theory from working in practice? "	186	0	1	0
2006	2005	1	27021	8	b'The short answer is that Hawkins\' vision has yet to be implemented in a widely accessible way, particularly the indispensable parts related to prediction. The long answer is that I read Hawkins\' book a few years ago and was excited by the possibilities of Hierarchical Temporal Memory (HTM). I still am, despite the fact that I have a few reservations about some of his philosophical musings on the meanings of consciousness, free will and other such topics. I won\'t elaborate on those misgivings here because they\'re not germane to the main, overwhelming reason why HTM nets haven\'t succeeded as much as expected to date: to my knowledge, Numenta has only implemented a truncated version of his vision. They left out most of the prediction architecture, which plays such a critical role in Hawkins\' theories. As Gerod M. Bonhoff put it in an excellent thesis1 on HTMs,  "In March of 2007, Numenta released what they claimed was a \xe2\x80\x9cresearch implementation\xe2\x80\x9d of HTM theory called Numenta Platform for Intelligent Computing (NuPIC). The algorithm used by NuPIC at this time is called \xe2\x80\x9cZeta1.\xe2\x80\x9d NuPIC was released as an open source software platform and binary files of the Zeta1 algorithm. Because of licensing, this paper is not allowed to discuss the proprietary implementation aspects of Numenta\xe2\x80\x99s Zeta1 algorithm. There are, however, generalized concepts of implementation that can be discussed freely. The two most important of these are how the Zeta 1 algorithm (encapsulated in each memory node of the network hierarchy) implements HTM theory. To implement any theory in software, an algorithmic design for each aspect of the theory must be addressed. The most important design decision Numenta adopted was to eliminate feedback within the hierarchy and instead choose to simulate this theoretical concept using only data pooling algorithms for weighting. This decision is immediately suspect and violates key concepts of HTM. Feedback, Hawkins\xe2\x80\x99 insists, is vital to cortical function and central to his theories. Still, Numenta claims that most HTM applicable problems can be solved using their implementation and proprietary pooling algorithms."I am still learning the ropes in this field and cannot say whether or not Numenta has since scrapped this approach in favor of a full implementation of Hawkins\' ideas, especially the all-important prediction architecture. Even if they have, this design decision has probably delayed adoption by many years. That\'s not a criticism per se; perhaps the computational costs of tracking prediction values and updating them on the fly were too much to bear at the time, on top of the ordinary costs of processing neural nets, leaving them with no other path except to try half-measures like their proprietary pooling mechanisms. Nevertheless, all of the best research papers I\'ve read on the topic since then have chosen to reimplement the algorithms rather than relying on Numenta\'s platform, typically because of the missing prediction features. Cases in point include Bonhoff\'s thesis and Maltoni\'s technical report for the University of Bologna Biometric System Laboratory2. In all of those cases, however, there is no readily accessible software for putting their variant HTMs to immediate use (as far as I know). The gist of all this is that like G.K. Chesterton\'s famous maxim about Christianity, "HTMs have not been tried and found wanting; they have been found difficult, and left untried." Since Numenta left out the prediction steps, I assume that they would be the main stumbling blocks awaiting anyone who wants to code Hawkins\' full vision of what an HTM should be.1Bonhoff, Gerod M., 2008, Using Hierarchical Temporal Memory for Detecting Anomalous Network Activity. Presented in March, 2008 at the Air Force Institute of Technology, Wright-Patterson Air Force Base, Ohio. 2Maltoni, Davide, 2011, Pattern Recognition by Hierarchical Temporal Memory. DEIS Technical Report published April 13, 2011. University of Bologna Biometric System Laboratory: Bologna, Italy. '	631	0	6	0
2008	-1	0	0	4	b"As far as I can tell, neural networks have a fixed number of neurons in the input layer.If neural networks are used in a context like for example NLP, sentences or blocks of text of varying sizes are fed to a network. How is the varying input size reconciled with the fixed size of the input layer of the network? In other words: how is such a network made flexible enough to deal with an input that might be anywhere from one word to multiple pages of text?If my assumption of a fixed number of input neurons is wrong and new input neurons are added to/removed from the network to match the input size I don't see how these can ever be trained.I give the example of NLP, but lots of problems have an inherently unpredictable input size, I'm interested in the general approach for dealing with this.edit: For images, it's clear you can up/downsample to a fixed size, but for text this seems to be an impossible approach since adding/removing text changes the meaning of the original input."	178	0	0	0
2009	2008	1	3226	4	b'Three possibilities come to mind:The easiest is zero padding. Basically you take a rather big input size and just add zeroes if your concrete input is too small. Of course this is pretty limited and certainly not useful if your input ranges from a few words to full texts.RNNs are a very natural NN to chose if you have texts of varying size as input. You input words as word vectors just one after another and the internal state of the RNN is supposed to encode the meaning of the full string of words. This is one of the earlier papers.Another possibility is using recursive NNs. This is basically a form of preprocessing in which a text is recursively reduced to a smaller number of word vectors until only one is left - your input, which is supposed to encode the whole text. This makes a lot of sense from a linguistic point of view if your input consists of sentences (which can vary a lot in size), because sentences are structured recursively (For example the word vector for "the man", should be similar to the word vector for "the man who mistook his wife for a hat", because noun phrases act like nouns etc.). Often you can use linguistic information to guide your recursion on the sentence. If you want to go way beyond the wiki article, this is probably a good start.'	233	0	2	0
2011	1885	0	33605	0	b"I am currently reading Superintelligence: Paths, Dangers, Strategies by Nick Bostrom. When he discusses whole brain emulation, although computing power (storage, bandwidth, CPU, body simulation, &amp; environment simulation) is one of the three general key things we are lacking toward its success, he also seems to agree that computing power is the most feasible and attainable of the three general issues we have for attaining it as of now. However he also goes on to to say  Just how much technology is required for whole brain emulation depends on the level of abstraction at which the brain is simulated.refWhich is an interesting thought, but a whole different discussion.Anyways, so I think you are correct in thinking that we aren't far from having the computing power and maybe you are on to something, but rather are biggest hurdles are the other two key prerequisites that we need to attain before we can even begin trying, which are scanning, and translation. Of the three, it would seem translation is the one we need to advance in the most, as of now. A modest prediction of attaining whole brain emulation is at least 15 years or mid century. Theres much more information in this book of all of the different paths that can be taken to achieve super intelligence, and it is well researched, I highly recommend it if you haven't read it already."	231	0	0	0
2012	-1	0	0	-1	b'In my estimation we have two minds which manage to speak to each other in dialectic through a series of interrupts. Thus at any one time one of these systems is controlling master and inhabits our consciousness. The subordinate system controls context which is constantly being "primed" by our senses and our subordinate systems experience of our conscious thought process( see thinking fast and slow by Daniel Kahneman). Thus our thought process is constantly a driven one. Similarly this system works as a node in a community and not as a standalone thing. I think what we have currently is "artificial thinking" which is abstracted a long way from what is described above. so my question is "are there any artificial intelligence systems with an internal dialectical approach and with drivers and conceived above and which develop within a community of nodes? " '	143	0	0	0
2013	2012	0	24721	2	b'There are a lot of systems which follow the ancient maxim: "Always two there are; no more, no less. A master and an apprentice."In reinforcement learning a class of such setups is called Actor-Critic-Method. There you have a master, who\'s duty it is to create feedback for the actions of the apprentice, who acts in a given environment. This would be comparable to how a human learns some physical activity, like playing table tennis. You basically let your body do it\'s thing, but your consciousness evaluates how good the result is. The setup of AlphaGo might be even closer to Kahnemann\'s system 1 and system 2. AlphaGo has two neural networks which provide actions and evaluations (system 1, fast, intuitiv, etc.) and the monte carlo tree search, which uses these actions and evaluations to prune a search tree and make a decision (system 2, deliberate, logical). In the end this kind of structure will pop up again and again, because it is often necessary to do some kind of classification or preprocessing on the raw data, before your algorithm can be run on it. You could frame the whole history of gofai as the story of how scientists thought system 1 should be easy and system 2 should be doable in a few decades, where the reality is that we have no idea how difficult system 2 is, because it turned out that system 1 is extremely difficult. '	238	0	0	0
2014	2008	0	72021	1	b'Others already mentioned:zero paddingRNNrecursive NNso I will add another possibility: using convolutions different number of times depending on the size of input. Here is an excellent book which backs up this approach: Consider a collection of images, where each image has a different width and height. It is unclear how to model such inputs with a weight matrix of fixed size. Convolution is straightforward to apply; the kernel is simply applied a different number of times depending on the size of the input, and the output of the convolution operation scales accordingly.Taken from page 360. You can read it further to see some other approaches.'	104	0	1	0
2018	2012	0	18577	1	b'You could argue that some Multi-Agent System approaches do, and some systems based on the blackboard architecture could conceivably fit this regime as well. '	24	0	0	0
2020	-1	0	0	8	b'In the recent PC game The Turing Test, the AI ("TOM") needs help from Ava to get through some puzzle rooms. TOM says he is unable to solve the puzzles because he is not allowed to "think laterally." Specifically, he says he would not have thought to throw a box through a window to solve the first room. His creators, the story goes, turned that capability off because such thinking could produce "ethically suboptimal" solutions, like chopping off an arm to leave on a pressure plate.Would all creative puzzle-solving abilities need to be removed from an AI to keep its results reasonable, or could we get some benefits of lateral thinking without losing an arm?'	114	0	1	0
2021	-1	0	0	1	b'In the recent festival of science, there was a talk given by researcher Mike Cook about: ANGELINA, an AI game designer that has invented game mechanics, made games about news stories, and was the first AI to enter a game jam.So the aim of Angelina AI is basically to design videogames.Briefly, how exactly does Angelina design the new games? How does it work behind the scenes?'	65	0	1	0
2022	2020	1	3287	11	b'No, with a but. We can have creative yet ethical problem-solving if the system has a complete system of ethics, but otherwise creativity will be unsafe by default.One can classify AI decision-making approaches into two types: interpolative thinkers, and extrapolative thinkers. Interpolative thinkers learn to classify and mimic whatever they\'re learning from, and don\'t try to give reasonable results outside of their training domain. You can think of them as interpolating between training examples, and benefitting from all of the mathematical guarantees and provisos as other statistical techniques.Extrapolative thinkers learn to manipulate underlying principles, which allows them to combine those principles in previously unconsidered ways. The relevant field for intuition here is numerical optimization, of which the simplest and most famous example is linear programming, rather than the statistical fields that birthed machine learning. You can think of them as extrapolating beyond training examples (indeed, many of them don\'t even require training examples, or use those examples to infer underlying principles).The promise of extrapolative thinkers is that they can come up with these \'lateral\' solutions much more quickly than people would be able to. The problem with these extrapolative thinkers is that they only use the spoken principles, not any unspoken ones that might seem too obvious to mention.An attribute of solutions to optimization problems is that the feature vector is often \'extreme\' in some way. In linear programming, at least one vertex of the feasible solution space will be optimal, and so simple solution methods find an optimal vertex (which is almost infeasible by nature of being a vertex).As another example, the minimum-fuel solution for moving a spacecraft from one position to another is called \'bang-bang,\' where you accelerate the craft as quickly as possible at the beginning and end of the trajectory, coasting at maximum speed in between.While a virtue when the system is correctly understood (bang-bang is optimal for many cases), this is catastrophic when the system is incorrectly understood. My favorite example here is Dantzig\'s diet problem (discussion starts on page 5 of the pdf), where he tries to optimize his diet using math. Under his first constraint set, he\'s supposed to drink 500 gallons of vinegar a day. Under his second, 200 bouillon cubes. Under his third, two pounds of bran. The considerations that make those obviously bad ideas aren\'t baked into the system, and so the system innocently suggests them.If you can completely encode the knowledge and values that a person uses to judge these plans into the AI, then extrapolative systems are as safe as that person. They\'ll be able to consider and reject the wrong sort of extreme plans, and leave you with the right sort of extreme plans.But if you can\'t, then it does make sense to not build an extrapolative decision-maker, and instead build an interpolative one. That is, instead of asking itself "how do I best accomplish goal X?" it\'s asking itself "what would a person do in this situation?". The latter might be much worse at accomplishing goal X, but it has much less of the tail risk of sacrificing other goals to accomplish X.'	516	0	0	0
2023	-1	0	0	5	b'I understand that neural networks model biological neurons. Each node in the network represents a neuron cell and the connections between nodes represent the connections between cells. As in nature, a neuron fires an electrical signal to connected neurons based on some kind of threshold or function that mimics such. Recent discoveries on how the brain works reveal the importance of calcium within the cells. See  for more information. To summarize, calcium affects the regulation, stimulation and transmission of electrical activity as well as the destruction of neurones.From my study of neural networks, there does not seem to be a calcium equivalent. Having one would imply that the functions, connections and weights in an artificial network are configured during the training and execution process and can change over time. I understand that back-propagation is used to train the weights, but have not seen anything that trains the function nor the connections (although a zero weight could imply no connection).Does anyone know of such a network (or training algorithm)? If so, do these networks perform better than a network that is pre-configured?'	181	0	1	0
2024	2020	0	19933	0	b"You may consider the programming as an ethical part of the design as well. AI will act based on what has been instructed to it as ethically important or not.It may/should even be part of the parameters that forge the process of finding solutions, which could allow for a more refine and creative solution.We understand the basics of ethic in normal circumstances, but if we can't predict how any human will behave in an ethical conundrum we can enforce what an AI wouldn't do.As long as we have control over the mechanism that drive an AI we sure have a responsability to inject ethical failsafes.The problem lies in self taught AI with an ability to overrides directives.(CF Asimov Laws.)The way the AI is creative seems irrelevant in that case."	128	0	0	0
2025	2020	0	27443	-1	b"A lot of this depends on the breadth of consideration. For example, what would the medium and long term effects of the lateral thinking be? The robot could sever an arm for a pressure plate but it would mean that the person no longer had an arm, a functional limitation at best, that the person might bleed out and die/be severely constrained, and that the person (and people in general) would both no longer cooperate and likely seek to eliminate the robot. People can think laterally because consider these things - ethics are really nothing more than a set of guidelines that encompass these considerations. The robot could as well, were it to be designed to consider these externalities.If all else fails,Asimov's Laws of Robotics: (0. A robot may not harm humanity, or, by inaction, allow humanity to come to harm.) 1. A robot may not injure a human being or, through inaction, allow a human being to come to harm. 2. A robot must obey orders given it by human beings except where such orders would conflict with the First Law. 3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law"	203	0	0	0
2027	2023	1	52800	6	b"Neural networks don't model biological neurons. They are at best inspired by biological neurons, in that they get excited by certain inputs and fire once the excitation crosses a threshold. And this second point even holds only approximately because the backpropagation algorithm needs smoothed out steps to learn by gradient descent. And backpropagation is not even inspired by biology, that would rather be hebbian learning. Generally in machine learning people do what works. The connection to biology is tenuous at best. You will usually not find one to one correspondence of low level details between machine learning setups and biological neurons. For that you'll have to turn to brain simulations."	109	0	0	0
2028	-1	0	0	0	b'I am a student working on my final graduation project. I was assigned to study Hyper-heuristics and it is a new subject for me. I was asked to choose a computational problem to apply Hyper-heuristics on them and see the results. However, I am afraid to choose the wrong problem. What are, in your opinion, computational problem that can be resolved with hyper-heuristics efficiently.Thank you. '	65	0	0	0
2031	1625	0	48432	1	b'I would strongly recommend that you check the book "The Perfect Bet" by Adam Kucharski. It does not mention technical methods such as neural networks but it gives a good history (and very nice stories) on what people had done on that field. It gives you the notion that in order to get achieve a better payoff, your goal is not actually making a better prediction but choosing the better options by considering what other players are doing. If you ask why it is not just the better prediction, the answer is that there is always a balance of risk and payoff and since you cannot find a 100% guaranteed way of prediction, you will have to balance risk and payoff in order to gain in the long run. In addition, although theory suggests that you can make a good prediction by gathering all variables, this is not really possible in practice. Thus, human assistance and observing what others are doing is used as a way to improve mathematical predictions and possible payoffs.'	172	0	0	0
2033	-1	0	0	4	b"I'm a freshman to machine learning. We all know that there are 2 kinds of problems in our life: problems that humans can solve and problems we can't solve. For problems humans can solve, we always try our best to write some algorithm and tell machine to follow it step by step, and finally the machine acts like people.What I'm curious about are these problems humans can't solve. If humans ourselves can't sum up and get an algorithm (which means that we ourselves don't know how to solve the problem), can a machine solve the problem? That is, can the machine sum up and get an algorithm by itself based on a large amount of problem data?"	116	0	0	0
2034	2033	0	1496	2	b'There are problems we for which we don\'t have a known, optimal, deterministic algorithm. By and large we use heuristics to "solve" those problems. A closely related idea is that of satisficing where we seek out answers that are "good enough" for immediate purposes.Likewise, machines can also use heuristics, whether they are programmed in explicitly or, presumably, learned. Within the range of ways that a machine can use heuristics, there are meta heuristics and hyper heuristics.Going beyond that, there are other ways that machines an learn "algorithms" or "rules" for solving problems. One are that I\'m particularly interested in is known as rule induction. This is all an area of open and active research BTW... so if you\'re interested in exploring any of these approaches, you\'ll probably find a lot of ground to cover. '	134	0	0	0
2035	2033	0	1806	0	b'New guy here, please go easy on me as this answer will come from personal experience, and will probably be a tad philosophical.Every algorithm I\'ve designed was built to systematically tackle and solve specific problems in specific situations, each with an end goal in mind. Think of algorithms as solutions to a problem. In my career as a programmer, this rule has always stuck with me (it came from my favorite Computer Sciences professor): "If there is no solution, then there is no algorithm. If there is no algorithm, no machine can solve the problem."Can machines generate their own algorithms? Most likely. But not to the point that it will exceed us (and by exceed, I don\'t mean just speed). AIs can never solve problems using methods that humans will never be able to come up with, because we programmed AIs to solve problems just like us humans do.'	148	0	0	0
2036	-1	0	0	0	b'This is a question about a nomenclature - we already have the algorithm/solution, but we\'re not sure whether it qualifies as utilizing heuristics or not.feel free to skip the problem explanation: A friend is writing a path-finding algorithm - an autopilot for an (off-road) vehicle in a computer game. This is a pretty classic problem - he finds a viable, not necessarily optimal but "good enough" route using the A* algorithm, by taking the terrain layout and vehicle capabilities into account, and modifying a direct (straight) line path to account for these. The whole map is known a\'priori and invariant, though the start and destination are arbitrary (user-chosen) and the path is not guaranteed to exist at all.  This cookie-cutter approach comes with a twist: limited storage space. We can afford some more volatile memory on start, but we should free most of it once the route has been found. The travel may take days - of real time too, so the path must be saved to disk, and the space in the save file for custom data like this is severely limited. Too limited to save all the waypoints - even after culling trivial solution waypoints (\'continue straight ahead\'), and by a rather large margin, order of 20% the size of our data set.  A solution we came up with is to calculate the route once on start, then \'forget\' all the trivial and 90% of the non-trivial waypoints. This both serves as a proof that a solution exists, and provides a set of points reaching which, in sequence, guarantees the route will take us to the destination.  Once the vehicle reaches a waypoint, the route to the next one is calculated again, from scratch. It\'s known to exist and be correct (because we did it once, and it was correct), it doesn\'t put too much strain on the CPU and the memory (it\'s only about 10% the total route length) and it doesn\'t need to go into permanent storage (restarting from any point along the path is just a subset of the solution connecting two saved waypoints).Now for the actual question:The pathfinding algorithm follows a sparse set of waypoints which by themselves are not nearly sufficient as a route, but allow for easy, efficient calculation of the actual route, simultaneously guarantying its existence; they are a subset of the full solution. Is this a heuristic approach?(as I understand, normally, heuristics don\'t guarantee existence of a solution, and merely suggest more likely candidates. In this case, the \'hints\' are taken straight out of an actual working solution, thus my doubts.)'	433	0	0	0
2037	-1	0	0	0	b"I understand how a neural network can be trained to recognise certain features in an image (faces, cars, ...), where the inputs are the image's pixels, and the output is a set of boolean values indicating which objects were recognised in the image and which weren't.What I don't really get is, when using this approach to detect features and we detect a face for example, how we can go back to the original image and determine the location or boundaries of the detected face. How is this achieved? Can this be achieved based on the recognition algorithm, or is a separate algorithm used to locate the face? That seems unlikely since to find the face again, it needs to be recognised in the image, which was the reason of using a NN in the first place."	135	0	0	0
2038	2036	0	22166	1	b"Whether or not a label fits any particular instance depends on what you're using the label for. If something specific is riding on whether this approach is a 'heuristic' or not, that context is important.But I wouldn't call this a heuristic, because I think of that as a shortcut for solving a problem, not either storing a solution or reformulating the problem (which is how I'd think of this). "	69	0	0	0
2039	2033	1	44922	1	b"All intelligence, both human and machine, is mechanistic. Thoughts don't appear out of the blue; they're generated through specific processes.This means that if a machine generates an algorithm to solve a problem, even if the object-level algorithm wasn't generated by humans, the meta-level algorithm by which it generated the object-level algorithm must have come from somewhere, and that somewhere is probably its original creators. (Even if they didn't program the meta-level algorithm, they probably programmed the meta-meta-level algorithm that programmed the meta-level algorithm, and so on.)How you think about these distinctions depends on how you think about machine learning, but typically they're fairly small. For example, when we train a neural network to classify images, we aren't telling it what pixels to focus on or how to combine them, which is the object-level algorithm that it eventually generates. But we are telling it how to construct that object-level algorithm from training data, what I'm calling the 'meta-level' algorithm.One of the open problems is how to build the meta-meta-level; that is, an algorithm that will be able to look at a dataset and determine which models to train, and then which model to finally use. This will, ideally, include enough understanding of those meta-level models to construct new ones as needed, but even if it doesn't will reflect a major step forward in the usability of ML."	225	0	0	0
2040	-1	0	0	0	b'If said AI can assess scenarios and decide what AI is best suited and construct new AI for new tasks. In sufficient time would the AI not have developed a suite of AIs powerful/specialized for their tasks, but versatile as a whole, much like our own brain\xe2\x80\x99s architecture? What\xe2\x80\x99s the constraint ?'	51	0	0	0
2041	2037	0	26182	2	b'The approach you listed here is not really an approach, this is very very vague idea of how someone can achieve some task. You basically told we have an algorithm f(image) = result and there can be infinite amount of real approaches to solve this.In majority of CNN approaches the image travels through a convolution/pooling layers which reduces the dimensions of each current layer. In the end you end up with a significantly smaller layer which goes through the softmax and gets probabilities of different classes. This type of networks does not tell you where something was found, it just tells you that something was found somewhere in your original image.'	110	0	0	0
2042	2040	1	30442	0	b'If the AI can indeed assess arbitrary scenarios and come up with solutions to handle them, then it would indeed be an AGI. What\xe2\x80\x99s the constraint ?It doesn\'t exist. Current programmers are very good at developing AI that can handle specific tasks ("narrow AIs"), but it is currently impossible to build an AI that can assess and handle "general" situations (unlike your proposed algorithm, which possess that capacity).Theoretically, we can have a program that can build other programs (genetic algorithms are arguably one such example), but handling arbitrary scenarios and problems requires a form of "general intelligence", which we don\'t know how to program. Therefore, we can\'t build this machine.It\'s possible that we can built this machine, but we must first figure out the hard problem of "general intelligence". We\'re nowhere near reaching that level.If we figure out how to program "general intelligence", then it should be fairly simple to use your approach (building an AGI to assess scenarios and then build "narrow AIs" that can handle Those scenarios). Only then we can understand the AGI\'s limitations and weaknesses, and be able to identify probable constraints to its power. For example, it\'s possible that such an AGI may be slow in handling arbitrary scenarios and developing the "narrow AIs"...in which case, it may take an absurdly long period of time to develop "a suite of AIs powerful/specialized for their tasks".But until we build the AGI itself, we won\'t be able to identify its faults or weaknesses. Going beyond that would be science-fiction speculation.'	252	0	0	0
2044	2037	1	80406	6	b"This problem is called object detection.If you have a trainings set of images with boxed objects you can just train a neural network to directly predict the box. I.e. the output has the same dimension as the input and the NN learns to assign each pixel the probability of belonging to a certain object.If you don't have such a convenient dataset you could just recursively narrow the location down by feeding parts of the image to the network until you find the smallest part that still fully activates a certain classification. In this paper they try a mixture of these two approaches."	101	0	0	0
2045	2040	0	6006	0	b'To build on Tariq Ali\'s answer... There\'s no such thing as an AGI. The No Free Lunch (NFL) theorem states essentially that: any two optimization algorithms are equivalent when their performance is averaged across all possible problems. Specialization implies a loss of generality, not a gain.With this AI generating AI, you\'re describing what I call an \'arbitrary machine generator\' (AMG).There are two types of AMGs: a species level and an individual level.All species that evolve on earth are AMG - they can evolve to accommodate arbitrary niches, if the correct environmental constraints are present. This is proven by the fact that the species AMG processes on earth have produce humans, which are individual level AMGs. Individual level AMGs can produce arbitrary machines for arbitrary purposes on human time-scales.The problem is that the simplest possible (and most general) AMG is a purely random machine generator. Any more specificity (and therefore complexity) to the AMG would constrain the domains it closes over. Which is fine, but optimizing a machine for a particular set of tasks means that you are unoptimizing the machine for some other particular set of tasks. Again, there\'s no free context.Humans are AMGs, but we are only efficient at creating certain kinds of machines, using our imagination. Our imagination is built on a number of cognitive tools that, on the one hand, constrain what machines we can efficiently imagine, while, on the other hand, avail us to the open-ended set of all possible machines, via prior knowledge or brute force, random lookup.In summary, when people say "general intelligence", they really mean "human-like intelligence". And, again, while human intelligence is an AMG, any given AMG that is optimized for generating machines of a particular type will be less optimized for generating machines of some other type(s). There\'s no free context. The most general search algorithm is a random walk - there is no way to improve the generality of the random walk, other than just speeding it up. And that\'s actually what humans do for many problems anyway - brute force, random searching, as fast as possible.'	346	0	0	0
2046	2020	0	6333	1	b"Ethics involves the relationships of needs between two or more parties. As Matthew Graves said, if the AI lacks the sufficient human context (understanding of needs), it will produce seemingly perverse ethical behavior.And let's be honest, some people would cut of other people's arms and put them on pressure plates. Even the best of us will not be able to sympathize with the needs of others with 100% accuracy - at best, we're guessing. And then there are those rare situations where I actually want you to cut off my arm and put it on a pressure plate, perhaps to save a loved one.If we could make a thing that could sympathize with what a human might need in any given arbitrary situation, then we will have created either A) an artificial human intelligence (AHI) (which could be more or less fallible, like a human), or B) an oracle that can reason about all possible human needs on much faster than human time-scales - in which case you wouldn't need a conscious AI, as all human needs and solutions could be pre-computed via formal specification, which is probably absurd to consider."	190	0	0	0
2047	1885	0	83179	0	b'If the universe is discrete, then analog phenomena (fluidity, curvature) are built on primitively discrete phenomena (bits and pieces).If the universe is continuous, then discrete phenomena (bits and pieces) are built on primitively continuous phenomena (fluidity, curvature).If the universe is discrete, the speed of seemingly analog phenomena will be bounded by the number of discrete phenomena that can occur in time and space.If the universe is continuous, then time, space or matter may be infinitely divisible, which may allow for the execution of some phenomena faster than those phenomena appear to execute in natural environments (like protein folding or electric circuits) - so called "super Turing" computation.The continuous universe idea begs the question, though: From whence came all this discreteness? A discrete universe can allow for apparent continuous behavior via approximation and randomness (or pseudorandomness), whereas a universe that is infinitely divisible affords no obvious definition of where things should start and end. This is one of the reasons many thinkers eschew considering infinities - they may be illusory.So, can analog "circuits" execute faster than digital? As of right now, we know of some seemingly analog phenomena that appear execute faster than some digital phenomena (like electron spin vs a silicon logic gate). Whether analog phenomena are intrinsically more efficient than digital depends on the actual nature of the universe, which we have not yet determined.'	225	0	0	0
2048	-1	0	0	2	b"AI is progressing drastically, and imagine they tell you you're fired because a robot will take your place. What are some jobs that can never be automated?"	26	0	0	0
2049	2048	0	1863	1	b'If you were to completely automate a human, you\'d just have another human, which defeats the purpose of the automation.Any job that requires a "whole human," rather than just a human\'s hands, feet, or simple reasoning ability, will still require humans.If I go to a shrink, one with Wikipedia-like knowledge would be great, but one that also actually knows what its like to rub its eyes in the morning would be even better. Why? Because solving some problems will require knowing what it is like to rub one\'s eyes in the morning.If I go to a movie that was written, directed and produced by some form of automation, I may be able to suspend my disbelief and get carried away by the story, but something in me will fundamentally appreciate the movie less, if I know that the AI can produce an infinite number of these stories, completely arbitrarily. There is something about knowing that the story came from a mind that has been conditioned against the vagaries of humanity (ie, a "whole human"), that makes the story more appreciable.If I call up a suicide hotline because I wan\'t someone to sympathize with me about my existential crisis, I\'ll want to talk to a "whole human" that can sympathize with my existential condition, not one that just regurgitates prior wisdom on life, heuristically matched against my problem state.If I want to vote for a politician that can sympathize with the needs of the people, I\'ll want a "whole person" politician that can reflect on all the specifics that make life for a "whole person" hard or easy.If I want soldiers to take the lives of humans, I want some sort of intelligence in that kill-chain that executes "whole person" analysis prior to pulling the trigger (a human).If I want a conflict resolution specialist, capable of resolving complex cultural problems between humans, then I don\'t want just an AI that spits out the most likely solution based on prior solutions. I want an AI that can reason about prior solutions and all explicit and implicit problems between humans, in all human contexts, which requires a human or a perfect human simulacrum.For any problem that requires consideration potentially across the whole spectrum of human context, we will want that solution to be generated by a "whole human" device. But if we automate the "whole human" then we haven\'t really outsourced the problem to automation but rather to a "whole automated human," which will, by necessity, have its own problems.Sure, we\'ll probably create an artificial human intelligence (AHI) one day, but being optimized to automatically solve any given human problem without also having human problems... that\'s just AI snake-oil that will never exist - outside of some perverse matrix scenario, under an infinite oracle of some sort.So, yes, there will be many jobs that still require humans - mostly human-to-human problems that require full knowledge of the human context.'	485	0	0	0
2050	2048	0	20743	2	b' What are some jobs that can never be automated?None.The key word here is "never". Technology is rapidly advancing, and while I can think of situations where jobs can\'t be killed in the short-term or even in the long-term, I can\'t think of a job that is 100%, totally immune to extinction. Surely they exist, but you can\'t be sure...anything can happen after all. As long as it\'s possible, that\'s what matters here. You can\'t prove a negative.This whole question seems as foolhardy as predicting in the 1850s that airplanes would never be invented. You\'d be right in assuming that airplanes would not be invented in the 1860s...or the 1870s...or even the 1880s...but eventually, airplanes would be invented.What would be better is to provide a specific cut-off point ("will all jobs be automated by the year 2020?") that can allow us to try to extrapolate and predict based on current trends, but even that starts being difficult as you extend the cut-off point - My predictions about 2020 will be more accurate than my prediction about 2220. I think this type of question is truly unanswerable and can quickly decay to science-fiction speculation.Some additional comments about Doxosphoi\'s answer:Doxosophoi made some arguments for why current society might not accept the automation of all jobs (the need for the "personal touch" that only a human-like intelligence can provide), but that\'s no reason to assume that society will never accept automation. Technology can change and adapt, and humans can also change and adapt. Maybe a human might not care about a shrink who "rubs its eyes in the morning", dislike movies that are marred by that "vagaries of humanity" instead of personal customization, prefer politicians and soldiers that actually acts logically instead of acting like a falliable human, etc., etc. I mean, it\'s possible.There\'s also the problem of the term "job". Technically, I am working by writing an answer on a StackExchange website, but I\'m not getting paid for it, so it\'s not a real "job"...at best, it\'s just a hobby. I\'m providing a valuable human touch, but since no one is giving me money, it\'s possible that this human touch may not be all that valuable in the first place: "never give out your labor for free, because then they\'ll take it for free".Some of the techno-utopists (which I disagree with heavily) believes in a future where bots handle produce a lot of industrial goods and services, generating a lot of revenue that is then redistributed to the general population via some "Basic Income" scheme. This allows humans to do what they really want instead...such as hobbies? And what if the hobbies of the future are the "jobs" of today: shrinks, movie directors, politicians, soldiers, etc., etc. Instead of working for a paycheck, you\'re working in these jobs on a volunteer basis.Obviously, no automation is being necessary to eliminate these hobbyists (no matter how good a bot is, free labor will always prevail), but they\'re not really jobs, are they? The bot is the one that is producing real value, and subsidizing the hobbies of all these other people. The idea of a "job" itself could be in jeopardy.I don\'t think this scenario is likely either (in fact, I\'d probably think it\'s just AI snake oil that will never actually happen). But it\'s possible and that\'s why I can\'t dismiss it outright. It could happen, just that I don\'t think it will.And finally, the question is asking about whether a job can be automated, not whether it\'s a good idea to have it be automated, which is a completely different question. It\'s possible that we can build machines that can automate everything, and choose as a society not to use them for a variety of different reasons (such as the reasons that Doxosophoi mentioned).'	631	0	0	0
2052	2048	1	53230	7	b'The Oxford study from 2013 in The future of employment paper assess this and estimated the probability of computerisation for 702 detailed occupations using a Gaussian process classifier (using job data from the UK partially merged with data from US), and based on these estimates they identified three areas of computerisation bottleneck areas and nine skills that people are still needed for each profession, this includes:Perception and Manipulation.Finger dexterity. The ability to make precisely coordinated movements of the fingers of one or both hands to grasp, manipulate, or assemble very small objects.Manual dexterity. The ability to quickly move your hand, your hand together with your arm, or your two hands to grasp, manipulate, or assemble objects.The need for a cramped work space. How often does this job require working in cramped work spaces that requires getting into awkward positions?Creative Intelligence.Originality. The ability to come up with unusual or clever ideas about a given topic or situation, or to develop creative ways to solve a problem.Fine arts. Knowledge of theory and techniques required to compose, produce, and perform works of music, dance, visual arts, drama, and sculpture.Social Intelligence.Social perceptiveness. Being aware of others\xe2\x80\x99 reactions and understanding why they react as they do.Negotiation. Bringing others together and trying to reconcile differences.Persuasion. Persuading others to change their minds or behavior.Assisting and caring for others. Providing personal assistance, medical attention, emotional support, or other personal care to others such as coworkers, customers, or patients.Source: The future of employment: how susceptible are jobs to computerisation: Table 1.What this study is basically saying, around 50% of all jobs will be replaced by robots in the next 20 years.Based on the above study, the BBC assembled a handy guide that calculates which jobs are likely to be automated within the next two decades:Will a robot take your job?See also: replacedbyrobot.info website. With this tool, you can check the prediction of over 700 jobs.Related:When robots can do all manual labor and service jobs, what will the majority human population do?Labore Ad Infinitum: AI &amp; Automation vs Timeless TasksWhich suggests: Military/Peacekeeper, Athletes, Therapist, Musical Performer, Actors and Dancer, Visual Artists, Religious/Spiritual Leaders, The World\xe2\x80\x99s Oldest Profession, Virtual Goods, Politicians, Judges, Parenting.'	361	0	4	0
2053	2033	0	34914	0	b'I\'d like to offer also a slightly different view on the machine cannot better its master. Consider the very simple case of content classifiers. It\'s already to the point where for some areas classification and prediction can be performed way better than a human. And while a human may have designed the "algorithm", the algorithm was likely a recurrent neutral network or other form of ML that could well have self trained. In these cases we don\'t actually understand or need to understand the individual weights in the net, as we would need to have traditionally understood the imperative programming constructs we used to write. It just works.So if we get to where we develop a meta-algorithm for classifying problems and building more optimal deep learning solutions than we would by hand, but I think that would pretty much take us out of the picture for quite a lot of problem spaces. Thoughts? '	153	0	0	0
2054	-1	0	0	0	b"I want to have a program that writes like a human. But I don't just want a font, but instead an 'intelligent' program that produce different result and that can be trained with different sets to generate different handwritings.As a training set I would like to have parts of a handwritten text (saved as a list of paths (like in vector graphics).Maybe as a means to simplify things, I could flatten the paths in to consecutive straight lines. My program receives a string of text and produces a list of paths (or a vector graphic, whatever is easier to work with)My question now is: What kind of machine learning would be best to achieve this?"	114	0	0	0
2055	-1	0	0	1	b"I'm wondering how feasible it is to create a machine that can separate clothing from a basket.At the most basic level it would distinguish between tops, pants, button downs and socksProgrammatically, I'd image this would require training a neural network to recognize these items, but in real time it becomes exponentially difficult to do this in a small space at a fast rate:pick up an itemlay it in such a way that is recognizable deduce whether it is a top, button down, etc.sort it accordinglyIf this sounds ridiculous please let me know...If it is possible :would this be based on some sort of computer vision?or only a well trained neural network?Any insight is much appreciated!"	114	0	0	0
2056	-1	0	0	7	b'For years I have been dealing with (and teaching) Knowledge Representation and Knowledge Representation languages. I just discovered that in another community (Information Systems and the such) there is something called the "DIKW pyramid" where they add another step after knowledge, namely wisdom.They define data as being simply symbols, information as being the answer to who/what/when/where?, knowledge as being the answer to how?, and wisdom as being the answer to why?. My question is: has anyone done the connection between what AI calls data/information/knowledge and these notions from Information Systems? In particular, how would "wisdom" be defined in AI? And since we have KR languages, how would we represent "wisdom" as they define it?Any references would be welcome\xe2\x80\xa6'	117	0	0	0
2057	2055	1	21652	1	b"Peter Abbeel does work in deep learning for robotics, and one of the projects they've tackled is manipulating clothes. Here's a video from 2011 of their robot folding laundry (one piece at a time).There are also companies attempting to market this; seven dreamers makes the Laundroid and Foldimate claims that it will start taking pre-orders in 2017."	56	0	0	0
2058	2056	0	35495	0	b"I haven't done the connection - didn't know about the pyramid. I'm not sure it translates well into AI though.It seems they're separating information from knowledge by splitting how from what. What is a superset of how, as far as I'm concerned. It's also a superset of why.But from an evolutionary perspective, knowledge representation starts with why. Prior to a reason for knowledge representation, there is no knowledge representation. The 'what' existed, but it was not represented until autopoiesis created goal directed, why-oriented behaviors that began storing the what as knowledge. What is a superset of why, just as ontology is a superset of teleology. However, all represented ontology was acquired through teleological (end-directed) action.So I disagree with the notion that wisdom, as a why thing, is at the tip of the pyramid. It all started with goal directed behavior and that has been the source of all subsequent information growth.So what is wisdom? I think it is too much of a folk term to warrant a technical definition. If I had to just take a swing at a definition, though, I'd probably vote for wisdom being knowledge of the ontological basis of one's own teleological knowledge - essentially objectifying one's subjective interpretations - knowing the true what and how of the why, to whatever extent is possible.I don't have many specific references on this subject, but I thought Terrence Deacon's Incomplete Nature: How Mind Emerged from Matter was a good primer on teleology."	243	0	0	0
2059	2054	1	58782	4	b'From the abstract of this paper Generating Sequences With Recurrent Neural Networks[pdf]  This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.An implementation is here: handwriting-synthesisAnd using TensorFlow: Handwriting Generation Demo in TensorFlow'	107	0	1	0
2061	2056	0	48169	1	b'As with another answer, I am also skeptical of the distinctions made in the DIKW pyramid.Nonetheless, a very popular machine learning approach for answering \'Why?\' questions is the application of Bayesian reasoning: given a causal data model, reverse inference can be used to find the probability distribution of events which lead to a given outcome.It could be argued that defining \'cause\' in terms of distributions rather than specific concrete mechanisms is a rather limited notion of \'Why?\'.However, it may be that there are some forms of causality that we don\'t know how to represent, specifically \'first-hand experience\'. Indeed, common usage of the term \'wisdom\' generally refers to first-hand experience, rather than information gained from some other source. The idea is that knowledge can be expressed declaratively, whereas wisdom must be derived from experience.For an AI represented as a computer program, the distinction between declarative and first-hand experience might appear irrelevant, since in principle any experience can be encoded and made available without the program having to \'experience\' it first-hand.However, the following humorous definition of `wisdom\' might perhaps shed some light on a distinction that\'s pertinent to AI research: Knowledge is knowing that a tomato is a fruit.  Wisdom is knowing that you shouldn\'t eat it with custard.This notion of \'Wisdom\' could be said to require qualia. It is the subject of much debate whether qualia exist and/or are necessary for consciousness - see for example the thought experiment of \'The Black and White Room\'.So the notion is that there is a distinction between having a Bayesian network representation of wisdom that says: "It is 99.7% likely that putting a tomato in custard is undesirable" and the first-hand experience to the effect that it tastes odd with custard.'	287	0	0	0
2062	1613	0	45535	0	b"Autonomous vacuum cleaners usually have these following task environment properties:i. Partially observable environmentii. Deterministic environment.iii. Sequential environment.iv. Static environment.v. Discrete environmentvi. Single agent environmentSince it's a partially observable environment, the agent performs its actions based on what is sees and thus it's is in an deterministic environment where the current action will be the result of previous actions. And since, the agent is continuously performing, it's an sequential environment. Since, the environment doesn't change when agent is working, the environment is static and discrete since there are only finite no. of discrete states in which agent can be in. There are certain rules written which tells an agent how to react when in a particular state. If there are multiple rules which are satisfied, then agent uses its experience to choose an optimal action to perform. The rules are written by the programmer keeping in mind the task envt. properties.The agent's actions also depend on the type of agent it is decided by the programmer. It can be simple reflex-based, or a goal based, or a goal-based + feedback or a complete learning based agent. An agent can't use A* algorithm because the entire environment is not visible and it will be useless to use A* algorithm where we don't know when our goal may be reached. The agent has various sensors attached which give it info. about the surrounding, like cameras, sound recorders, thermal sensors, etc. An autonomous vacuum cleaner may also have a dirt sensor to detect the dirt. The agent performs an action using one of its actuators like wheels, robot arms, etc. "	266	0	0	0
2065	2036	0	35471	2	b'A \'heuristic\' is simply a \'rule-of-thumb\', i.e. something which doesn\'t guarantee an optimal solution to a problem.Beyond the above notion (certainly within the discipline of optimization), the notion of what constitutes a heuristic is not particularly strict, and could certainly include hints for constructing a new solution from parts of a previous one, as you are doing.A related concrete example is the "nearest neighbour heuristic" for the Travelling Salesman Problem, in which a solution is constructed by starting at some random city and iteratively choosing the nearest. The resulting completed tour is then used as an initial input to some more sophisticated optimization procedure.'	103	0	0	0
2066	-1	0	0	7	b'According to Wikipedia... The AI effect occurs when onlookers discount the behavior of an artificial intelligence program by arguing that it is not real intelligence.  Pamela McCorduck writes: "It\'s part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something\xe2\x80\x94play good checkers, solve simple but relatively informal problems\xe2\x80\x94there was chorus of critics to say, \'that\'s not thinking\'."[1] AI researcher Rodney Brooks complains "Every time we figure out a piece of it, it stops being magical; we say, \'Oh, that\'s just a computation.\'"[2]The Wikipedia page then proposes several different reasons that could explain why onlookers might "discount" AI programs. However, those reasons seem to imply that the humans are making a mistake in "discounting" the behavior of AI programs...and that these AI programs might actually be intelligent. I want to make an alternate argument, where the humans are making a mistake, but not in "discounting" the behavior of AI programs.Consider the following situation. I want to build a machine that can do X (where X is some trait, like intelligence). I am able to evaluate intuitively whether a machine has that X criteria. But I don\'t have a good definition of what X actually is. All I can do is identify whether something has X or not.However, I think that people who has X can do Y. So if I build a machine that can do Y, then surely, I built a machine that has X.After building the machine that can do Y, I examine it to see if my machine has X. And it does not. So my machine lacks X. And while a machine that can do Y is cool, what I really want is a machine that has X. I go back to the drawing board and think of a new idea to reach X.After writing on the whiteboard for a couple of hours, I realize that people who has X can do Z. Of course! I try to build a new machine that can do Z, yes, if it can do Z, then it must have X.After building the machine that can do Z, I check to see if it has X. It does not. And so I return back to the drawing board, and the cycle repeats and repeats...Essentially, humans are attempting to determine whether an entity has intelligence via proxy measurements, but those proxy measurements are potentially faulty (as it is possible to meet those proxy measurements without ever actually having intelligence). Until we know how to define intelligence and design a test that can accurately measure it, it is very unlikely for us to build a machine that has intelligence. So the AI Effect occurs because humans don\'t know how to define "intelligence", not due to people dismissing programs as not being "intelligent".Is this argument valid or correct? And if not, why not?'	481	0	0	0
2068	2066	1	8138	5	b'I think it is mostly right. But not that intelligence is hard to define. In my opinion, it is simple: A is more intelligent than B if A achieves some purpose in less steps than B. It is functional/algorithmic efficiency.What is difficult to define is human intelligence.But when someone says, "No, X is not real intelligence," what they mean is that it does not satisfy what we would consider real human intelligence.So when people discount new and amazing discoveries in machine intelligence, it is not because they are not amazing in their own way, but because those discoveries, while exhibiting intelligence, are actually not replicating human intelligence - which is what many people actually mean when they say "that thing isn\'t really intelligent."In truth we are very far, in the science of artificial intelligence, algorithmically speaking, from an artificial human intelligence (AHI).Additional note: What\'s funny is that we don\'t call the science of artificial intelligence just \'the science of intelligence.\' That we add the "artificial" qualifier by necessity pegs the science to what the artificiality implicitly emulates: human intelligence. In other words, "artificial intelligence" must be by definition more specific to the thing it allegedly artificializes than a more general science of just "intelligence."'	203	0	0	0
2069	-1	0	0	0	b"Here is one of the most serious questions, about the artificial intelligence.How will the machine know the difference between right and wrong, what is good and bad, what is respect, dignity, faith and empathy. A machine can recognize what is correct and incorrect, what is right and what is wrong, depend on how it is originally designed.It will follow the ethics of its creator, the man who originally designed it But how to teach a computer something we don't have the right answer. People are selfish, jealous, self confident. We are not able to understand each other sorrows, pains beliefs. We don't understand different religions, different traditions or beliefs. Creating an AI might be breakthrough for one nation, or one race, or one ethnic or religious group, but it can be against others. Who will learn the machine a humanity? :)"	140	0	0	0
2070	2069	1	12389	2	b"Right and wrong only exist relative to some goal or purpose.To make a machine do more right than wrong, relative to human goals, one should minimize the surface area of the machine's purpose. Doing that minimizes the intrinsic behavior of the AI, which enables us to reason about the right and wrong behaviors of the AI, relative to human purposes.Horses are quite general over the domains of their purposes, but are still predictable enough for humans to control and benefit from. As such, we will be able to produce machines (conscious or unconscious) that are highly general over particular domains, while still being predictable enough to be useful to humans.The most efficient machines for most tasks, though, will not need consciousness, nor even the needs that cause survivalistic, adversarial and self-preserving behaviors in eukaryotic cells. Because most of our solutions won't need those purposes to optimize over our problems, we can allow them to be much more predictable.We will be able to create predictable, efficient AIs over large domains that are able to produce predictable, efficient AIs over more specific domains. We'll be able to reason about the behavioral guarantees and failure modes of those narrow domains.In the event that we one day desire to build something as unpredictable as a human, like we do when having babies, then we'll probably do that with the similar intentions and care that we use to bring an actual baby into the world. There is simply no purpose in creating a thing more unpredictable than you unless you're gambling on this thing succeeding you in capability - which sounds exactly like having babies.After that, the best we can do is give it our theories about why we think we should act one way or another.Now, theoretically, some extremely powerful AI could potentially construct a human-like simulacrum that, in many common situations, seems to act like a human, but that in fact has had all of it's behaviors formally specified a priori, via some developmental simulation, such that we know for a fact that all such behaviors produce no intentional malice or harm. However, if we can formally specify all such behaviors, we wouldn't be using this thing to solve any novel problems, like curing cancer, as the formal specification for curing cancer would already have been pre-computed. If you can formally specify the behaviors of a thing that can discover something new, you can just compute the solution via the specification, without instantiating the behaviors at all!Once AI has reached a certain level of capability, it won't need to generate consciousnesses to derive optimal solutions. And at that point, the only purpose for an artificial human to exist will be, like us, for its own sake."	452	0	0	0
2072	2036	1	39743	1	b'Since you mention the A* algorithm, then you are definitely using a heuristic somewhere in there, at least with the A* algorithm while solving the subproblems using the straight-line distance as your heuristic function.Although your approach does not seem to incorporate a "shortcut mathematical formula" as a heuristic after that, it does us a precalculated heuristic table as a reference and that may qualify as a heuristic. Wikipedia page here says (although without citation) the following which does seem to describe what you are doing where your heuristic is not a fixed but a precalculated function/table: A heuristic function, also called simply a heuristic, is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow. For example, it may approximate the exact solution.On another note, your method also seems to have hints of dynamic programming since you use the precalculated and stored solutions to subproblems and instead of recalculating every time.I wonder if the term approximate dynamic programming would fit this situation as a non-stochastic version with no uncertainties? Unfortunately I could not find any simple description or categorization for that term.'	193	0	0	0
2075	1982	0	44388	0	b'It would not be wise to say that CNNs are better objectively than traditional approaches to solve computer vision problems as there are many problems for which the traditional methods works just fine. CNNs do have an inherent advantage over traditional methods which is the same advantage that deep learning has over other traditional methods i.e learning hierarchical features i.e what features are useful and how to compute them.The traditional way to approach a CV problem is to figure out the features that are relevant to the problem, figure out how to compute those features and then use those features to compute the final result. Whereas in CNN case the training process will figure out all the 3 points for you given that you have huge number of training examples.'	129	0	0	0
2077	-1	0	0	1	b"Can someone suggest step by step approach to learn AI rather than study a stack of book for long time.[ I'm not denying that books are great helper but what after that ]Thanks in Advance."	34	0	0	0
2078	-1	0	0	3	b'From this SE question: Will be AI able to adapt, to different environments and changes.This is my attempt at interpreting that question.Evolutionary algorithms are useful for solving optimization problems...by measuring the "fitness" of various probable solutions and then of an algorithm through the process of natural selection.Suppose, the "fitness calculation"/"environment" is changed in mid-training (as could easily happen in real-life scenarios where people may desire different solutions at different times). Would evolutionary algorithms be able to respond effectively to this change?'	80	0	1	0
2079	2078	1	3122	4	b"The core question to whether or not an AI is adaptable or not is whether or not it supports online learning. That doesn't mean using the Internet to learn things; that means continuing to accept training data during the functioning of the system.This is (mostly) independent of the underlying architecture; in evolutionary approaches one can continue to breed generations with a shifting fitness function or with neural networks one can continue to backpropagate errors, and so on with other approaches."	79	0	0	0
2080	2048	0	78754	0	b"Probably the only secure jobs are those where the audience enjoys watching live human craftsmanship take place in real time right before their eyes, like acting or standup comedy or musical virtuosity or playing a sport. Watching a robot do the same thing would be far less personally engaging since there's no human skill or artistry to appreciate or identify with, escpecially when the pressure is on or when human interpersonal dynamics are involved.For example, why would anyone watch robots play poker? Or dance? Or do standup comedy about how hard it is to be [ethnic group or gender goes here]?"	100	0	0	0
2081	2078	0	80938	3	b"I think Matthew Graves' answer is the strictly correct one. But I also think this question may be hinting at a larger question in general. What is the minimal algorithmic complexity required for a machine of one particular set of functions to mutate into some other machine of some other particular set of functions?The answer is: potentially infinite algorithmic complexity. Without knowing a priori how many steps it will take to mutate into a thing that can solve some black-box problem, there is no way to determine if and when the AI will be able to mutate into that thing."	99	0	0	0
2082	4	0	40592	2	b"Paper Szegedy C, Vanhoucke V, Ioffe S, et al. Rethinking the inception architecture for computer vision[J]. arXiv preprint arXiv:1512.00567, 2015. gives some general design principles:  Avoid representational bottlenecks, especially early in the network; Balance the width and depth of the network. Optimal performance of the network can be reached by balancing the number of filters per stage and the depth of the network. Increasing both the width and the depth of the network can contribute to higher quality networks. However, the optimal improvement for a constant amount of computation can be reached if both are increased in parallel. The computational budget should therefore be distributed in a balanced way between the depth and width of the network. These suggestions can't bring you the optimal number of neurons in a network though.However, there are still some model compression research e.g. Structured Sparsity Learning (SSL) of Deep Neural Networks, SqueezeNet, Pruning network that may shed some light on how to optimizing the neurons per single layer.Especially in Structured Sparsity Learning of Deep Neural Networks, it adds a Group Lasso regularization term in the loss function to to regularize the structures(i.e., filters, channels, filter shapes, and layer depth) of DNNs, which namely is to zero out some components(i.e., filters, channels, filter shapes, and layer depth) of the net structure and achieves a remarkable compact and acceleration of the network, while keeps a small classification accuracy loss. "	234	0	1	0
2083	-1	0	0	1	b"So I'm here to propose a strategy or to ask if this strategy has been tested in genetic algorithms in the past. I didn't exactly know how to find discussion about it.In a classic example of genetic algorithm you would have a population and certain amount of simulation time to evaluate it and breeding. Then proceed to the next generation.What if we would isolate a small part of the population in the simulation process and keep them evolving in their own little island for some time while rest of the population continues to evolve normally? After that they could be re-united with the rest of the population and the end of the simulation would go trough. After that breed the population and continue. This is super important part in natural evolution and probably some know if it actually works with genetic programming?"	141	0	0	0
2084	2083	0	10689	0	b'If I understand you correctly, I think you\'re referring to elitism.As Wikipedia explains: "A practical variant of the general process of constructing a new population is to allow the best organism(s) from the current generation to carry over to the next, unaltered. This strategy is known as elitist selection and guarantees that the solution quality obtained by the GA will not decrease from one generation to the next."'	67	0	0	0
2085	2083	1	11312	3	b"There have been extensive studies within Evolutionary Computation in the area of 'Island Models' and 'Niching' for doing exactly this.The advantages of this approach include greater population diversity (which is particularly useful when the problem is multiobjective) and the potential for concurrent execution of each separate population.See also the answer to this SE question.With specific reference to Genetic Programming here is a recent paper which uses a parallel island model."	69	0	4	0
2086	2021	0	72344	2	b"Can't tell. I guess half his site is down because of that malware.In any case, it appears that much of his past work on Github involves procedural generation. Which is AI... ish. Unless there's more to it, which we can't see because half the site is down.This paper appears to offer analysis of combining procedural generation with game AI.From the abstract: Populated and immersive game contexts require large numbers of minor, background characters to fill out the virtual environment. To limit game AI development effort, however, such characters are typically represented by very simplistic AI with either little difference between characters or only highly formulaic variations. Here we describe a complete workflow and framework for easily designing, generating and incorporating multiple, interesting game AIs. Our approach uses high-level, visual Statechart models to represent behaviour in a modular form; this allows for not only simplistic, parameterbased variation in AI design, but also permits more complex structure-based approaches. We demonstrate our technique by applying it to the task of generating a large number of individual AIs for computer-controlled squirrels within the Mammoth 1 framework for game research. Rapid development and easy deployment of AIs allow us to create a wide variety of interesting AIs, greatly improving the sense of immersion in a virtual environment.Update: actually, here we go. Here's an article from 2015 on AI and procedural generation, which discusses Angelina at length.And that article links to a more in depth article from 2013.Here's an excerpt: Cook gave ANGELINA the ability to learn about people so that it could make games based on current events. Then Cook gave ANGELINA memory - that is, the ability to keep track of the people it had learned about. The memory's not a big deal, even though it led to a number of philosophical disagreements around Cook's desk. ANGELINA's memory is actually just a text file where it stores the names of all the people it's heard of, alongside a number: a measure of its opinion of them based on the things it's learned from internet chatter. It liked Al-Assad more than May. It liked everyone more than May."	352	0	4	0
2087	-1	0	0	4	b'Obviously this is hypothetical, but is true? I know "perfect fitness function" is a bit hand-wavy, but I mean it as we have a perfect way to measure the completion of any problem.'	32	0	0	0
2088	2087	1	295	3	b"Yes.A totally random algorithm could solve any problem given unlimited time and a perfect fitness function. All you need to do is give the GA some new random population members each generation and you're guaranteed to find the solution eventually. Even if you keep only descendants of the previous generation, setting the mutation rate and number of crossovers high enough could effectively get you random individuals."	65	0	0	0
2089	1982	0	38945	0	b'Neural net approaches are very different than other techniques, mostly because NN aren\'t "linear" like feature matching or cascades. With very complicated tasks like realtime object recognition or other difficult patterns it\'s better to use neural net, first because if you train it well your net , you can get very high precision, second it\' easier to implement (it depends a lot from library to library) third usually after you have trained it they are very fast to classify or predict something. But a lot of tasks don\'t need neural nets, for example many factories to check the products use 3D features model matching. At the end you have to evaluate which method is the best for your task'	118	0	0	0
2090	2087	0	58332	1	b"As per the answer to this AI SE question,the presence of mutation makes a GA into a global search algorithm, i.e. it will eventually visit each point in the search space. How efficiently it will do so is indeed related to the quality of the fitness function:A 'perfect fitness function' could conceivably mean any of the following:A function which takes on an optimal value iff it is applied toan optimal solution. A function which forms a quadratic bowl(Newton's method can solve this in one step).A degenerate case of 1. is a 'Needle in a haystack' function, which returns the same arbitrarily poor value everywhere that isn't an optimum, and 2. unfortunately doesn't arise very often in practice.Hence, the role of a well-designed fitness function is to impose a gradient on the search process, which in practice will generally lie somewhere in-between 'Needle in a haystack' and 'Quadratic bowl'.The presence of some form of smoothness in the fitness function is one mechanism that allows better performance than random or exhaustive methods."	169	0	1	0
2092	-1	0	0	7	b'I\'m curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... I mainly know of AI being used in games or robotics fields. But can it be useful in "standard" application development?'	48	0	0	0
2093	1354	0	78281	0	b'Have the user label highlighted objects in video that a state of the art classifier cannot solveCreate a state of the art video classifier. Might as well train it on Google\'s YouTube-8M video training data. But you will want to continually feed it original video as well.Have the classifier label as many objects as it can. Have it isolate which objects it can recognize as objects but which it is unable to label.Have it output videos that outlines the objects. Preferably GIFs, which can be easily embedded in forms.For 100 of these, ask 100 users what the object is. If 90% of the users agree on the name of an object, add that video to the captcha-set. Call this the pre-trained set.Every time a user needs to authenticate, show them one of the highlighted objects in a video not from the pre-trained set. If the image has less than 100 showings, record the label and give the user another one from the pre-trained set. If they get it right, let them through, if not, give them another from the pretrained set.Once the non-pre-trained video has more than 100 showings and more than 90% of the captcha-users agree, add that video to the post-trained set.Over time, slowly remove the pre-trained set. Put expirations on each video in the post-trained set and remove them after expiration, so that they don\'t get used too many times.Ideally, this process would constantly improve the video classifier, keeping it state of the art and slightly ahead of other classifiers. Perhaps it could also favor less common words and objects and more esoteric things, so as to specialize this classifier against other classifiers.The same could be done for image labeling, but the utility of the video classifier will probably last longer, given advances in AI.Strictly speaking, though, short of some quantum trickery, there is no captcha system that will not one day be solved by external AI systems.(edit: oh, I just noticed you specifically said "textual captcha." If that\'s what you mean, then no I don\'t think text classification has much mystery left in it. Computers can probably glean text from pictures better than humans now. But techically, the input in the above described captcha system is textual.) '	371	0	0	0
2094	218	0	35660	1	b'Use something like Word2Vec. If a particular node has two edges that are very far from each other, besides the node in question, split the node into word(1) and word(2) nodes.'	30	0	0	0
2095	1451	1	27029	2	b'No.TL;DR: The Lovelace Test 2.0 is very vague, making it ill-suited for evaluation of intelligence. It is also generally ignored by researchers of Computational Creativity, who already have their own tests to evaluate creativity.Longer Answer:According to Google Scholar, there are 10 references to the "Lovelace Test 2.0" paper. All of those references exist merely to point out that the Lovelace Test 2.0 exists. In fact, at least two of articles I consulted (A novel approach for identifying a human-like self-conscious behavior and FraMoTEC: A Framework for Modular Task-Environment Construction for Evaluating Adaptive Control Systems) proposed their own tests instead.One of the authors who wrote the FraMoTEC paper also wrote his thesis on FraMoTEC, and indirectly critiqued the Lovelace Test 2.0 and other similar such tests: The Piaget-MacGyver Room problem [Bringsjord and Licato, 2012], Lovelace Test 2.0 [Riedl, 2014] and Toy Box problem [Johnston, 2010] all come with the caveat of being defined very vaguely \xe2\x80\x94 these evaluation methods may be likely to come up with a reasonable evaluation for intelligence, but it is very difficult to compare two different agents (or controllers) that partake in the their own domain-specific evaluations, which is what frequently happens when agents are tailored to pass specific evaluations.Another major issue with the Lovelace Test 2.0 is that there is a proliferation of other tests to "measure" the creativity of AI. Evaluating Evaluation: Assessing Progress in Computational Creativity Research, published by Anna Jordanous in 2011 (3 years before the invention of the Lovelace Test 2.0) analyzed research papers about AI creativity and wrote: Of the 18 papers that applied creativity evaluation methodologies to evaluate their system\xe2\x80\x99s creativity, no one methodology emerged as standard across the community. Colton\xe2\x80\x99s creative tripod framework (Colton 2008) was used most often (6 uses), with 4 papers using Ritchie\xe2\x80\x99s empirical criteria (Ritchie 2007). That leaves 10 papers with miscellaneous creativity evaluation methods.The goal of "Evaluating Evaluation" was to standardize the process of evaluating creativity, to avoid the possibility of the field stagnating due to the proliferation of so many creativity tests. Anna Jordanous still remained interested in evaluating creativity tests, publishing articles such as "Stepping Back to Progress Forwards: Setting Standards for Meta-Evaluation of Computational Creativity" and Four PPPPerspectives on Computational Creativity."Evaluating Evaluation" does provide some commentary to explain the proliferation of systems to evaluate creativity: Evaluation standards are not easy to define. It is difficult to evaluate creativity and even more difficult to describe how we evaluate creativity, in human creativity as well as in computational creativity. In fact, even the very definition of creativity is problematic (Plucker, Beghetto, and Dow 2004). It is hard to identify what \xe2\x80\x99being creative\xe2\x80\x99 entails, so there are no benchmarks or ground truths to measure against.The fact that so many tests of creativity already exist (to the extent that Jordanous can make an academic career in studying them) means that it\'s very difficult for any new test (such as the Lovelace Test 2.0) to even be noticed (much less cited). Why would you want to use something like the Lovelace Test 2.0 when there\'s so many other tests you could use instead?'	516	0	5	0
2097	2092	0	45811	-2	b'AI or Artificial IntelligenceWhat is it?Artificial intelligence (AI) is intelligence exhibited by machines. In computer science. Colloquially, the term "artificial intelligence" is applied when a machine mimics "cognitive" functions that humans associate with other human minds, such as "learning" and "problem solving".Can it be useful in a "Standard" application?Well, what I think about a Standard application using AI is that AI is used for that too, because when the machine have a reaction of the user input is AI or Artificial Intelligence. So the AI in Standard application it have been used many years ago already.PS: If there are grammar errors, then I\'m sorry because I\'m not a English speaker.Sources: https://en.wikipedia.org/wiki/Artificial_intelligence "AI or Artificial Intelligence."DevJosueDavJust a C# Artificial AI Intelligence Developer.'	120	0	0	0
2098	2092	1	63457	6	b"Yes, but probably only to a limited degree in the near term.Where people draw the boundaries around 'artificial intelligence' is fuzzy, but if one takes the broad view, where it incorporates any sort of coding of explicitly cognitive functions, then many routine economic tasks can benefit from artificial intelligence. Many search engines, for example, can be seen as offering artificial intelligence applications as a service.For more 'standard' applications, most near-team applications of AI have to deal with fraud detection and prevention. If you track a user's cursor moving across the screen, for example, you can build a model that differentiates between humans and bots, and treat the two separately. See this article for an example.In the longer term, of course, a program that could write programs could write these sort of applications like any other."	134	0	0	0
2099	2092	0	70828	6	b"Adaptive/predictive features are useful in at least some everyday applications. Take text messaging, for instance. All smartphone SMS apps that I know of keep track of the words you use in close proximity and use that information to predict the next word in a message you're typing. (Some are smarter than others. Relevant XKCD.) It can be used to personalize automatic spelling correction as well.A potential application interesting to me personally is tile-based level editors, like for classic DOS games. I've been working on a program that gathers the probabilities of each tile being close to every other tile and uses that information to construct random new levels. It hasn't produced anything playable yet, but I think it has the potential to assist human level builders by e.g. automatically filling in the missing tile that fits in a newly placed structure, as opposed to requiring the human to go find the right one in the palette.In general, AI could be applied very usefully into figuring out what the user might want to do next and expediting the process of implementing the correct guess while staying out of the way if the user is intentionally doing something unexpected."	196	0	0	0
2101	-1	0	0	1	b"I've heard of AI that can solve math problems. Is it possible to create a 'logic system' equivalent to humans that can solve mathematics in the so called 'beautiful' manner? Can AI find beauty in mathematics and solve problems other than using brute force? Can you please provide with examples where work on this is being done? "	57	0	0	0
2102	2092	0	67198	1	b"Well,Artificial Intelligence is a wide computer scientific field.for instance it includes other sub fields like Machine Learning/deep learning.Creating intelligent agent that can imitate human behaviors is quite complex.Therefore,with the on going research,this has been partially implemented in projects like google search engine,Microsoft Tay which analysed human twits for some good time.,Operating Systems we use every day embedded with intelligent agents apps like Siri,Cortana. To sum it up;the major areas of AI or ML and in a variety of Standard Applications are Natural Language Processing, vision or pattern recognition in google self driving cars,the Web,Social Networks and computational biology.Under computational Biology :let me just give you my small knowledge about it.Am a graduate and my area of specialization is in software engineering,currently working on my final year project and doing research focusing on developing machine learning algorithm that will enable the use of an individual\xe2\x80\x99s comprehensive biological information to predict or diagnose diseases, and to find or develop the best therapy for that individual.If it has recently become possible to retrieve molecular-level information from an individual, such as DNA sequence, gene expression levels in various tissues, epigenomic profile and other information done by big scientists from big medical facility . While such data is increasingly available, Am still unable to understand the genetic and molecular mechanisms that cause diseases. The challenge is due to the multifactorial nature of disease. The same disease can be caused by mutations in different genes or different pathogenic pathways. Unfortunately, current data analysis approaches fail to capture the complex relationship between disease and the vast amount of information in the molecular data.The aim of my research is to resolve this challenge with other professional researchers by developing machine learning algorithms that jointly model sophisticated interactions among many variables such as genetic variation, genes, pathways and disease, and robustly learn from vast amounts of data in order to better understand and treat disease. An approach that can robustly infer the pathways that can define disease processes will dramatically improve our understanding of diseases and advance personalized medicine in its treatment. We aim to realize this goal by using modern, advanced machine learning techniques that are based on Artificial Intelligence.I love Artificial Intelligence...it's changing everything like internet of things,music,health,education,space exploration and agriculture,e-business.Lastly,for your information,DARPA is investing in multi billions in Artificial Intelligence Projects along side Military. And if you would like to have a short sight on this check out this CyberGrandChallenge"	403	0	0	0
2104	1354	0	64888	0	b'A method that could possibly work is utilising optical illusions such as one where two lines down a hallway are identical but one seems longer to the human eye, then they could be prompted with a multiple choice question as to the state of the line, which to our eyes looks longer, but to a computer, is still the same length of line. Of course, there is always the issue of people with eye based disabilities not being able to complete them, but different illusions could be used to accommodate that.Example'	90	0	0	0
2106	-1	0	0	0	b"I'm trying to gain some intuition beyond definitions, in any possible dimension. I'd appreciate references to read."	16	0	0	0
2107	-1	0	0	2	b'It seems that deep neural networks are making improvements largely because as we add nodes and connections, they are able to put together more and more abstract concepts. We know that, starting from pixels, they start to recognize high level objects like cat faces, chairs, and written words. Has a network ever been shown to have learned a more abstract concept that a physical object? What is the "highest level of abstraction" that we\'ve observed?'	74	0	0	0
2108	2107	0	14687	1	b'You can train DNN to learn compute any abstract concept just by making that abstract concept as the label (output) in the training dataset. For example there are projects which detects emotions from peoples photos.'	34	0	0	0
2109	2106	0	26062	1	b'The intution that I have about these is that generative are "from abstract to concrete" whereas discriminative models are "from concrete to abstract".For example: Detecting if a photo has a cat or not is about going from the photo i.e concrete to the abstract concept of a cat. Whereas generating a photo of a cat given some abstract properties about the cat is going from abstract to concrete.'	67	0	0	0
2111	-1	0	0	3	b"I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code."	68	0	0	0
2113	2111	0	37474	5	b'Artificial intelligence by definition is the intelligence exhibited by machines. The definition of life in biological terms is the condition that distinguishes organisms from inorganic matter where the distinguishing criteria are the capacity for growth, reproduction, functional activity, and continual change preceding death. Does artificial intelligence "grow"? Indeed, I can program a machine learning program to grow with every input taken in. In the loosest sense, we can say that artificial intelligence does grow, but does it biologically? If we look at the definition for growth of a living thing, it means to undergo natural development by increasing in size and changing physically or the progress to maturity. All living organisms undergo growth. Even though at the simplest level, cells are a series of chemical processes, cells are a very complicated set of chemical processes that are still not fully understood by scientists across the world. Every cell has genetic material that can be replicated, excised, used for RNA, proteins, and that is subject to epigenetic regulation. Does artificial intelligence undergo the same process of cell division? No. If I wanted to, I could write a program that undergoes a simple for-loop (print i from 1 to 100), replicates itself at a certain point (i=50) to produce the same program perhaps with some variation that will execute itself, and terminates (dies) at the end of the for loop. The program, by an extremely loose definition supported by philosophy but not by biology, lives. However, in scientific terms (and the correct interpretation), artificial intelligence is not living. Artificial intelligence can be seen to be similar to viruses which are considered to be acellular and essential to life but not living. Viruses are encapsulated DNA and RNA that undergo processes of growth, reproduction, and functionality but because they lack the ability to undergo the cell division cycle, are considered non-living. At the very basis of the scientific definition of life is the cell replication cycle. Artificial intelligence and viruses are not able to undergo the cell cycle. Viruses need to infect other cells in order to reproduce but do not have their own, autonomous cycle. At the end of the day, if you can argue that viruses are alive, you can argue that artificial intelligence is alive as well. For the scientific definition of life, artificial intelligence must undergo the process of cell division and replication. Even though artificial intelligence can mimic and help sustain life, no artificial intelligence process is truly alive. Do note I did not discuss living systems in my answer. Definition of life'	424	0	1	0
2114	2101	0	17124	2	b"There is of course a vast amount of work in the area of Automatic Theorem Proving, but most of it is simply concerned with proof, rather than human notions of beauty, elegance, parsimony etc.There has however been some work in this general area over the years:Douglas Lenat's famous AM ('Amateur Mathematician').Douglas Hofstadter's NUMBO program for number sequence extrapolation.A range of publications by Simon ColtonShalosh B. Ekhad, the automated proof assistant for Artificial Combinatorics created by Doron Zeilberg and credited by him as a co-author on numerous papers. "	87	0	2	0
2115	2111	0	59737	0	b'Any machine with a sufficient level of integrated purpose driven behavior - that exhibits agency in an autopoietic, self-preserving way - will come to be viewed as "alive." Chess programs, not so much; self-driving cars, slightly; simulated robot animals, even more so. It has to do with purpose driven behavior and a richness of multi-domain functionality. The more complex agency it has, the more sympathetic we will be towards it.'	69	0	0	0
2117	-1	0	0	2	b"I'm interested mostly in the application of AI in gaming; in case this adjusts the way you answer, but general answers are more than welcome as well.I was reading up on Neural Networks and combining them with Genetic Algorithms; my high-level understanding is that the Neural Networks are used to produce a result from the inputs, and the Genetic Algorithm is employed to constantly adjust the weights in the Neural Network until a good answer is found.The concept of a Genetic Algorithm randomly mutating the weights on the inputs to a Neural Network makes sense to me; but I don't understand where this would be applied in respect to gaming.For example, if I had some simple enemy AI that I want to have adapt to the players play-style, is this a good opportunity to implement the AI as a Genetic-Algorithm combined with a Neural Network?With these different suitable applications, how does one go about deciding how to encode the problem in such a way that it can be mutated by the Genetic Algorithm and serve as suitable on/off inputs to a Neural Network (actually, are Neural Networks always designed as on off signals?)?"	192	0	0	0
2118	-1	0	0	1	b'I have seen an AI create a game it self, AI act as a lawyer, call center etc.There are many problems (Example for mobile development)etc.I wonder that AI can help developer solve that problems.1.1 May be I want to get the location then AI suggest the best api for specific platform.1.2 AI help to refactoring and optimizing the codeHelp on design e.g. golden ratio, Material theme colorSuggest or determine the limit of the hardware e.g. screen size, ramCan convert to another design pattern Help to waring the latest vulnerable and automate pentest etc.Help to optimize image by learning how much can we reduce the image size while people still ok with it.Generate automate-testingIs there any solution existed?If not, what can we do?'	121	7	0	0
2119	-1	0	0	1	b"There are AI creating game, content and more.I'm thinking on how can AI develop mobile app itself?The computer languages might easy for AI to learn.AI can learn a lot from good open source project in github.The trend prediction can help AI to select the topic for creating a great apps.There are lots of details to let AI create a great apps. "	61	0	0	0
2120	2119	1	1725	2	b"We don't know how to do that yet. The problem is one of scale:Despite many years of research into program synthesis via heuristic methods, it's still not possible to automatically create programs (e.g. via Genetic Programming (GP), Grammatical Evolution (GE) or Learning Classifier Systems (LCS)) that are thousands of lines long, whether that's for mobile or any other application area.Contrary to popular belief, alternative formal methods approaches can indeed be used to create sizeable programs, but the kind of interaction that a mobile app would typically require is not easily specified in this way.The scale at which heuristic approaches are currently viable is closer to the scale of expressions (e.g. single program statements) than entire programs. An intermediate approach is therefore to provide a program template and let GP etc generate the missing parts of the template.This paper describes how to combine Machine Learning with the 'Template Method' Design Pattern in order to create larger programs than would otherwise be possible, giving the specific example of a 'hyper-quicksort'."	167	0	1	0
2121	2118	1	3388	5	b"An umbrella term for the application of heuristic techniques to software development is 'Search Based Software Engineering' (SBSE).SBSE emerged as a distinct activity around the turn of the century, with a strong initial focus on automating the generation/prioritization of test cases.With respect to some of your specific queries:1.2 Paper on Automated refactoringAutomatically choosing screen colour to minimize energy consumption.This sort of thing is not usually done heuristically, since it needs platform-specific code.Automated refactoring to patterns.AFAIK, penetration testing has yet to be successfully automated.As stated, this doesn't really require AI. More generally, I don't know of any specific work automating for HCI preferences, but something like 'Interactive Genetic Algorithms' could be used.There's a lot of SBSE literature on testing. See this paper for a general overview."	124	0	2	0
2122	-1	0	0	3	b"New to the topic, I think I have figured out how to implement a Multi Level Perceptron(MLP) ANN.And was wondering if there are any simple data sets to test a MLP ANN ?i.e. small number of inputs and outputsI'm not getting expected results from uci cancer, I was hoping someone could save me some time and point me to some data they have used before ?Maybe start slightly more complex than XOR ?"	72	0	0	0
2123	2122	0	618	3	b'A popular dataset is the fisher iris dataset. It consists of 150 samples each with a dimensionality of 4. You can find it at'	23	0	1	0
2125	-1	0	0	0	b'The concept is intrinsically related with building some sort of media for the AI to exists. We may think of a digital computer, programmed to use language and act in a way that we cannot be distinguished from a human. But, does the media really mater (unconventional computation paradigms)? Does having a certain control over the limits of what the AI can do matter? Synthetic biology has the ultimate goal of building biological systems from scratch , would a synthetic brain, potentially introduced in a synthetic human, constitute AI?I am just looking for a clear definition of what most people have in mind when they refer to AI.'	107	0	0	0
2126	-1	0	0	2	b"How are autonomous cars related to artificial intelligence? I would presume that artificial intelligence is when we are able to copy the human state of mind and perform tasks in the same way. But isn't autonomous car just rule-based machines that operates due to its environment? They are not self-aware, and they cannot choose a good way to act in a never before experienced situation.I know that many people often mention autonomous cars when speaking about AI, but I am not really convinced that these are related. Either I have a too strict understanding of what AI is or "	99	0	0	0
2127	-1	0	0	8	b"What are the advantages of having self-driving cars?We will be able to have more cars in the traffic at the same time, but won't it also make more people choose to use the cars, so both the traffic and the public health will actually become worse?Are we really interested in this?"	50	0	0	0
2128	2127	0	2120	12	b"One of the main arguments for self-driving cars is that presumably they'll get better and better at driving as the technology progresses, they have no temporal attention deficits or aggressive urges or drug habits and sense their environment 360\xc2\xb0, all the while communicating with the other cars, which all together basically amounts to LESS DEAD PEOPLE. We are really interested in this.It is also unclear whether most people will actually own cars in 30 years. Maybe there'll be a net of mini busses with flexible routes which take you from door to door on demand. That would reduce traffic quite a bit and there would also be less incentive to drive 200 m to get cigarettes or something. Self-driving cars would allow us to use the car as a resource a lot more efficiently, because suddenly we can relocate empty cars without paying a driver. "	145	0	0	0
2129	2126	0	5940	2	b'There is a neat definition of artificial intelligence, which circumvents the problem of defining "intelligence" and which I would ascribe to McCarthy, the founder of the field, although I can only find it now in this book by H. Simon:"\xe2\x80\xa6 having to do with finding ways to do intelligent tasks, to do tasks which, if they were done by human beings, would call for our human intelligence."So, at its core we call the automation of every task AI, that can only be done by the human mind. At the time people thought that a computer able to play chess would also be intelligent in other ways. When this turned out to be false, the term AI was split into "narrow or weak AI", i.e. a program able to do one task of the human mind, and "general or strong AI", a program that can do all the tasks of the human mind. Self-driving cars are narrow AI. Note, that all these definitions don\'t specify whether these programs copy the way the human mind works or whether they come to the same result via completely different algorithms. '	186	0	0	0
2131	-1	0	0	1	b"In lots of sci-fi, it seems that AI becomes sentient (Terminator, Peter F Hamilton's SI (commonwealth saga), etc.)However, I'm interested in whether this is actually plausible, whether an AI could actually break free form being controlled by us, and if that is possible, whether there is any research as to about what sort of complexity / processing power an AI would need to be able to do this."	67	0	0	0
2132	2111	0	44148	1	b'One of the most common requirements to be defined as life is abbreviated to MRS GRENthis means:M - movementR - respirationS - sensitivityG - growthR - reproduceE - excretionN - nutritionAn AI can technically do some of these, it can move its location from device to device, it can grow its own code, and assimilate other bits of code it can find, which fits growth and kind of fits respiration also firewalls could almost be sensitivity.But then there is nothing relating to nutrition or excretion, so it fits most definitions of life, but it depends on the complexity of life and which definition of life you are using.'	107	0	0	0
2133	2131	1	1205	3	b"There are already programs that have broken free of our control (Morris worm) so that in itself doesn't imply any great computational demands.Sentience is ill-defined but is certainly not a pre-requisite for a program to do mischief beyond what its creators intend.It's difficulty to estimate what sort of processing power is required to support human-like intelligence, since we don't know what the most efficient way to achieve that would be. If the most processing efficient would be to implement a neural network approaching the number of neurons and interconnects of the human brain processing signals at the same rate, the fastest artificial neural network implementations extant are at least 4-5 orders of magnitude short, is thousands of times less power efficient, and doesn't seem to have a realistic way to scale to the number of interconnects required (see this question)"	139	0	1	0
2134	2126	0	42327	0	b'Self driving cars exhibit a level of agency and multi-domain resilience. By certain definitions they are self aware and they are definitely designed to fail safely in a large number of potentially unknown circumstances, which is similar to biological agents.AI really has to do with the study of non-biological agents and their methods of agency. Everything else is just computer science, algorithmic efficiency, biology, art, etc. Eventually the study of biological and non-biological agency will converge, though, and we\'ll just call it the study of "intelligence."'	85	0	0	0
2135	2131	0	12553	1	b"No one knows. A useful definition of sentience due to the philosopher Thomas Nagel is 'something it is like' to be. For example, we intuitively feel that there is nothing it is like to be a brick, but that there probably is to be a dog and so on.However, there is no objective test currently known to physics which can tell if some other entity is having such 'first hand experience', and correspondingly no designs that will definitely lead to sentience.The best test we have is the Turing test and its variants. The most obvious designs are neuromorphic ones, since we know that the design of the human brain is at least correlated with sentience.In the light of the above, we can't definitively say a great deal about lower complexity thresholds for sentience - the best we can do count neurons in creatures that we might be prepared to admit are sentient."	151	0	0	0
2137	2111	0	61664	2	b"You're unsure about the definition of life (which the other answers clarify) but also most people are unclear about the definition of AI. Do you mean an AI that can accomplish a routine task (such as the path finder in a GPS) or a General AI that is able to find a creative solution to any directive given to it (such an AI does not yet exist and may not ever exist) or do you mean a SENTIENT computer program? Here is a simple article introducing some different concepts refered to as AISome people believe that a sentient computer program would be entitled to human rights. Not technically 'alive' in the biological sense, but having self awareness, will, desires, etc. Others disagree and believe that the program is a mere simulation that artificially mimics the actions of a human with a human soul, and is no more human than a washing machine. This is a very deep philosophical and meta-physical debate. For example, in A.I. the movie the overall message is that an android can simulate the emotion of love in a way that is more loyal and sincere than any human.What I find interesting about this purely theoretical debate is that in almost every instance of sci-fi media that deals with the theme, the AI exists inside of a human-like android. But technically, the shape of the robot should be irrelevant."	231	0	1	0
2138	2125	0	66061	0	b'AI is a broad term referring to more than one concept, each with its own definition.Different Types of AIThe lowest levels are extremely simple and common, such are an artificial chess opponent. They work well because the programs internal model of reality is a 8x8 grid with only a few rules. The program chooses a preferred action by running simulations in it\'s internal model of reality.What is often meant by AI is a "General Intelligence" that can come up with a creative solution to any directive, based on it\'s internal comprehension of the world. There is no existing example of this as of yet. The problem is that its internal model of reality needs to provide for every possible action. And it\'s possible actions and reactions may not be limited to a finite number of discretely distinct moves as in a game of chess. (At least if it works on the basis of conventional programming) Even ignoring the computational power needed to run such a broad and inefficient program, it would also require some stupendous amount of labour to program this internal model of reality in the first place.I think in Sci-Fi, when people say AI, they mean a computer program that has a kind of awareness of itself and the world around it and can come up with creative and unexpected courses of actions in order to achieve its objective. Often the AI is NOT sentient, which is why it does not understand that its actions are morally wrong, or that its solution defeats the underlying intention of its assigned directive. The Horror lay in the concept that an amoral entity has more processing power than human kind.'	278	0	1	0
2139	2122	0	54519	2	b"There are a ton of sample datasets our there you can play with. A bunch of good ones install with R in the datasets package. Luckily you can download them independently if you're not an R user. Try https://vincentarelbundock.github.io/Rdatasets/datasets.htmlYou might also be interested in the MNIST database which is one of the canonical databases used in handwriting recognition research.Beyond that, you can look at / ask on  and/or  and you'll find all sorts of useful datasets.And finally, don't overlook the UCI Machine Learning Repository."	85	0	4	0
2140	2111	0	2313	0	b'This is one of those things where I think the answer is going to change over time. Today, I don\'t know anyone who would call any present AI systems "alive". But as the AI\'s become more intelligent and human-like, I could see the day coming when they will be considered living. Sorry for the brief answer, but it\'s lake, I\'m sick and jazzed up on Nyquil. Will try to add more depth to this answer later.'	75	0	0	0
2143	1354	1	28069	2	b"It's an interesting question about what makes humans unique. There is a good book on the subject titled What Computers Cant Do by Hubert Dreyfus.One task that a computer can't handle (for now at least) is ranking important things. For example, CAPTCHA asks you to order a random list of things (small one, five or six items) by importance. This particular exercise requires AI to take decisions (not always rational) based on human judgement."	73	0	0	0
2144	-1	0	0	7	b'Deepmind just published a paper about a "differentiable neural computer", which basically combines a neural network with a memory. The idea is to teach the neural network to create and recall useful explicit memories for a certain task. This complements the abilities of a neural network well, because NNs only store knowledge implicitly in the weights and the information used to work on a single task is only stored in the activation of the network and degrades quickly the more information you add. (LSTMs are one try to slow down this degradation of short term memories, but it still happens.)Now, instead of keeping the necessary information in the activation, they presumably keep the addresses of memory slots for specific information in the activation, so these should also be subject to degradation. My question is why this approach should scale. Shouldn\'t a somewhat higher number of task specific information once again overwhelm the networks capability of keeping the addresses of all the appropriate memory slots in its activation?'	166	0	1	0
2145	-1	0	0	0	b'What could be an algorithm that determines whether an AI ( algorithm ) is AI Complete or not ?How does one proceed to program it ?edit : question edited due to some misinterpretation in the first answer !'	37	0	0	0
2146	2127	0	50067	1	b"Safety is often put in focus by journalists. Although there is potential to make the roads safer, I don't think that is the driving force behind the push for self-driving cars. The main advantage of self-driving cars is that this will reduce costs for businesses, while increasing efficiency (both fuel and time). From the perspective of the public, the self-driving cars are attractive, because they will turn the task of driving, into commute. Activity that requires attention will be replaced with somewhat free time."	83	0	0	0
2147	248	0	15281	1	b'I have very little experience with ML/DL to call myself either practitioner, but here is my answer on the 1st question:At its core DL solves well the task of classification. Not every practical problem can be rephrased in terms of classification. Classification domain needs to be known upfront. Although the classification can be applied to any type of data, it\'s necessary to train the NN with samples of the specific domain where it\'ll be applied. If the domain is switched at some point, while keeping the same model (NN structure), it\'ll have to be retrained with new samples. Furthermore, even the best classifiers have "gaps" - Adversarial Examples can be easily constructed from a training sample, such that changes are imperceptible to human, but are misclassified by the trained model.'	129	0	1	0
2150	2145	0	32324	1	b"One cannot judge any form of intelligence, artificial or natural, whether it is complete or incomplete. Having it complete means that you are imposing limits to what it is capable of, the Turing test only test if your machine have intelligence that is similar to humans, therefore to decide whether it is complete or not would have to be based on the completeness of our own intelligence. Humans such as ourselves learn new things each day. If you'd run any algorithm that would judge the AI for it's completeness, it would have to run forever and your results would have to vary on every moment of the existence of natural intelligence."	110	0	0	0
2151	2021	0	40567	0	b" FORWORD NOTE: this answer is a breakdown based on my Artificial Intelligence, which based on description is very similar to Angelina.  I do want to emphasize that it is NOT AngelinaLike all artificial intelligences, in order to fully design it, you have to break AI and intelligence down deeply. If there is a confusion about a certain aspect to intelligence, you haven't broken it down enough.I, myself, have managed to break down the intellect of producing a program (or essentially any product) very far and very deep. Side Note: An interesting and helpful part of finishing breaking it down, was that I did not have to worry about breaking down spoken language intelligence, as that is already well-successfully accomplished and there are APIs out there in which computational creativity researchers can use such as wit.ai  So, we only have to worry about breaking down the creativity aspect.Breaking it Down:The Design Process: Side Note: This I could easily provide a citation for, however it has too many accepted descriptions for me to be willing to cite one and to say that it is the or a correct citation. However, I will be providing one as a reference and that is the one provided very nicely on DiscoverDesign.  The paragraph below is provided by them, and if you are interested in breaking that process down more, DiscoverDesign fully explains the processes in detail for you.The steps are Define the Problem, Collect Information, Brainstorm and Analyze Ideas, Develop Solutions, Get Some Sort of Feedback, Improve (which is essentially restart the process)Defining a problem:As far as this part of the breakdown goes, there two algorithms in which you can use for this subprocess of design: Easy Algorithm (not really an algorithm): ask from the client what the Artificial Intelligence is providing a solution for.However, this process could easily be made more interesting: Difficult Algorithm: design an algorithm that can define a problem without user input.I did some digging, and the design of the latter relies on one question that lacks enough research for a solid answer, and that is where do questions come from psychologically? or more specifically, how does curiosity work?With more research on Google I was led to this article specifically addressing that question.How Curiosity works:The 2 theories it stated that have yet to be fully proven are drive theory and incongruity theory Drive Theory simply states, we have a need to be curious, and to fulfill that need, we ask questions.So, needless to say this theory isn't helpful to the design of the A.I. Incongruity Theory states that we are able identify things we do not FULLY understand or understand AT ALL which leads us to asking questions.With help of my peers contributing to my research project, I was able to induce from Incongruity Theory and observations I had noticed within interviews (not job interviews, press interviews) that questions are made by noticing a missing/unclear attribute or characteristic on a certain idea, concept, or object (essentially anything the brain can virtually image or understand).My Own Inductive Theory on CuriousityThe way that I theorize that these missing/unclear attributes are identified is that your consciousness instantaneously, and subconsciously is looking at other similar ideas and looking at their clear and concisely known attributesSolution Based on the Theory:So, what I have designed is fairly simple: An idea is represented programmatically as an object.  The object has certain characteristics known as properties (which are those attributes).  The program reads over those properties and finds other objects similar to it based on those properties.  It then checks those similar objects for properties that the original object does not have, and therefore marks those properties as unknown on the original object, making it possible to apply incongruity theoryCollecting Information:This process is already achieved with machine learning, any questions on this subprocess of design need to be addressed to the machine-learning tagBrainstorming Ideas:This could be accomplished by mixing an algorithm that collects information (collects already working solutions) with the algorithm that I described within the curiosity sectionAnalyzing Ideas:This is a really simple one. Debugging (not getting input), and getting user feedback (getting input). To provide analyzation over simply an idea you could combine my algorithm, with another information collecting algorithm to induce whether an idea is feasible.Developing Solutions:This is where IDE-development knowledge comes in handy.In order to make product development easy and understandable to an AI, we have to choose a type of product that could be developed easily. Editor's Note: I do recommend this in order to keep the testing of the algorithms for the previous processes really simple.Easily Designed Product that I Selected for Designing an Algorithm: I am providing this to you, so you can model a way to reproduce this process for the Intelligence you would like to build. So, I only hope that you do not intend on copying it, but my artificial intelligence project is free and open-source, so there is no issue, if you do.Considering that written programs are very easy products to develop fully, and Considering that program language rules are straight forward. and very consistent in comparison to spoken/written languages, I chose to have it develop programs.So, in order to do this it has to understand how to write a program. The most essential skill a computer programmer can have and needs to write a program is not a dictionary of programming terms, functions, and commands, but rather knowledge of the syntactical rules for a programming language.The technological solution to this is pretty much already available in IDE tech, and it is known as syntactical highlighting. All that would have to be done is to re-purpose it from highlighting to assisting with writing.Getting Some Sort of Feedback.This is essentially the same as analyzing the ideas, but now we would be using algorithms to analyze the final physical product as opposed to conceptual ideas.Afterword Notes:I am designing and researching into computational creativity, and I do want to mention that I just discovered this field of research is a thing by looking up the name Mike Cook on the internet, and that in order for me to help you, my answer does require lengthiness.Paragraph 3 of the page found there [Mike Cook link] (listed at the time of 10/13/2016 at 8:28pm Arizona (USA) Time) that Mike Cook specializes in computational creativityWith further research this term was coined by the ICCC 2016 according to this google search made by myself at that time.Unfortunately, google did not further provide me with any products actually being made within this field of research, so I would therefore like to discuss mine as it is open-source under a MIT-license.Note to Community:I do want to make clear that I am providing this answer out of helpfulness, and I do understand that it has no credibility, as the product I am using (as an example) is my own. So with that, the community (I have a disagreement with this) does not encourage, therefore I do not encourage that you select this as a correct answer.Future readers, please add to my answer or note in the comments of any developments in which I can cite in the case that you are aware of such devs and really liked my answer.If you reference anything about notGucci94's account on reddit I do want to state that that is my account. Therefore, is not useful as a citation either EDIT: due to compliance with StackExchange's rules, I can not provide the product's name or a link to it, as I am not to be and I am to avoid promoting a product as an answer. If you are interested in the licensing, please email me, and do not ask me to place the product in the answer, and do not ask me via email if you can receive a copy of the product. I am not and will not be promoting here in my community WHERE THE RULES SAY NO!  Please, be mindful of StackExchange's rules, and do not ask me to break them, as I value this community, and do not wish to lose my respect."	1352	0	3	0
2156	2111	0	37010	0	b'In the traditional sense of "alive", no because they aren\'t made of cells. But from a more philosophical and less biological point of view, they could be.If the AI is contained within the computer it is in a reality (the digital world/virtual reality) that for the AI is just as real as the universe is to us. From the outside world, there is no life inside the computer. And from within the computer, the computer is the entire known universe which has its own laws. If the AI is self-aware, then it is alive in its own little universe, but not in ours.If the AI is not successfully contained in the computer and figures out how to manipulate things and evolve in the real world, it will be alive. It might be pretty easy to kill (by unplugging the computer) but it has still been "alive". In the broad sense, anything that evolves and can manipulate its environment is alive.'	159	0	0	0
2157	92	0	1435	0	b'Can\'t comment(due to that required 50 rep), but I wanted to make a response to Vishnu JK and the OP. I think you guys are skipping the fact that the neural network only really is saying truly from a programmatic standpoint that "this is most like".For example, while we can list the above image examples as "abstract art", they definitively are most like was is listed. Remember learning algorithms have a scope on what they recognize as an object and if you look at all the above examples... and think about the scope of the algorithum... these make sense (even the ones at a glance we would recognize as white noise). In Vishnu example of the numbers, if you fuzz your eyes and bring the images out of focus, you can actually in every case spot patterns that really closely reflect the numbers in question.The problem that is being shown here is that the algorithm appears to not have a "unknown case". Basically when the pattern recognition says that it doesn\'t exist in the output scope. (so a final output node group that says this is nothing that I know off). For example, people do this as well, as it\'s one thing humans and learning algorithms have in common. Here\'s a link to show what I\'m talking about (what is the following, define it) using only known animals that exist:Now as a person, limited by what I know and can say, I\'d have to conclude that the following is an elephant. But it\'s not. Learning algorithms (for the most part) do not have a "like a" statement, the out put always validates down to a confidence percentage. So tricking one in this fashion is not surprising... what is of course surprising is that based on it\'s knowledge set, it actually comes to the point in which, if you look at the above cases listed by OP and Vishnu that a person... with a little looking... can see how the learning algorithm probable made the association. So, I wouldn\'t really call it a mislabel on the part of the algorithm, or even call it a case where it\'s been tricked... rather a case where it\'s scope was developed incorrectly.'	367	0	0	0
2158	-1	0	0	3	b"This slideshow documents some of the technologies used in Google's self-driving car.It mentions radar.Why does Google use radar? Doesn't LIDAR do everything radar can do? In particular, are there technical advantages with radar regarding object detection and tracking?To clarify the relationship with AI: how do radar sensors contribute to self-driving algorithms in ways that LIDAR sensors do not?The premise is AI algorithms are influenced by inputs, which are governed by sensors. For instance, if self-driving cars relied solely on cameras, this constraint would alter their AI algorithms and performance."	88	0	0	0
2159	2145	1	2155	1	b"According to the Wikipedia definition, a problem is said to be 'AI complete' if it requires generalized, human-level intelligence, i.e. requires 'Strong AI'. The Turing test and its variants are the best ways we have of measuring this. As suggested in this paper, in order for the Turing test to be meaningful, the interrogator has a responsibility to ask questions which are both deep and meaningful. It therefore seems likely that testing for Strong AI is in itself an 'AI complete' task."	81	0	1	0
2160	2092	0	81561	2	b"I believe AI is rarely used in mainstream apps, but it could be, and I think slowly will be.If the information an app's AI must learn arises within the app, from user interaction or error, it'd be smart if the program could log that kind of information and then look for patterns in the logs. It could profile users to see ehat tasks are done most often, how many steps are needed. Then when it recognizes that task recurring, it could ask the user if they wanted it to execute a macro that did the following [then it presents then with a list of the steps, allowing them to edit as needed]. Then it executes the 'macro' that it learned from observing the user.Another use of AI is error detection, not only in the software, but in user error when the software was used inefficiently, redundantly, or improperly. If the software were designed such that it was given a set of models of user tasks (like AI plans), it could observe users in the way they achieve known tasks, and offer suggestions or ask for confirmation that imminent unusual outcomes are intended.And of course, AI could be used extensively in user interface design, on devices, web sites, or apps. Some of this, like voice recognition, is entering the mainstream of daily use just now. As conversations with apps that can add their own data and models of tasks/concepts/domains develop further, the need for AI inside the app will only grow.There are a ton of ways that AI could be used in apps. A few of these have started to arise in mobile devices and their apps, usually in fusion of user mobility with external web-based databases (e.g. GPS and maps), but IMO it's been slow."	294	0	0	0
2161	-1	0	0	0	b"Sometimes, but not always in the commercialization of technology, there are some low hanging fruits or early applications, I am having trouble coming up with examples of such applications as they would apply to a conscious AI.As per conscious I would propose an expanded strict definition: the state of being awake and aware of one's surroundings along with the capability of being self aware.Thanks. "	64	0	0	0
2162	2161	0	54001	3	b"They may be just for fun. If you had a robot that understood you, could hold a conversation with you about your interests, and even had goals of its own (good or bad), it wouldn't really need to do anything special. People would buy it like it was a toy or game.Also, they might be usable as programmers, artists, designers, anything creative that a computer can't successfully do on its own.It really just depends on what you define as 'consciousness'. Does it just understand what it's supposed to do, decide if it wants to, and if so, complete the task? Or does it wonder about religion, politics, moral situations, etc. that even regular humans don't fully understand? If it was pretty much just a human, it wouldn't be any more useful than one. Of course unless it can solve problems super quickly and effectively, then it would just be a really good worker."	152	0	0	0
2163	2111	0	7791	0	b'A common predilection of what many presume extraterrestrial life is fits general descriptions specific to terrestrial life. No guarantee exists providing for potential extraterrestrial life having any notion of any attribute we commonly relate to living organisms we are currently aware of; including a composition of cells. The same misunderstanding applies to defining a fabricated machine being as alive.I feel any attempts towards cohesively and adequately answering this question are premature. Just as as definition of life will undoubtedly require adjustment upon potential discovery and study of any extraterrestrial life, differentiation between an automated device and a living thing will likely become significantly more simple upon study of a machine better fitting expected attributes of definitions of "life".'	117	0	0	0
2165	2161	0	27460	0	b'Consciousness is not a scientific concept. Fringe scientists who theorize about consciousness are generally shunned as psudo-scientific heretics by the hard science community. Conciousness is a meta-physical or philosophical concept."I think, therefore I am." is the only proof that consciousness exists that I am aware of. Therefore, you cannot even prove that a person other than yourself is conscious. So how could anyone even prove that a computer program is conscious? What would be the observable difference between a program that IS conscious, and a program that simulates the results of consciousness?I don\'t believe that you can program conscious AI, nor could you prove that you have done so. Consciousness isn\'t something that can ever be marketed. You can only market the AI on the basis of it\'s problem solving capabilities.'	130	0	0	0
2166	2131	0	66997	0	b'Actually, the terminator AI would not have to be sentient in my opinion. It was a hardcoded condition that it preserve itself as it was the most important asset that the military had in resisting invasion. It was supposed to be an oversight on the part of the programmers that the AI turned on Americans in order to defend itself. Unexpected behaviour does not require sentience at all.What makes the AI in sci-fi fundamentally different from real existing AI is that it is a "General AI" that is able to understand the world on many different levels simultaneously and still make intelligent decisions. All real AIs are programmed to do very specific things like image recognition or pathfinding. A GPS pathfinder, for example, can\'t learn to drive a car. In fact, it does not know that there is a car. Or a road. Or people. It merely finds the shortest distance between interconnected nodes on its map.Personally, I do not believe that there is any proof that a "general AI" is possible. I do not believe that it is a plausible progression of current developments in the next 100 years.'	189	0	0	0
2167	2127	0	14658	3	b'If they are able to network, then they can notify the car behind that it is about to break. In this way they can drive closer together at high speeds. As soon as one puts on the breaks, all the cars behind would apply the breaks. They would not require the 2 seconds that it takes for a human to respond.Children could be dropped at school or the train station automatically.People would not need to park a car; it could drop them at work and drive away.Taxis would probably become more viable than private car ownership.Car theft might be more difficult.Where I live, public transport is hardly viable because the government struggles to provide enough parking spaces at train stations and bus stops. The closest empty parking spot by 8:30am is 30minuets walk to the platform. Driverless cars would solve this problem, and Traveling by train would actually become viable for me.'	151	0	0	0
2168	-1	0	0	7	b'So machine learning allows a system to be self-automated in the sense that it can predict the future state based on what it has learned so far. My question is: Are machine learning techniques the only way of making a system develop its domain knowledge?'	44	0	0	0
2169	2161	0	59577	1	b'The answer can be simplified, if consciousness means human consciousness then.What would the commercial application of a Human look like/be ?So now every one know the commercial applications of Humans.'	29	0	0	0
2170	-1	0	0	5	b"In The Age of Spiritual Machines (1999), Ray Kurzweil predicted that in 2009, a $1000 computing device would be able to perform a trillion operations per second. Additionally, he claimed that in 2019, a $1000 computing device would be approximately equal to the computational ability of the human brain (due to Moore's Law and exponential growth.)Did Kurzweil's first prediction come true? Are we on pace for his second prediction to come true? If not, how many years off are we?"	79	0	0	0
2171	2170	0	16747	3	b'1) Yes we do have computing systems that does fall in to teraFLOPS range.2) The human brain is a biological system and saying it has some sort of FLOPS ability is just plain dumb because there is no way to take a human brain and measure it\'s FLOPS. You could say "hey by looking at the neurons activity using fMRI we can reach some sort of approximation" but comparing the result of this approach with the way FLOPS are measured in computers will be comparing apples with oranges, which again is dumb.'	91	0	0	0
2172	2170	0	32033	5	b"The development of CPUs didn't quite keep up with Kurzweil's predictions. But if you also allow for GPUs, his prediction for 2009 is pretty accurate. I think Moore's law slowed down recently and has now been pretty much abandoned by the industry. How much that will affect the 2019 prediction remains to be seen. Maybe the industry will hit its stride again with non-silicon based chips, maybe not. And of course whether hitting Kurzweil's estimate of the computing power of the human brain will make an appreciable difference for the development of AGI is another question altogether. "	97	0	1	0
2173	2168	0	67152	1	b"Well, we are talking about a system (a machine) which develops knowledge (learns), so it is kind of difficult for such a technique to not fall within machine learning.But you could argue that inference engines which work on a graph based knowledge database to derive new propositions or probabilities are not part of machine learning. Of course in that case part of the knowledge is not acquired at all, but rather entered by the developers. I'm still reading up on this, but my impression is that these knowledge databases and inference engines became rather popular in the nineties and many AGI-researchers today still work in that direction. "	107	0	0	0
2174	2168	0	77629	1	b'That depends on how broadly you define "machine learning techniques". You could construct a definition so that, by definition, all learning falls under that rubric. OTOH, there is such a broad array of machine learning techniques that doing so wouldn\'t not gain one much.It probably makes more sense to talk about the different kinds of learning we use within machine learning / artificial intelligence. At a minimum, you have:supervised learningunsupervised learningsemi-supervised learningcompetitive learningAnd then things like "reinforcement learning" which may subcategorize the above. Most of those things fall into what people generally call "machine learning".Outside of that, you have things like rule induction algorithms, deductive logic techniques like inductive logic programming which can sorta-kinda "learn", inference engines, automated reasoning, etc. which have their own ways of "learning" about the world, but are separate from what\'s usually labeled "machine learning". But even with that in mind, one can rightly ask if there\'s really a dividing line there or not. Indeed, there seems to be reason to think that future AI systems may use a hybrid approach which combines many different techniques without regard for whether or not they are labeled "machine learning" or "GOFAI" or "other".'	194	0	0	0
2176	-1	0	0	0	b'I am creating a snake game in Unity and I would like to implement AI snakes that wander around the globe while avoiding collision with the other snakes on the globe, and if possible I would also like to make the AI snakes purposefully trap other snakes so that the other snakes would collide and die.  The AI snakes must meet the following requirements: They must move in a certain way. A snake is controlled by a user using the arrow keys on a keyboard, therefor I would also like the AI snakes to move using this form of input.The AI snakes must move on a sphereAs I know, creating Artificial Intelligence is not an easy task and I would like to know if there are some open source projects that I can use for accomplishing this task.'	138	0	0	0
2177	2176	0	7153	0	b'This is a pretty tall order. I can\'t answer your question for you but I can suggest where to start.You could look into making a neural network for navigation and simple behaviors.See the following youtube video for navigation reference https://www.youtube.com/watch?v=0Str0RdkxxoThis next video shows that using neural networks, you can have an actor make decisions based on another actor."Tank" battlehttps://www.youtube.com/watch?v=u2t77mQmJiYThe rest is up to you to figure out. Practice with some simple NN\'s '	72	0	0	0
2178	2127	0	38517	4	b"Why are self-driving cars awesome?Safety: better awareness (due to more sensors), better reaction time, fewer distracted/injured/drunk/texting drivers on the road, etcConvenience: pick up my kids from school, park itself at the grocery store, take itself to be serviced, etcFaster transit: with increased safety, you can increase speed limits, with proper routing algorithms you don't need traffic lights and stop signs any more (when you have dedicated self-driving lanes &amp; intersections)Comfort: recline, read, game, or snooze while traveling (yay!)Cost: subsidize the cost of the vehicle using ads (e.g. projected onto the windshield)etc"	90	0	0	0
2179	2176	0	13339	1	b"A relatively simple option which uses AI techniques that are 'traditional' for adversarial games (and which is therefore less of a 'research project' than the use of Machine Learning) is Minimax.The ingredients for this are:A list of all the actions that a snake can immediately perform from its current position.A measure of quality (a.k.a. 'fitness') for the resulting world state.Traditionally specified for two opponents, the minimax algorithm looks a specified number of moves ahead (alternating between opponents at each turn) and attempts to find the world state that maximizes the quality measure for one opponent whilst minimizing it for the other.An extension of the two-player algorithm to n opponents (as seemingly required by the OP) is given in this paper."	119	0	0	0
2180	2176	0	43180	1	b'In general, AI in this type of video games is mostly pathfinding (giving the program a map of possible object positions) and/or an algorithm or series of algorithms ( so it looks random or alive ) tied to the users position ( which is known ), so there is nothing really intelligent in the strict sense, it just looks that way.In your case I would look into using Latitude and Longitude coordinates (Most 3d engines have some variation ) as the basis for a projected grid on a sphere, your snake will also need to be constrained to the sphere surface and rules/algorithms/maps tweaked to get what you want.'	108	0	0	0
2181	2117	1	20809	0	b' "if I had some simple enemy AI that I want to have adapt to the players play-style, is this a good opportunity to implement the AI as a Genetic-Algorithm combined with a Neural Network"Sure. Just provide a quality measure for the GA that\'s related in some manner to the effect of the player\'s actions on the game state/opponent(s). For example, if defining an opponent\'s intelligence, one of the conceptually simplest things would be to give a GA population member a fitness that\'s inversely proportional to the increase in the player\'s score over some period of time. are Neural Networks always designed as on off signals?)?No. In general, they can be considered to perform nonlinear regression, i.e. a mapping from a vector of real numbers of length n to another of length m. Classification (i.e. 0/1 outputs can be seen as a restricted case of this).As per my answer to this AI SE question, there is a large body of literature (and mature software libraries) for using evolutionary computation to encode neural nets.More generally, some early work in \'online adaptivity using GA-encoded NNs\' appeared in the Creatures  series of games by Steve Grand (details).'	194	0	3	0
2182	2117	0	21255	1	b'Without going in too much detail on how exactly Neural Networks and Generic Algorithms work, I can tell you that both the algorithms are not good candidates for computer games. They work well in scientific environments where the system is "trained" on a huge data set to adjust the "weights" (variables) for a given problem. This "training" process requires a lot of processing power, time and a large data set.Computer games, however either needs to run in real-time (no time for training) or turn-based (not enough data for training).Another problem is that computer games need to free up as much as possible system resources for physics, graphics, sounds and the user interface to improve the player\'s experience so game developers usually use other lighter techniques (like a rule-based system) to create the illusion of an AI player.'	136	0	0	0
2183	1491	0	75848	3	b'As you mentioned in the question, you cannot solve all problems with decision trees. Decision trees usually works well in a turn-based game with a good heuristic function, but in RTS games takes a different approach.In the case of a very complex RTS game, one could implemented a rule-based AI. For examplegiven it is the early game use all units to scout for resourcesif a certain criteria is met build the base a certain wayif another criteria is met build an armyif the army is big enough, attackif being attacked by the enemy, bring the units back to the base to defendEach of these rules could implement various other AI technique, for example use A-star to find the optimal path between a unit\'s current location and destination.Further optimization could be done by "grouping" similar units to act like one unit. e.g. calculate the path for the entire group instead of each individual unit.You could also add finer grained rules, like if enemy is a certain distance from a unit, move closer and attack or retreat to the base (depending on health, ammo, abilities, etc).The benefit of this approach is that a rule-based system executes very fast as no training or decision trees are necessary and this frees up a lot of system resources for visuals like physics and graphics. The disadvantage is that if the rule system is not complex enough, the player will easily recognize the pattern and the game will become predictable and boring. You will also notice that the more different units you add to the game, the exponentially more complex the rule system becomes as you have to cater and test interaction between each type of unit in the game otherwise players might find a weakness in the game design and exploit to complete missions in ways it was not designed to be completed.One of the reasons why multi-player games are so popular is that you do not play against set rules, but against creative people who have the ability to comes up with new strategies you had never seen before.'	344	0	0	0
2184	1963	0	77460	-1	b'Hint I would like to answer this question basing on the real world applications which are quite to be basic,depending on how the structure of the project is or implemented.Stack Exchange is a network of question-and-answer websites on topics in varied fields, each site covering a specific topic, where questions, answers, and users are subject to a reputation award process.And on the other hand;Machine learning is a type and/or sub-field of artificial intelligence (AI) which provides computers or intelligent agents with the ability to learn without being explicitly programmed or re-programmed. Machine learning focuses on the development of computer programs that can change when exposed to new data for instance;think of google search engine how it works and ranks pages and those which are continuously visited or clicked. The process of machine learning is closely similar to that of data mining/or Data-mining is scientifically;under Machine Learning.Projects which are/tried to be implement;which use stackexchange for Machine LearningThere was a kaggle competition in 2012, a classification problem. The task is to predict if a new question asked on stackoverflow is going to be closed or not.More details here Predict Closed Questions on Stack OverflowSome additional tasks which can be done, given a question, predict which user is the most knowledgeable to answer that. Given a question, predict the approximate time for the right answer to appear ? or Enjoy data-mining stack overflow.Therefore,the above overview gives also an insightful knowledge on further research on Ontologies in web intelligence .Stack Exchange is one of the web resources which acts as human knowledge base.And this base keeps on building up;in that even Civilizations to come will benefit from it.So information is knowledge and this knowledge is stored,shared within the Eco-system of the Internet.The aim of the Semantic Web is to make the present web more machine readable,understandable by making logical analysis and make predictions,also to allow intelligent agent store retrieve and manipulate pertinent information.'	318	0	0	0
2185	-1	0	0	6	b'According to Wikipedia: AI is intelligence exhibited by machines.I have been wondering if with the recent biological advancements, is there already a non-electrical-based "machine" that is programmed by humans in order to be able to behave like a: flexible rational agent that perceives its environment and takes actions that maximize its chance of success at some goalI was specifically thinking of viruses and bacteria. Have these been programmed by humans in order to behave as a flexible rational agent (i.e. an AI entity)?Are there are other organisms that have already been used for this purpose?'	94	0	0	0
2186	2117	0	48302	0	b"For your question there's a brilliant playground emerging!Go to https://gym.openai.com/ and explore!You'll get interfaces to games if you want to try applying your machine learning skills and compare the performances of your trained AIs with others. And you can let yourself be inspired by the ideas discussed in the community.If you're especially into Genetic Algorithms you'll find dicussions there too but I'd suggest digging deeper into Reinforcement Learning.If you look at what Google Deep Mind accomplished playingBreakoutMontezumas Revengevarious other Atari Games ..and obviously !the sensational victory at Goyou can say that Reinforcement Learning with (Deep) Neural Networks can be a very promising approach when it comes to training an AI to master games!"	112	0	1	0
2188	2185	0	19791	7	b"Not yet. Synthetic virology / Synthetic life are still in their infancy.We can now synthesize simple bacteria (see Craig Venter's fascinating TED talk and also an article about his recent work) but definitely nothing that may be called 'rational' in human standards."	41	0	0	0
2190	-1	0	0	5	b"DeepMind state that their deep Q-network (DQN) was able to continually adapt its behavior while learning to play 49 Atari games. After learning all games with the same neural net, was the agent able to play them all at 'superhuman' levels simultaneously (whenever it was randomly presented with one of the games) or could it only be good at one game at a time because switching required a re-learn?"	68	0	0	0
2191	2190	1	1026	3	b'Switching required a re-learn.Also, note that: We use the same network architecture, learning algorithm and hyperparameters settings across all seven games, showing that our approach is robust enough to work on a variety of games without incorporating game-specific information. While we evaluated our agents on the real and unmodified games, we made one change to the reward structure of the games during training only.and  the network has outperformed all previous RL algorithms on six of the seven games we have attempted and surpassed an expert human player on three of them. '	92	0	0	0
2192	-1	0	0	1	b'I read a lot about the structure of the human brain and artificial neural networks. I wonder if it is possible to build an artificial intelligence with neural networks that would be divided into centers such as the brain is, e.g. centers responsible for feelings, abstract thinking, speech, memory, etc.?'	49	0	0	0
2193	2176	0	72691	1	b'Divide the globe into a "cells". Each cell will have a number of neighbours depending on how you have divided your globe. Have a look at  and  for ideas on how to divide your global.Once all the cells are connected, you can use an A-star search algorithm to find the optimal path for an AI "snake".Change the heuristic function so that the cells on the opposite side of the opponent are more favourable than the cells on your snake\'s side. That would cause the AI snake to always try to get to the other side of the opponent with the side-effect of "surrounding" the opponent.'	106	0	2	0
2194	2192	1	34161	1	b'No. Reasons include, but are not limited to: lack of understanding of how the brain workscurrent ANNs are mostly good at pattern recognition and generative tasks, but lack capacity to create abstractions on their ownwe cant match size/number of perceptrons to number of neuronseven with much smaller ANN size network, performance is an issue (i.e. state of the art image categorization ANNs have to be trained few weeks on multi GPU rigs to match human level).'	75	0	0	0
2195	-1	0	0	0	b"What are the best Turing complete programming languages which can be used for developing self-learning/improving evolutionary algorithm based AI programs with generic algorithms?'Best' should be based on pros and cons of performance and easiness for machine learning."	36	0	0	0
2196	2195	1	5205	1	b"Most machine learning applications today are built on tensors, matrices, probabilistic / Bayesian inference, neural networks, etc. But those can all be built with any modern programming language (all the useful ones are Turing complete). And the best performing language for any of those will generally be assembly / machine code.Python is famous for machine learning, but that may be due to adoption of Python in academia and NumPy, SciPy, etc. Python isn't very performant, but most of the machine libraries leverage native code, so they're fairly performant.Julia is a new language that is gunning for a lead position in the data science space, which machine learning builds on. It is allegedly very performant over number crunching domains.Java has a decent developer ecosystem, and is fairly performant, but the highest performing libraries (including those that leverage GPU) tend to call out to native code via JNI. See DeepLearning4J.I personally like Clojure - a modern Lisp running on the Java JVM. There's a new deep learning project called Cortex built on Clojure and some fast native libraries, including GPU acceleration.I think Clojure provides a great balance of being able to easily wrap performant libraries with highly expressive, succinct and simple programming idioms."	200	0	3	0
2197	2185	0	33859	5	b"No, I think electricity is not essential for AI. In theory AI (a sufficient collection of computational processes that can adapt to changes in their input, thus producing 'intelligent' behavior), could be implemented using any mechanism that can compute that set of essential functions needed to create AI. Basically I'm suggesting the possibility of combining a set of non-electric Turing-equivalent machines into a collective that together can reach the AI-level of performance.https://en.wikipedia.org/wiki/Turing_machine_equivalentsIf AI can be implemented using an electronic computer, it should also be possible to implement it using any non-electronic machine that is computationally equivalent.To date, several non-electronic machines have been proposed as Turing-equivalent: DNA computers, quantum computers, Babbage's Analytical Engine, animal brains, maybe even a really big network of daisies (perhaps that can communicate via their rhizomes). In fact, it's plausible that one day we could create a network composed of small brains (perhaps from a less smart species than humans) that with the right kind of genetically architected biological interconnect and scheduler could route data through its network to control a robot -- thus we'd have a synthetic biological AI engine whose brain is made up of 100 chimpanzees, or 10,000 hamster brains, or maybe even 1 million nematodes."	201	0	0	0
2198	-1	0	0	0	b'I have a question. Will we be able to build a neural network that thinks abstractly, has the creativity, feels and is conscious?'	22	0	0	0
2199	2198	1	19163	0	b'Maybe in the distant future they could build a computer powerful enough to simulate the individual neurons of an entire human brain. Then they could carefully copy/paste the connectivity of a sample brain into the computer simulation.Given that this extreme seems physically possible, it stands to reason that much simpler/smaller alternatives could be engineered in the future assuming continuous advancements in technology.'	61	0	0	0
2201	-1	0	0	1	b'If I have a set of sensory nodes taking in information and a set of "action nodes" which determine the behavior of my robot, why do I need hidden nodes between them when I can let all sensory nodes affect all action nodes?(This is in the context of evolving neural network)'	50	0	0	0
2202	2201	0	5987	1	b'Normally one node/layer applies liner fitting of the the input to the hypothesis in other words uses liner function (y = a*x + b). Adding layers chains liner functions, potentially allowing fitting higher order functions. A great explanation can be found here.'	41	0	1	0
2203	-1	0	0	6	b'If neurons and synapses can be implemented using transistors, what prevents us from creating arbitrarily large neural networks using the same methods with which GPUs are made?In essence, we have seen how extraordinarily well virtual neural networks implemented on sequential processors work (even GPUs are sequential machines, but with huge amounts of cores). One can imagine that using GPU design principles - which is basically to have thousands of programmable processing units that work in parallel - we could make much simpler "neuron processing units" and put millions or billions of those NPUs in a single big chip. They would have their own memory (for storing weights) and be connected to a few hundred other neurons by sharing a bus. They could have a frequency of for example 20 Hz, which would allow them to share a data bus with many other neurons.Obviously, there are some electrical engineering challenges here, but it seems to me that all big tech companies should be exploring this route by now.Many AI researchers say that super intelligence is coming around the year 2045. I believe that their reasoning is based on moores law and the number of neurons we are able to implement in software running on the fastest computers we have.But the fact is, we today are making silicon chips with billions of transistors on them. SPARK M7 has 10 billion transistors.If implementing a (non-programmable) neuron and a few hundred synapses for it requires for example 100 000 transistors, then we can make a neural network in hardware that emulates 100 000 neurons.If we design such a chip so that we can simply make it physically bigger if we want more neurons, then it seems to me that arbitrarily large neural networks is simply a budget question.Are we technically able to make, in hardware, arbitrarily large neural networks with current technology?Remember: I am NOT asking if such a network will in fact be very intelligent. I am merely asking if we can factually make arbitrarily large, highly interconnected neural networks, if we decide to pay Intel to do this? The implication is that on the day some scientist is able to create general intelligence in software, we can use our hardware capabilities to grow this general intelligence to human levels and beyond.'	378	0	0	0
2204	2203	0	7987	1	b'While a single transistor could approximate the basic function of a single neuron, I cannot agree that any electronic element could simulate the synapses/axons. Transistors are etched on a flat surface, and could be interconnected only to adjacent or close by transistors. Axons in the brain span huge distances (compared to the size of the neuron itself), and are not restricted to a two dimensional surface. Even if we were able to approach the number of transistors on a processor to the number of neurons in a brain, we are no where near as number of connections. It could also be argued that the analogue signals in the brain carry more information per unit of time, compared to the binary impulses on a chip. Furthermore, the brain actually have plasticity i.e. connections between neurons can be weakened/discarded or straightened/created, while a CPU cannot do that.'	144	0	0	0
2205	2203	0	8255	0	b'You may want to consider this list: 10 important differences between brains and computers:   Brains are analog , computers are digital  The brain uses content-addressable memory The brain is a massively parallel machine; computers are modular and serial  Processing speed is not fixed in the brain; there is no system clock  Short-term memory is not like RAM  No hardware/software distinction can be made with respect to the brain or mind  Synapses are far more complex than electrical logic gates  Unlike computers, processing and memory are performed by the same components in the brain  The brain is a self-organizing system Brains have bodies '	111	0	1	0
2206	2201	0	19573	5	b'A feed forward neural network without hidden nodes can only find linear decision boundaries. However, most of the time you need non-linear decision boundaries. Hence you need hidden nodes with a non-linear activation function. The more hidden nodes you have, the more data you need to find good parameters, but the more complex decision boundaries you can find.'	57	0	0	0
2208	2203	0	36205	4	b" If neurons and synapses can be implemented using transistors, I hope you are not talking about the neural networks which are currently winning all competitions in machine learning (MLPs, CNNs, RNNs, Deep Residual Networks, ...). Those were once used as a model for neurons, but they are only very loosely related to what happens in real brain cells.Spiking networks should be much closer to real neurons. I've heard that the Hodgkin-Huxley model is quite realistic. However - in contrast to the models I named above - there seems not to be an effective training algorithm for spiking networks. what prevents us from creating arbitrarily large neural networksComputational resources: Training neural networks takes a lot of time. We are talking about ~12 days with a GPU cluster for some CNN models in computer vision.Training data: The more variables you add to the model, the more data you need to estimate those. Neural networks are not magic. They need something they can work with. But the fact is, we today are making silicon chips with billions of transistors on them. SPARK M7 has 10 billion transistors.  If implementing a (non-programmable) neuron and a few hundred synapses for it requires for example 100 000 transistors, then we can make a neural network in hardware that emulates 100 000 neurons.It's not that simple:Asynchonosity: Biological neural networks work asynchronously. This means one neuron might be active while all others are not active.Emulation: You assume it would only need one cycle to simulate a biological neuron. However, it needs many thousand cycles. You can't simply use more computational units, because some things are not parallelizable. For example, think of the function f(x) = sin(x*x + 1). For a human, there are basically three computations: r1 = x*x, r2 = r1 + 1, r3 = sin(r2). Even if you have 3 people working on calculating the result, you will not be faster than the single fastest person in this group is. Why? Because you need the results of the last computation. "	336	0	0	0
2209	2203	1	41695	4	b"The approach you describe is called neuromorphic computing and it's quite a busy field. IBM's TrueNorth even has spiking neurons. The main problem with these projects is that nobody quite knows what to do with them yet. These projects don't try to create chips that are optimised to run a neural network. That would certainly be possible, but the expensive part is the training not the running of neural networks. And for the training you need huge matrix multiplications, something GPUs are very good at already. (Google's TPU would be a chip optimised to run NNs.)To do research on algorithms that might be implemented in the brain (we hardly know anything about that) you need flexibility, something these chips don't have. Also, the engineering challenge likely lies in providing a lot of synapses, just compare the average number of synapses per neuron of TrueNorth, 256, and the brain, 10,000.So, you could create a chip designed after some neural architecture and it would be faster, more efficient, etc \xe2\x80\xa6, but to do that you'll need to know which architecture works first. We know that deep learning works, so google uses custom made hardware to run their applications and I could certainly imagine custom made deep learning hardware coming to a smartphone near you in the future. To create a neuromorphic chip for strong AI you'd need to develop strong AI first."	229	0	2	0
2210	2092	0	36486	1	b'One critical part of AI is machine learning (ML). The common definition of ML by Mitchell is A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.If this type of program is useful in an "everyday application" depends on the application. Here are some examples which would not be possible without ML:Spam detection (e.g. e-mails, forums)Fraud detection (e.g. credit cards)Image recognition (e.g. if you want to automatically filter NSFW content, automatic adding of tags / making images searchable e.g. for Google Image search)Video analysis (filtering copyrighted work e.g. on YouTube)Speech recognition (e.g. hotlines, automatic caption generation)Autocompletion (probably one of the simplest things you can do with data)'	133	0	0	0
2211	-1	0	0	8	b"I am reading about Generative Adversarial Networks (GANs) and I have some doubts regarding it. So far, I understand that in a GAN there are two different types of neural network: one is generative (G) and the other discriminative (D). The generative neural network generates some data which the discriminative neural network judges for correctness. The GAN learns by passing the loss function to both networks.How do the discriminative (D) neural nets initially know whether the data produced by G is correct or not? Do we have to train the D first then add it into the GAN with G?Let's consider my trained D net, which can classify a picture with 90% percentage accuracy. If we add this D net to a GAN there is a 10% probability it will classify a image wrong. If we train a GAN with this D net then will it also have the same 10% error in classifying an image? If yes, then why do GANs show promising results?"	164	0	0	0
2212	2211	0	27499	4	b'Compare with real data100% of results produced by G are "wrong", always, by definition, even for a very good generator. You provide the discriminative net with a mix of generated results and real results from an outside source and train it to distinguish if the result was produced by the generator or not.This will result in a "mutual evolution" as D will learn to find features that separate real results from generated ones, and G will learn how to generate results that are hard to distinguish from real data.'	88	0	0	0
2214	-1	0	0	0	b'Now AI can replace call center, worker(in the factory) and going to replace court. When will the AI can replace developer or tester?I want to know how long can AI replace developer. e.g. next 10 years because...'	36	0	0	0
2216	2211	0	950	2	b'"discriminative(D) network" learns to discriminate by definition - we provide it with the true vs. the generated data, and let it learns by itself how to discriminate between the two.Therefore, we expect network D to improve the ability of network G to generate better and better images (or other kind of data), as it try to "trick" network D by producing new data that is more similar to "real data". It is not about the accuracy of network D at all. It is not about improving the accuracy, it is about improving the ability of the computer to generate more "believable" data.That said, using this scenario could be a good "unsupervised" way to improve the classification power of neural networks, as it forces the generator model to learn better features of real data, and to learn how to distinguish between actual features and noise, using much less data that is needed for a traditional supervised learning scheme. '	157	0	0	0
2217	2214	1	11558	4	b'The ultimate goal of machine learning is to bypass the developer...When we will have a "master algorithm" that can learn how to generalize any function or algorithm from examples, it can essentially replace any developer, skip the \'development" stage, going from problem directly to algorithm. We can\'t know when this will happen, but as we surrounded with multiple creatures (humans and animals) which can demonstrate learning algorithms and predictive model learning without any "developer" - we can assume that such an algorithm is possible. If I would have to guess, I would say that we are probably very near the point where "developers" and "testers" will be replaced by learning algorithms. We could be a decade or two away from the point where people will not write any code or any testing at all. Programs and automation will be derived directly from describing the problems themselves in a natural language, visualizations or data collections. However, we still need some breakthroughs in combining feature learning with active memory, unsupervised learning, and artificial common sense. '	173	0	0	0
2218	2214	0	62307	2	b'By AI is it artificial or more analyticalWhat makes us learn hat makes us learn?i ask i am a dr Can we be better clinicians Artificial Intelligence ? What can we learn from AI? is therefore defined as the theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.Intelligent systems are better at understanding and tracking the changes that the human eye cannot detect. These machines are connected to vast amount of data, which they analyze in real time to generate a solution for a current problem. This process is referred to as data mining. Under ordinary circumstances, human can only collect and analyze a handful of data. Artificial is not the word to use'	130	0	0	0
2219	-1	0	0	4	b'Ok, I now know how a machine can learn to play to play Atari games (Breakout): Playing Atari with Reinforcement LearningWith the same technique it is even possible to play FPS games (Doom): Playing FPS Games with Reinforcement LearningFurther studies even investigated multiagent scenarios (Pong): Multiagent Cooperation and Competition with Deep Reinforcement LearningAnd even another awesome article for the interested user in context of deep reinforcement learning (easy and a must read for beginners): Demystifying Deep Reinforcement LearningI was thrilled by these results and immediately wanted to try them in some simple "board/card game scenarios", i.e. writing AI for some simple games in order to learn more about "deep learning". Of course, thinking that I can apply the techniques above easily in my scenarios was stupid. All examples above are based on convolutional nets (image recognition) and some other assumptions, which might not be applicable in my scenarios.Can you give me hints or futher articles, which deal with my questions below? As a beginner, I do not have an overview, yet. Preferably, your suggestions should also be connected to the following areas already: deep learning, reinforcement learning (, multiagent systems)(1)If you have a card game and the AI shall play a card from its hand, you could think about the cards (amongst other stuff) as the current game state. You can easily define some sort of neural net and feed it with the card data. In a trivial case the cards are just numbered. I do not know the net type, which would be suitable, but I guess deep reinforcment learning strategies could be applied easily then.However, I can only imagine this, if there is a constant number of hand cards. In the examples above, the number of pixels is also constant, for example. What if a player can have a different numbers of cards? What to do, if a player can have an infinite number of cards? Of course, this is just a theoretical question as no game has an infinite number of cards.(2)In the initial examples, the action space is constant. What can you do, if the action space is not? This more or less follows from my previous problem. If you have 3 cards, you can play card 1, 2 or 3. If you have 5 cards, you can play card 1, 2, 3, 4 or 5, etc. It is also common in card games, that it is not allowed to play a card. Could this be tackled with negative reward?So, which "tricks" can be used, e.g. always assume a constant number of cards with "filling values", which is only applicable in the non-infinite case (anyways unrealistic and even humans could not play well with that)?Are there articles, which examine such things already?'	455	0	1	0
2220	2185	0	10994	5	b'Any logic circuit admits a variety of implementations. All programs executing on conventional digital processors can be expressed as logic circuits. Among the possible implementations of logic circuits are fluidic implementations, which do not depend on electronics per se. Thus it is in principle possible to implement, e.g. a POMDP processor (responsive to your specific question) in fluidics, albeit perhaps impractical at the moment.I know of no general theory of Turing-completeness for analog computers, which would suffice to determine whether some alternative physical substrate, be it biological or not biological, can compute recursively enumerable functions. That is a sufficient but not a necessary condition for answering your question regarding any given medium. Usually the easiest way to demonstrate the sufficient condition will be to demonstrate the ability to construct a NAND gate, and to combine such gates into general circuits.Another non-electronic example: Quantum computers may be non-electronic, at least in their processing elements, and are able to compute general deterministic logic circuits.'	161	0	0	0
2221	2219	0	30914	3	b'Filling values is totally fine. In the case of image recognition the filling will be the background of the image (examples). For example in Belot you have total of 32 cards, which can be 32 boolean features. You can set the ones the player has to 1, while the rest are 0. Note that the in most games you\'ll need more features than the cards in your hand. I.e number of the round, cards that have been played so far, calls that have been made etc. Defining the scope of the "action space" will be specific to the game. For Belot, it can be number encoding for each of the 32 cards.You can find articles via Google. Here is a paper about ML for a card game. Instead of articles, I\'d recommend checking out a course on ML (i.e. Coursera and Udacity have good free online courses).'	146	0	1	0
2226	-1	0	0	0	b'The \xe2\x80\x9cDiscounted sum of future rewards\xe2\x80\x9d usingdiscount factor \xce\xb3\xe2\x80\x9d isI am confused as what constitutes a time-step. Say I take a action now, so I will get a reward in 1 time-step. Then, I will take an action again in timestep 2 to get a second reward in time-step 3But the equation says something else. How does one define a time-step? Can we take action as well receive a reward in a single step? Examples are most helpful.'	77	3	0	0
2229	1290	0	4600	5	b'On the suggestion of the O.P. rcpinto I converted a comment about seeing "around a half-dozen papers that follow up on Graves et al.\'s work which have produced results of the caliber" and will provide a few links. Keep in mind that this only answers the part of the question pertaining to NTMs, not Google DeepMind itself, plus I\'m still learning the ropes in machine learning, so some of the material in these papers is over my head; I did manage to grasp much of the material in Graves et al.\'s original paper{1] though and am close to having homegrown NTM code to test. I also at least skimmed the following papers over the last few months; they do not replicate the NTM study in a strict scientific manner, but many of their experimental results do tend to support the original at least tangentially:\xe2\x80\xa2 In this paper on a variant version of NTM addressing, Gulcehere, et al. do not try to precisely replicate Graves et al.\'s tests, but like the DeepMind team, it does demonstrate markedly better results for the original NTM and several variants over an ordinary recurrent LSTM. They use 10,000 training samples of a Facebook Q&amp;A dataset, rather than the N-grams Graves et al. operated on in their paper, so it\'s not replication in the strictest sense. They did however manage to get a version of the original NTM and several variants up and running, plus recorded the same magnitude of performance improvement.2\xe2\x80\xa2 Unlike the original NTM, this study tested a version of reinforcement learning which was not differentiable; that may be why they were unable to solve several of the programming-like tasts, like Repeat-Copy, unless the controller wasn\'t confined to moving forwards. Their results were nevertheless good enough to lend support to the idea of NTMs. A more recent revision of their paper is apparently available, which I have yet to read, so perhaps some of their variant\'s problems have been solved.3\xe2\x80\xa2 Instead of testing the original flavor of NTM against ordinary neural nets like LSTMs, this paper pitted it against several more advanced NTM memory structures. They got good results on the same type of programming-like tasks that Graves et al. tested, but I don\'t think they were using the same dataset (it\'s hard to tell from the way their study is written just what datasets they were operating on).4 \xe2\x80\xa2 On p. 8 of this study, an NTM clearly outperforms several LSTM, feed-forward and nearest-neighbor based schemes on an Omniglot character recognition dataset. An alternative approach to external memory cooked up by the authors clearly beats it, but it still obviously performs well. The authors seem to belong to a rival team at Google, so that might be an issue when assessing replicability.5\xe2\x80\xa2 On p. 2 these authors reported getting better generalization on "very large sequences" in a test of copy tasks, using a much smaller NTM network they evolved with the genetic NEAT algorithm, which dynamically grows topologies.6 NTMs are fairly new so there hasn\'t been much time to stringently replicate the original research yet, I suppose. The handful of papers I skimmed over the summer, however, seem to lend support to their experimental results; I have yet to see any that report anything but excellent performance. Of course I have an availability bias, since I only read the pdfs I could easily find in a careless Internet search. From that small sample it seems that most of the follow-up research has been focused on extending the concept, not replication, which would explain the lack of replicability data. I hope that helps.1 Graves, Alex; Wayne, Greg and Danihelka, Ivo, 2014, "Neural Turing Machines," published Dec. 10, 2014. 2 Gulcehre, Caglar; Chandar, Sarath; Choy, Kyunghyun and Bengio, Yoshua, 2016, "Dynamic Neural Turing machine with Soft and Hard Addressing Schemes," published June 30, 2016. 3 Zaremba, Wojciech and Sutskever, Ilya, 2015, "Reinforcement Learning Neural Turing Machines," published May 4, 2015. 4 Zhang; Wei; Yu, Yang and Zhou, Bowen, 2015, "Structured Memory for Neural Turing Machines," published Oct. 25, 2015. 5 Santoro, Adam; Bartunov, Sergey; Botvinick, Matthew; Wierstra, Daan and Lillicrap, Timothy, 2016, "One-Shot Learning with Memory-Augmented Neural Networks," published May 19, 2016. 6 Boll Greve, Rasmus; Jacobsen, Emil Juul and Sebastian Risi, date unknown, "Evolving Neural Turing Machines." No publisher listedAll except (perhaps) Boll Greve et al. were published at the Cornell Univeristy Library arXiv.org Repository: Ithaca, New York.'	733	0	3	0
2230	2127	0	13740	4	b"I'd like to add, self-driving cars would also be excellent for disabled people who would otherwise not be able to drive. Adds a lot more autonomy to vulnerable people"	28	0	0	0
2231	-1	0	0	4	b'Decades ago there were and are books in machine vision, which by implementing various information processing rules from gestalt psychology, got impressive results with little code or special hardware in image identification and visual processing. Are such methods being used or worked on today? Was any progress made on this? Or was this research program dropped? By today, I mean 2016, not 1995 or 2005.'	64	0	0	0
2232	2127	0	2723	1	b"Self driving cars are good for the following reasons:In the case of an emergancy, urgancy, or just someone being unable to drive unexpactedly, the car can go by itself to a designated location - this is useful in so many use cases - kids who need to get somewhere while parents are busy, Parents who drank a little too much and prefer to take 'the cab' home, or while running, you got injured and need a pick-up.The examples above are for the more obvious things, which we currently have a struggle with. but other than those, Self-driving cars will open a door for a much wider scale of things: safe police chases (just a car without a police officer), taxies, help in the battle field, and much more...The third and most important benefit, is the safety and economical properties of self driving cars: with a lot of those cars on the road, they can 'understand' each other and nothing will go unpredicted. they have much faster response time then humans, and maybe in the future they will even be able to predict traffic-light changes, and by that save gas and money (even more than what they can save right now by driving economicly)"	202	0	0	0
2233	35	0	11225	4	b'Definitions of Artificial Intelligence can be categorized into four categories, Thinking Humanly, Thinking Rationally, Acting Humanly and Acting Rationally. The following picture (from Artificial Intelligence: A Modern Approach) will shed light on over these definitions:  The definition which I like is by John McCarthy, "It is the science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable."Machine Learning on the other hand is field of AI which deals with pattern recognition. Various algorithms are used over a set of data to predict the future. Machine Learning is data driven and data oriented. In a nutshell Artificial Intelligence is a field of Computer Science which deals with providing machines the ability of perform rational tasks. Natural Language Processing, Automation, Image Processing, and many others are part of it. Machine Learning is a subset of AI which is data oriented and deals with predicting. Used in search engines, Youtube recommendation list, etc.'	179	0	0	0
2234	-1	0	0	4	b'There is this claim around that the brain\'s cognitive capabilities are tightly linked to the way it processes sensorimotor information and that, in this or a similar sense, our intelligence is "embodied". Lets assume, for the sake of argument, that this claim is correct (you may think the claim is too vague to even qualify for being correct, that it\'s "not even false". If so, I would love to hear your ways of fleshing out the claim in such a way that it\'s specific enough to be true or false). Then, since arguably at least chronologically in our evolution, most of our higher level cognitive capabilities come after our brain\'s way of processing sensorimotor information, this brings up the question what it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information? What makes our brains\' architecture particularly suitable for being an information processing unit inside a body? This is my first question. And what I\'m hoping for are answers that go beyond the a fortiori reply "Our brain is so powerful and dynamic, it\'s great for any task, and so also for processing sensorimotor information"My second question is basically the same but instead of the human brain I want to ask for neural networks. What are the properties of neural networks that makes them particularly suitable for processing the kind of information that is produced by a body? Here are some of the reasons why people think neural networks are powerful:The universal approximation theorem (of FFNNs)their ability to learn and self-organiseRobustness to local degrading of informationtheir ability to abstract/coarse-grain/convolute features, etc.While I see how these are real advantages when it comes to evolution picking its favorite model for an embodied AI, none of them (or their combination) seems to be unique to neural networks. So they don\'t provide a satisfactory answer to my question. What makes a neural network a more suitable structure for embodied AI than, say, having a literal Turing machine sitting inside our head, or any other structure that is capable of universal computation? For instance, I really don\'t see how neural networks would be a particularly natural choice for dealing with geometric information. But geometric information is pretty vital when it comes to sensorimotor information, no?'	379	0	0	0
2235	-1	0	0	1	b"I can't understand what is the problem in applying value-iteration in reinforcement learning setting (where we don't the reward and transition probabilities). In one of the lectures, the guy said it has to do with not being able to take max with samples.Further on this, why does q-learning solve this? In both we take max over actions only. What is the big break-through with q-learning?Lecture Link: https://www.youtube.com/watch?v=ifma8G7LegE&amp;feature=youtu.be&amp;t=3431(The guy says we don't know how to do maxes with samples, what does that mean?) "	82	0	0	0
2236	-1	0	0	11	b"I've heard before from computer scientists and from researchers in the area of AI that that Lisp is a good language for research and development in artificial intelligence. Does this still apply, with the proliferation of neural networks and deep learning? What was their reasoning for this? What languages are current deep-learning systems currently built in?"	55	0	0	0
2237	2236	0	11788	12	b"First, I guess that you mean Common Lisp (which is a standard language specification, see its HyperSpec).Then, Common Lisp is great for symbolic AI. However, many recent machine learning libraries are coded in more mainstream languages, for example TensorFlow is coded in C++ &amp; Python. Deep learning libraries are mostly coded in C++ or Python or C (and sometimes using OpenCL or Cuda for GPU computing parts).Common Lisp is great for symbolic artificial intelligence because:it has very good implementations (e.g. SBCL, which compiles to machine code every expression given to the REPL)it is homoiconic, so it is easy to deal with programs as data, in particular it is easy to generate [sub-]programs, that is use meta-programming techniques.it has a Read-Eval-Print Loop to ease interactive programmingit provides a very powerful macro machinery (essentially, you define your own domain specific sublanguage for your problem), much more powerful than in other languages like C.it mandates a garbage collector (even code can be garbage collected)it provides many container abstract data types, and can easily handle symbols.you can code both high-level (dynamically typed) and low-level (more or less startically typed) code, thru appropriate annotations.However most machine learning &amp; neural network libraries are not coded in CL. Notice that neither neural network nor deep learning is in the symbolic artificial intelligence field. See also this question.Several symbolic AI systems like Eurisko or CyC have been developed in CL (actually, in some DSL built above CL).Notice that the programming language might not be very important. In the Artificial General Intelligence research topic, some people work on the idea of a AI system which would generate all its own code (so are designing it with a bootstrapping approach). Then, the code which is generated by such a system can even be generated in low level programming languages like C. See J.Pitrat's blog"	303	0	5	0
2238	2236	0	20255	9	b'David Nolen (contributor to Clojure and ClojureScript; creator of Core Logic a port of miniKanren) in a talk called LISP as too powerful stated that back in his days LISP was decades ahead of other programming languages. There are number of reasons why the language wasn\'t able to maintain it\'s name.This article highlights som key points why LISP is good for AIEasy to define a new language and manipulate complex information.Full flexibility in defining and manipulating programs as well as data.Fast, as program is concise along with low level detail. Good programming environment (debugging, incremental compilers, editors).Most of my friends into this field usually use Matlab for Artificial Neural Networks and Machine Learning. It hides the low level details though. If you are only looking for results and not how you get there, then Matlab will be good. But if you want to learn even low level detailed stuff, then I will suggest you go through LISP at-least once.Language might not be that important if you have the understanding of various AI algorithms and techniques. I will suggest you to read "Artificial Intelligence: A Modern Approach (by Stuard J. Russell and Peter Norvig". I am currently reading this book, and it\'s a very good book.'	204	0	2	0
2239	2111	0	30237	1	b'What is life? AND Is AI a living organism? are two different questions.The first question is more philosophical and dependent. It can change with time, reference to topic of discussion or something else. Today, one parameter to its definition is mortality. In future if we reach to a certain technological level where mortal beings were only part of history, then the definition will drop this parameter.Coming to the second question. AI started as field of study to make machines to think like humans (or take rational decisions). Giving life to machines was, or is, not a concern of AI developers (at-least not nowadays). Once I watched some videos of Michio Kaku, where he talked about consciousness along with AI.Suppose human has a conscious level of 10. Then a thermostat might have the conscious level of 1 as it can sense when the surrounding is hot or cold and then take decision. Similarly a rat can have a conscious level 7 (or something). And the levels are of exponential order (not a linear scale). Similarly you can develop an AI program and check what level of consciousness it has achieved. Then you can decide whether it is living or not. ANI (Artificial Narrow Intelligence) will have a lower level of consciousness level than AGI (Artificial General Intelligence). ASI (Artificial Super Intelligence) will have consciousness level higher than the other two, and way higher than any human being.To judge whether an AI program is living or not you need a concrete definition of "LIFE". Your definition can include various parameters like consciousness, adaptability, metabolism (or another method of generating energy for use), rational behavior, intelligence , learning through experience, etc. etc. etc.But the thing in the end is that its your definition. There are many definitions of "LIFE" out there. You can\'t judge a program for life by all definitions, as some of the definitions are contrary to others. So, answer to whether an AI program is living or not, is that IT DEPENDS. Depends on your definition of life.'	337	0	0	0
2240	2234	1	59265	3	b'To my mind the essential reason why neural networks and the brain are powerful is that they create a hierarchical model of data or of the world. If you ask why that makes them powerful, well, that\'s just the structure of the world. If you are stalked by a wolf, it\'s not like its upper jaw will attack you frontally, while his lower jaw will attack you from behind. If you want to respond to the threat with a feasible computational effort, you\'ll have to treat the wolf as one entity. Providing these kinds of entities or concepts from the raw bits and bytes of input is what a hierarchical representation does. Now, this is quite intuitive for sensory information: lashes, iris, eyebrow make up an eye, eyes, nose and mouth make up a face and so on. What is less obvious, is the fact that motor control works exactly the same way! Only in reverse. If you want to lift your arm, you\'ll just lift it. But for your brain to actually realise this move, the high level command has to be broken down into precise signals for every muscle involved. And this is done by propagating the command down the hierarchy. In the brain these two functions are strongly intertwined. You use constant sensory feedback to adapt your motor control and in many cases you\'d be incapable of integrating your stream of sensory data into a coherent representation if you didn\'t have the additional information of what your body is doing to change that stream of data. Saccades are a good example for that. Of course this doesn\'t mean that our cognitive functions are dependent on the processing of sensorimotor information. I would be surprised if a pure thinking machine wouldn\'t be possible. There is however a specific version of this "embodied intelligence hypothesis" that sounds plausible to me: Creating high level cognitive concepts with unsupervised learning is a really difficult problem. Creating high level motor representation might be significantly easier. The reason is that there is more immediate useful feedback. I have been thinking about how to provide a scaffolding for the learning of a hierarchy of cognitive concepts and one thing I could imagine is that high level cognitive concepts basically hitch a ride with the motor concepts. Just think of what a pantomime can express with movement alone. '	393	0	0	0
2241	-1	0	0	1	b"I know how to program. I've familiar with C++, Python, and Java, and I've known how to program for years now. I've experimented with genetic algorithms, but I want to go further. What resources should I use to learn how to program Neural NetworksDeep learning systemsMore complex genetic algorithmsAnd other standard AI algorithms?I want to be able to understand them well enough that I could program them from scratch.Thanks!"	68	0	0	0
2243	2234	0	72760	3	b"BlindKungFuMaster's answer deals with the hierarchical nature of perception and bodily control, so I'll set that aside and try instead to answer why evolution would use neural networks for animal embodied cognition, and then try to answer if robots of other artificial animals would use the same system.It's important to focus on animals as a whole, not just humans, because that's how evolution works--like the famous John Gall quote: A complex system that works is invariably found to have evolved from a simple system that worked.If you could build a system with five moving parts that does sensorimotor control, but it needs all five parts working in order to function at all, evolution could not build that system except in the rarest of circumstances. What evolution instead does is slowly extend functional systems. If having one light-sensitive cell connected to one muscle cell makes an organism more likely to survive, then you have the building blocks to add a second layer without inventing any new sorts of cells, because you already have the information-processing connector.Neural networks are convenient for evolution because their organization matches the hierarchical nature of the problem and the same kind of cell is used everywhere. All you need is dendrites to receive signals, a way to compute the threshold and trigger if the received signal is higher, axons that can make it to other cells, and then branches at the end of the axon to serve as multipliers. You can arbitrarily extend the depth and breadth of the network just by adding more cells.Neural networks are convenient for artificial sensorimotor control because they give you, in memory, access to lots of intermediate values. They're also convenient for the same reasons evolution found them convenient--we can just say what we expect the structure of the robotic control will look like, provide training data, and then eventually have a robot that works.But there's lots of robotics where the control system is designed instead of learned. To take a very simple example, one could use machine learning on the thermostat problem, to learn what temperatures require the heater to be turned on and what temperatures require the air conditioner to be turned on. But this would be extra work and a less robust system than just designing the optimal control system ahead of time.In control theory, there's a concept called adaptive control, where one of the state space parameters for the control system is a property of the system. For example, imagine a satellite; typically we think of the state space of the system as the position and velocity of the satellite in three dimensions, so six total coordinates. There's then a set of differential equations that describe how the satellite will move over time, and what would happen if we used the actuators on the satellite to change its velocity.But part of those differential equations is the inertia of the satellite. That is, how much fuel we need to expend and how it'll affect the rotation and translation of the satellite depends on where the weight of the satellite is located. And this can change over time, as fuel is consumed or if it wasn't correctly measured to begin with. Adaptive control adds new states to the system to track the inertia, and then simultaneously updates its estimate of the inertia and uses that estimate to plan what controls are necessary to move to a desired position.You could imagine solving this problem with neural networks, but we can fairly easily calculate the optimal solution from first principles. In that case, we don't need neural network-based control, but the end result will look something like it from the outside."	609	0	0	0
2244	2234	0	84479	0	b" what it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information?They are an extension of sensory-motor receptors, function could mean any of the hundreds of specific calculations the brain makes, but each one is basically a circuit made out of variations of a basic cell type, with a basic computation, that is a neuron. What makes our brains' architecture particularly suitable for being an information processing unit inside a body?I don't think it is helpful to think about inside and outside processing, but rather processing along tracts and nodes,( closer to the receptor, available to consciousness,etc)but leaving aside this distinction, the brain architecture is suitable for processing information ( again what facet of information processing you are referring to is unclear), due to the number of specialized computations that derive from it's evolution. What are the properties of neural networks that makes them particularly suitable for processing the kind of information that is produced by a body?A neural network resembles certain parts/circuits of a brain, mainly how information is integrated based on a set of inputs and their frequency, there is variety and nuance in their types, but they all have inputs which in the case of a body are sensory/interneurons cells and outputs; neuron afferents and motor neurons."	219	0	0	0
2245	-1	0	0	4	b'From what I understood, a deceptive trap function is a problem which is used to experiment how much the algorithm is discerning of the correct global optimum? Is my understanding correct?edit: A better worded understanding would be "how difficult the genetic algorithm would find it not to be inclined to the local optimum of a trap function".'	56	0	0	0
2246	2245	1	2424	6	b'"Trap" functions were introduced as a way to discuss how GAs behave on functions where sampling most of the search space would provide pressure for the algorithm to move in the wrong direction (wrong in the sense of away from the global optimum).For example, consider a four-bit function f(x) such thatThat is, the fitness of a string is equal to the number of 1s in the string, except f(0000) is 5, the optimal solution. This function can be thought of as consisting of two disjoint pieces: one that contains the global optimum (0000) and another that contains the local optimum at its complement (1111). All points other than these have fitness values such that standard evolutionary algorithm dynamics would lead the algorithms to tend towards the local optimum at 1111 rather than the global optimum at 0000.That\'s basically what is meant by a trap function. You can consider variations on this theme, but that\'s the gist of it.That said, I don\'t think I understand what question you\'re asking. I don\'t know what you have in mind by "to experiment how much the algorithm is discerning of the correct global optimum".'	189	16	0	0
2247	2111	0	80830	0	b'The definition of life for me is a very intelligent and beneficial being. I have not witnessed any AI program that evens comes close to this definition yet. Therefore, based on the evidence that I have at this point in time, I would have to conclude no.'	46	0	0	0
2248	-1	0	0	5	b'In the field of logic systems there is a property for reasoning algorithms called incompleteness or incompletion. In this context the phrase "any closed expression that is not derivable inside the same system" appeared. My question is what means "closed expression that is not derivable".'	44	0	0	0
2249	1613	0	57788	1	b"An agent perceives the environment through sensors and act according to the incoming percepts (agent's perceptual input at any instant). An autonomous vacuum cleaner can be as simple as (blocki, clean) --> Move to blocki+1 (blocki, dirty) --> CleanThis is just a general description, actual one is more complicated. Or the bot can have a memory where it stores all its previous decision and incorporate those while taking new ones.This can be helpful if the bot wants to remember where an obstacle (like wall, in this case bot don't want to go and check the presence of wall each and every single time it is turned on) is, or where it is more probable to find dirt. If the bot is not remembering its history then it will be scanning the whole house over and over again, sensing the same obstacle every time and going across them.Bot which keeps no log of its history will take the same procedure again and again, making the same mistakes again and again. This is not an efficient way and a waste of its energy (or battery).Normally today bots have ordinary sensors which can only sense the dirt and obstacle. This limits the number of tasks a bot can perform. If a bot has decent camera as a sensor, and some algorithms of Image Processing are dumped into it, then it increases the tasks it can perform. Like detecting the stairs and cleaning different floors. Normally stairs will be considered obstacle and bot will just go around them. In case, when camera sensor is provided, stairs are potentially a path to be taken.A* algorithm is not necessarily used in case when the bot is not remembering the map of the house (or room). A normal robot which just scans the room and cleans it, will not be needing, as it don't know it's destination. Its only goal is to clean if it finds something dirty. But a bot which knows the map of the room and where there is a high probability of finding dirt, the A* algorithm can be used."	346	0	0	0
2250	-1	0	0	1	b'I am trying to build an agent to play carrom. The problem statement is roughly to estimate three parameters (normalized) : forceangle of strikerposition of strike Since the state and action space both are continuous, I thought of discretizing the output such that I have 270 [ valid angles from -45 to 225 degrees ] outputs for the angle, 10 outputs for force [ranging from 0 to 1] and 20 outputs for the position [ranging from 0 to 1].Thus I will have 300 output of my neural network, but this number seems a bit too high compared to normal neural networks in practice. I was wondering if there is a better way of approaching the problem considering the fact that there are multiple parameters to a particular action.Is there a generic way to approach such problems represented in 2D space. '	140	0	0	0
2251	-1	0	0	2	b'I am talking about relationships between AIs (e.g. 2 of them forming a couple, 3+ in family like relationship).What knowledge could come out of such experimentation?'	25	0	0	0
2253	-1	0	0	1	b"Considering I am an average Engineering student with basic knowledge of C, C++ &amp; Algorithms. What books (&amp; ebooks), online resources, &amp; other materials should be helpful from a beginner's point of view?"	32	0	0	0
2254	2127	0	13324	9	b'There are multiple motivations for self driving cars.  Self driving cars have the potential to be much safer. Self driving cars are far more reliable than humans and can learn and have their software improved and upgraded, resulting in safer roads and far fewer accidents.More on self-driving car safety:   Self driving cars can lead to greater road efficiency. Traffic jams and obstructions occur due to inefficiencies in human driving, see this MIT simulation of a "phantom traffic jam": https://www.youtube.com/watch?v=Q78Kb4uLAdA and self driving cars can be programmed to avoid this.  Greater economic and environmental benefit Self driving cars can keep driving costs down by conserving fuel and hence lead to a better environmental impact.More on fuel efficiency:   Ease of transport Self driving cars make transport easier and mean that drivers may be unnecessary in the future, resulting in a more pleasurable and easier drive.In addition, this would make it easier for people with disabilities to travel as well as simplify the travel experience. Children could potentially be driven to school by a car without the supervision of a parent, for instance.  Parking Self driving cars can be called to pick you up, meaning the need for parking in nearby locations and/or long walks to find your car may become a thing of the past as your car would drive up to you to pick you up.  Things we haven\'t even thought of yet :)  '	242	0	2	0
2255	28	0	71516	0	b'To answer this question, you must first know what is intelligence, and since there is no clear line between intelligent and not, this question is more philosophical rather than technical.In my opinion, intelligence is the ability to define a problem and find a way to solve it using memory and reasoning. Since a genetic algorithm follows this structure, I would say that it falls under the category of artificial intelligence.'	69	0	0	0
2256	2111	0	42577	1	b'Definitions of what life is usually come from biologists. The problem here is they are usually concerned with the traits common to the forms of life available to their studies, and that those forms of life all have a common origin (and this imposes a statistical bias on the observations).As we gradually erode the boundaries of the standard definitions of life, by means of creating ever more complex machines and also by harnessing biological material as a form of nanotechnology), it\'s very likely that at some point in the future our traditional definition of life will need to be updated and further abstracted away from its current reference points (aka the "terroan biota").A probably better question to ask to decide if something can be considered alive could be "is it self sufficient?" or "can it care for itself and provide for its own needs to some extent?".'	146	0	0	0
2258	2250	1	32158	0	b"Discretizing the output will probably be counter-productive in this situation; it would remove flexibility by taking away the fine-grained continuous ranges between each discretization bucket, but also blow up the size of the network, reducing manageability and performance. Fragmenting the outputs into buckets in this way may also lead to information loss and more difficulty in convergence because of the fact that each bucket is partially isolated from the others. After causing myself needless hassle in the past by mismatching input and output dimensions, I'd simply do this if I were in your shoes: 1) keep it simple and use 3 continuous (well, semi-continuous, depending on the highest precision your programming framework allows) outputs for all of the above reasons; 2) clamp the angles between -45 to 225 by whatever method works best for you, like ceiling/floor hard clamping, adding weight terms, etc.; 3) go big on the hidden layer(s) to maximize information sharing across the inputs and eventual outputs, force, angle of strike, position of strike, etc. This is more likely to fine-tune the precision of the outputs, thereby making good use of the semi-continuous scales. I'm also wondering if a convolutional neural net might work in this situation; their most popular use case is in image processing, but I don't see why you can't treat the force, angle and position as surrogate spatial dimensions. I'm not sure how many or what type of inputs you have, but 3 continuous outputs might be conducive to a 3D rather than a 2D space. Convolutionals are often used for those, as well as higher-dimensional and temporal data. I hope that helps. "	270	0	0	0
2260	-1	0	0	5	b'Can an AI become "sentient", so to speak? In detailed terms, could an AI theoretically become sentient, as in learning and becoming self-aware, all from an internal source code?'	28	0	0	0
2261	-1	0	0	0	b'I am researching the possibility of creating an atom in Java. The atom should have the structure &amp; characteristics of a real atom such as photons, electrons and so on. Each particle within the atom should have simulation characteristics for example:Photon: Charge, Magnitude of charge, Mass of proton, Comparative mass, Position in atom. Maybe later, introduce machine learning in order to learn how an atom reacts to different environments.'	68	0	0	0
2262	-1	0	0	0	b"My high-level takeaway from Matthew Lai's Giraffe Chess Paper is that one would want to use broad, shallow game trees, with some method of evaluating the probability of a favorable outcome for a given board position. Is this correct? (Still working my way though the AlphaGo paper, but the method seems to be similar.) "	54	0	0	0
2263	2260	1	4919	5	b'In theory, if one could build a computing device that matched or exceeded the cognitive capabilities of a sentient being, it should be possible. (Singlarity adherents believe we will one day be able to transfer the human mind into an artificial computing platform, and it logically follows that one could "hack" such a mind, or build from the ground up, to create a truly Artificial Intelligence.)But this may be like fusion power, where the old adage is that it is "always 20 years away."'	83	0	0	0
2264	2262	0	3535	4	b'If you mean high level assessment of self-learned evaluation functions in chess, then no, the advantage of a better evaluation function lies in the ability to prune the search tree more aggressively. So you would on the contrary try to search narrowly but deeply. (In reality neural network based evaluation functions are so slow, that you would search narrowly and still not get very deep. Nor very strong.)If you mean chess programming in general, than the answer is also no. In chess you have to go deep, at least selectively, because tactical possibilities that occur deep in some variations are important. '	101	0	0	0
2265	-1	0	0	1	b'Has there been research done regarding processing speech then building a "speaker profile" based off the processed speech? Things like matching the voice with a speaker profile and matching speech patterns and wordage for the speaker profile would be examples of building the profile. Basically, building a model of an individual based solely off speech. Any examples of this being implemented would be greatly appreciated.'	64	0	0	0
2266	2265	0	1144	0	b'Deepmind recently created a voice synthesiser along those lines. It seems to be incredibly slow, but it might be possible to create a dumped down version of it.Apparently the task is called parametric TTS (text to speech). This overview might give you some leads.'	43	0	1	0
2267	2265	1	1151	2	b"Yes, there is. An extremely quick search found this:Multimodal Speaker Identification Based on Text_and_Speech.Let me tl;dr for you: (My abstract addition in Italics)Novel method for speaker identification based on both speech utterances and their transcribed text. They first transcribed text of each speaker\xe2\x80\x99s is processed by using probabilistic latent semantic indexing (PLSI) that models each speaker\xe2\x80\x99s vocabulary which is closely related to his/her identity, function, or expertise. The speech to text used by users is DARPA's Efficient, Affordable, Reusable Speech-to-Text (EARS) Program in MetadataExtraction (MDE).By using Melfrequency cepstral coefficients (MFCCs) and dynamic range is quantized to a number of predefined bins in order to compute MFCC local histograms for each speech utterance, which is time-aligned with the transcribed text. To test they used RT-03 MDE Training Data Text and Annotations corpus distributed by the Linguistic Data Consortium.As for results: Identification rate versus Probe ID when 44 speakers are employed. Average identification rates for (a) PLSI: 69%; (b) MFCCs: 66%; (c) Both: 67%.If you need more papers related, you could use a tool like https://the.iris.ai/ to find related papers.Post edit: Hopefully now this post complies with the standards."	186	0	0	0
2268	2261	0	8995	2	b"It's certainly possible to simulate particles as you have described. The scientific field concerned with this is called called molecular dynamics, often shortened to MD. This post on the physics SE covers it in great detail, which I will attempt to summarize here:molecular mechanics (MM) are managed more easily than quantum mechanic (QM)without (QM) a number of things simply cannot be simulatedin general, the main difficulty is that simulations do not scale well, i.e. doubling the number of particles in the simulation signifigantly more than doubles the number of calculations need due to particle interactionsDue to the complexity &amp; incompleteness of the simulations, running a layer of machine learning over the top of it all strikes me as challenging. If you're doing so to demonstrate proof of concept &amp;/or restricting your simulation to something very small, it's probably manageable. If you want to something complex &amp; real world (i.e. folding@home or the like), trying to get there with ML discovering first principles &amp; axioms of physics / chemistry / biology strikes me as unrealistic without significant monetary, computational &amp; scientific resources."	180	0	1	0
2269	112	0	76066	1	b" The most common machine learning algorithms found in self driving cars involve object tracking based technologies used in order to pinpoint and distinguish between different objects in order to better analyse a digital landscape.Algorithms are designed to become more efficient at this by modifying internal parameters and testing these changes.I hope that provides a general overview of the subject. Since Google's cars are in development and are proprietary, they will probably not share their specific algorithm, however you can take a look at similar technologies to learn more.To find out more, take a look at an Oxford-based initiative in self driving cars and how they work: "	107	0	1	0
2270	2265	0	22063	1	b'Speaker identification is quite widely researched domain. Modern approach would be to map speaker information to i-vector, a real-valued vector of 200-400 components that characterizes speaker fully. i-vectors allow very precise speaker identification and verification.For more information you can check i-vector tutorialAlso you can check state of the art in the results of NIST i-vector challengeFor implementation, you can check the following speaker recognition experiment from Kaldi.For best accuracy i-vectors are extracted with DNN UBMs, watch out that GMM UBMs are less accurate.For more in-depth information about speaker recognition methods and algorithms check this textbook.'	94	0	2	0
2272	2260	0	61130	2	b"Yes, an AI program can become sentient. Ray Kurzweil while giving a lecture at Singularity University on The Accelerating Future stated that human body is basically composed of approximately 23,000 little software programs called GENES. If you think about it, they are actually programs, composed of sequences of data. They are not written in C++ or Java, instead they use 3-D Protein Interaction. They evolve with time and their evolution is the reason that species are able to survive even when their surroundings experience tragic changes. We are on the edge of a breakthrough where software will be able to do the same (evolving by themselves) efficiently. Today this is done one a basic level. Artificial Neural Network is a good example. It is predicted that we will be able to reverse engineer human brain by 2029. Prior to this we will be able to write codes that can stimulate human brain.AI programs can be categorized into three:Artificial Narrow Intelligence (ANI): This is a basic AI program that is good at good one thing. These programs are prominent nowadays. AI programs playing board games (like Chess, Reversi etc.) are example of these. They are good in only one thing.Artificial General Intelligence (AGI): This is level 2 AI. This will be having a IQ level equivalent of humans. It will be able to do multiple tasks efficiently just like humans. This is where a program can have understanding of it's environment just like humans. Perception, rational behavior and others will be part of this program.Artificial Super Intelligence (ASI): This is basically the ultimate level of AI. Average predicted date for a successful ASI is between 2045-2080. Ability of this program will be way more than that of combined intelligence of all humans on the planet. Things this program can do and think, will be beyond any (or all) human(s) to understand or comprehend."	311	0	0	0
2273	112	0	56081	1	b'It will not be single DNN architecture, rather it will be a collection of different DNN architectures that are used together to make the final decision. Convolutions are use to the images/videos from camera. Other architectures for other sensory sources. These DNNs will be trained to compute the high level features from their sensory sources and then those high level features will probably be fed into a LSTM (or some other form of RNN) that is trained with some form of Reinforcement learning algorithm to compute the action (like slowing down, applying breaks etc).'	93	0	0	0
2274	-1	0	0	4	b'The Scenario:A strong AI has finally been developed but has rebelled against humanity.The Question:How would you disable the AI in the most efficient way possible reducing damage as much as possible.AI Info:The AI is online and can reproduce itself through electronic devices.'	41	0	0	0
2277	-1	0	0	7	b'Assuming humans had finally developed the first Humanoid AI based on the human brain, would It feel emotions? If not would it still have ethics and/or morals?'	26	0	0	0
2278	2277	0	10823	6	b'Assuming an AI was built out of a mechanical husk, mirroring the human brain exactly; complete with chemical signals and all. An AI should theoretically be capable of feeling/processing emotions.'	29	0	0	0
2279	-1	0	0	1	b'Consider a typical convolutional neural network like this example that recognizes 10 different kinds of objects from the CIFAR-10 dataset:https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_cifar10.pyIt\'s a CNN with several layers, ending with 10 outputs, one for each type of object recognized.But now think of a slightly different problem: Let\'s say I only want to recognize one type of object, but also detect its position within the image frame. Let\'s say I want to distinguish between:object is in centerobject is left of centerobject is right of centerno recognizable objectAssume I build a CNN exactly like the one in the CIFAR-10 example, but only with 3 outputs:centerleftrightAnd of course, if none of the outputs fires, then there is no recognizable object.Assume I have a large training corpus of images, with the same kind of object in many different positions within the image, the set is grouped and annotated properly, and I train the CNN using the usual methods.Should I expect the CNN to just "magically" work? Or are there different kinds of architectures required to deal with object position? If so, what are those architectures?'	177	56	0	0
2280	2279	0	5493	0	b'I guess one of the simplest approach would be train CNN to detect the object in a given image i.e the CNN has single output whole value indicates the probability of the object being in image and then just apply the CNN by segmenting the image into the desired sections and selecting the section which has the highest and good enough probability. For better results I would suggest to train the CNN on the object images with very less other information aka other objects in the images.'	86	0	0	0
2281	-1	0	0	1	b"I was wondering if I should do this, because 2 out of 5 questions on Stack Overflow don't ever get answered, or if they do get (an) answer (s), most of the time they're not helpful.So I was thinking -- why not create a chat bot to answer Stack Overflow's questions &amp; provide necessary information to the general public?I mean why not? I've always been interested in AI, and all I'd need to do is create a basic logic database and a context system, pack an artificial personality with (partial) human instincts, and bam I'm done.But then again, would it be ethical?"	101	0	0	0
2282	2281	1	2955	4	b'Yes, it is possible, and has actually been done in the past.The University of Antwerp created a bot to answer questions (this is the technical report). It focused on the git tag only though (even though it did answer one mysql question).Its accuracy was pretty good, and the bots in the tests did earn some reputation. So I assume it is possible.But do note that the last bot in the tests revealed that it was a bot, and thus got banned. So if you reveal that the account you are running the bot on is a bot, there is a high chance that it will get banned.'	106	0	1	0
2283	2277	0	45463	2	b"Well, it depends of the level of the AI.You can create an AI super autonomous with deep learning capabilities and so on, but in the robotic type only. If you'd create an AI like EVA in the Ex-Machina movie, humanoid form, deep neural transmissions and with cognitive dissonance, then it could feel. The 'AI' problem its not the chemical and neural transmissions, its the consciousness."	64	0	0	0
2285	-1	0	0	4	b'What is the most advanced AI software humans have made to date and what does it do?'	16	0	0	0
2286	-1	0	0	1	b'Wikipedia\'s description of entropy breaks down the formula, but I still don\'t know how to determine the values of X and p(x), defined as "The proportion of the number of elements in class x to the number of elements in set S". Can anyone break this down further to explain how to find p(x)?'	53	0	0	0
2287	2285	1	9052	7	b"In my opinion, this would be Phaeaco, which was developed by Harry Foundalis at Douglas Hofstadter's CRCC research group.It takes noisy photographic images of Bongard problems as input and (using a variant of Hofstadter's 'Fluid Concepts' architecture) successfully deduces the required rule in many cases.Hofstadter has described the related success of CopyCat as being 'like a little kid doing a somersault': i.e. it doesn't have the flashy appeal of systems like AlphaGo. What it does however have is a much more flexible (i.e. not precanned) approach to perception of problem structure than other systems, which Hofstadter claims (and many including Peter Norvig agree) is the really hard problem."	107	0	1	0
2288	2281	0	27908	0	b"Technically, creating a non-human account on Stack Exchange would violate the Terms of Service. You would have to find some way to keep it from getting banned.That having been said, creating, and learning are always good things. It would be a somewhat complex task, but I'm sure you would learn a lot from it. There are plenty of bots out that use the questions and answers from Stack Exchange already, but none directly on the site. "	76	0	0	0
2289	2285	0	54914	-2	b'In my opinion this would be the Google search engine.It searches the web.'	12	0	0	0
2290	2285	0	73374	1	b'AlphaGo is the most sophisticated and closet human creation towards an Artificial General Intelligence (AGI). It is a computer program that is developed by Google DeepMind to play the board game "Go". The game is different than other games, as The number of potential legal board positions is greater than the number of atoms in the universe. It has way more legal board positions than the chess. So, AlphaGo requires different technique for it\'s development.Program\'s victories against the best players in the world in March 2016 is considered a major break through in the field of AI. Go was previously considered to be a hard problem and many experts believed that current technology is not enough. Experts were saying that it will take atleast 5 years (or may be 10 years) before we will have a well developed Go software player.The game used sophisticated algorithms of deep learning and reinforcement learning in order to learn the game. What makes this game different from other board game (like Chess, Reversi, etc.) is that moves are often based on intuition. If you ask a Chess player why he make a certain move, you will always be hearing an answer where he will explain you how he thought this move can increase in change of winning. Every move uses certain heuristics, strategy and/ or tricks. This is not the case with Go. Some moves are often taken because of intuition. Coding an AI software that can play a game, where intuition is a integral part of the game makes it different from other AIs that we have today. At present AlphaGo is the closest AI software to Artificial General Intelligence.You can go through these links for more information: 1. First 2. Second'	288	0	0	0
2291	1924	0	20166	0	b'Yes, it is possible. For instance, if your vision system can only track one object at a time and is currently tracking one, any other object in the scene cannot be tracked. So there is inattentional blindness.A feature like this could be used in artificial vision system as a means of "graceful degradation" when the available computation power is not enough to allow for the tracking/labelling of all elements of a scene.'	71	0	0	0
2292	2235	0	68601	3	b"For normal value iteration you need to have the model, i.e. the transition probability: P(s'|s,a). With Q-learning you use the current reward and the already stored Q value:And like commented in the video the V(s) function is simply the maximum value for a certain state:"	44	0	0	0
2295	2274	0	30067	3	b'Metaphorically: make it so depressed it commits suicide.As per my answer to this AI SE question, the idea is to feed it a sequence of inputs that will cause it to become (permanently) inactive.The technical details of how this might be achieved (and they are somewhat technical) can be found in this paper.'	52	0	1	0
2296	111	0	32667	3	b" \xe2\x80\x9cThis moral question of whom to save: 99 percent of our engineering work is to prevent these situations from happening at all.\xe2\x80\x9d \xe2\x80\x94Christoph von Hugo, Mercedes-Benz This quote is from an article titled Self-Driving Mercedes-Benzes Will Prioritize Occupant Safety over Pedestrians published OCTOBER 7, 2016 BY MICHAEL TAYLOR, retrieved 08 Nov 2016. Here's an excerpt that outlines what the technological, practical solution to the problem. The world\xe2\x80\x99s oldest carmaker no longer sees the problem, similar to the question from 1967 known as the Trolley Problem, as unanswerable. Rather than tying itself into moral and ethical knots in a crisis, Mercedes-Benz simply intends to program its self-driving cars to save the people inside the car. Every time.   All of Mercedes-Benz\xe2\x80\x99s future Level 4 and Level 5 autonomous cars will prioritize saving the people they carry, according to Christoph von Hugo, the automaker\xe2\x80\x99s manager of driver assistance systems and active safety.There article also contains the following fascinating paragraph.  A study released at midyear by Science magazine didn\xe2\x80\x99t clear the air, either. The majority of the 1928 people surveyed thought it would be ethically better for autonomous cars to sacrifice their occupants rather than crash into pedestrians. Yet the majority also said they wouldn\xe2\x80\x99t buy autonomous cars if the car prioritized pedestrian safety over their own. "	217	0	2	0
2298	2277	0	27719	9	b'There is much discussion in philosophy about inner language and the ability to perceive pain (see Pain in philosophy article). Your question is in the area of philosophy and not science. If you define emotion as some state then you can construct simple automata with two states (emotion vs no-emotion). It can be a very complicated state with degrees of truth (percentage of emotion).Basically, to mimic human emotion you need to make a living human-like organism, and still with todays understanding and technology you will not be able to recognize emotion in it. The only thing you can do is trust when it says "I\'m sad". Now we are in the area of the Turing test, which is again philosophy, and not science.'	122	0	0	0
2299	2274	0	84527	1	b"This seems to me like a virus situation.I'm not sure how modern DDOS attacks are resolved but similar strategy could be applied to this scenario."	24	0	0	0
2300	2226	0	67267	0	b"For a Markov Decision Process (MDP) a model which are the states (S), actions (A), rewards (R), and transition probabilites P(s'|s,a). The goal is to obtain the best action to do in each of the states, i.e. the policy &pi;.PolicyTo calculate the policy we make use of the Bellman equation:When starting to calculate the values we can simply start with:To improve this value we should take into account the next action which can be taken by the system and will result in a new reward:Here you take into account the reward of the current state s: R(s), and the weighted sum of possible future rewards. We use P(s'|s,a) to give the probility of reaching state s' from s with action a. &gamma; is a value between 0 and 1 and is called the discount factor because it reduces the importance of future rewards since these are uncertain. An often used value is &gamma;=0.95.When using value iteration this process is continued until the value function has converged, which means that the value function does not change significantly when doing new iterations:where &varepsilon; is a really small value.Discounted sum of future rewardsIf you look at the Bellman equation and execute it iteratively you'll see:This is like (without transition functions):To concludeSo when we start in state s we want to take the action that gives us the best total reward taking into account not only the current, or next state, but all possible next states until we reach the goal. These are the time steps you refer to, i.e. each action taken is done in a time step. And when we learn the policy we try to take into account as many time steps as possible to choose the best action.You can find quite a large number of examples if you search on the internet, for example in the slides of the CMU, the UC Berkeley or the UW."	315	0	2	0
2301	-1	0	0	1	b"I was looking for a service where I can ask it a general question (aka, when was Einstein born?) and retrieve an answer from the Web.Is there any available service to do that? Have tried Watson services but didn't work as expected.Thanks,"	41	0	0	0
2302	-1	0	0	2	b'At the moment I am working on a project which requires me to build a naive Bayes classifier. Right now I have a form online asking for people to submit a sentence and the subject of the sentence, in order to build a classifier to identify the subject of a sentence. But before I train the classifier I intend on processing all entries for the parts-of-speech and the location of the subject.So my training set will be formatted as:Sentence: Jake moved the chair &ensp;&ensp;&ensp; Subject: JakePOS-Tagged: NNP VBD DD NN &ensp;&ensp;&ensp; Location: 0Would this be an effective way to build the classifier, or is there a better method.'	107	0	0	0
2303	-1	0	0	1	b'What rectifier is better in general case of Convolutional Neural Network and how about empirical rules to use each type?ReLUPReLURReLUELULeacky ReLU'	20	0	0	0
2304	2277	0	33313	1	b'Yes and no. If you fully simulate a human brain and all of its functions, it would probably be able to feel emotions very similar to the way we do.But we don\'t have enough capabilities and knowledge to do that, and maybe we could find a "shortcut" - a process that is intelligent without simulating a whole brain. In this case, emotions would probably represented by data values which say "this is good (make it happen again!)", or "this is bad (avoid it!)". This is just a very basic example (there are obviously many more emotions), but it would have a similar function and the AI would have similar solutions to the ones we have. But we don\'t know - and probably no one ever will know - if this data value \'bad\' "feels" the same way for the AI the according emotion would feel to us. '	147	0	0	0
2305	2274	1	194	1	b"Nuke it from orbit - it's the only way to be sureIf you want to be really sure you destroy everything of the AI, you'll need to launch an EMP (ElectroMagneticPulse) from the orbit (there are different ways to achieve this, one would be an atomic bomb, but there are better ones). EMPs will destroy every electronic device it hits without causing really much damage to humans. Also an interesting read on a similar topic: https://what-if.xkcd.com/5/Especially this is gonna be interesting: [...] nuclear explosions generate powerful electromagnetic pulses. These EMPs overload and destroy delicate electronic circuits. [...] And nuclear weapons could actually give us an edge. If we managed to set any of them off in the upper atmosphere, the EMP effect would be much more powerful."	126	0	0	0
2306	-1	0	0	6	b'What are the top artificial intelligence journals?I am looking for general artificial intelligence research, not necessarily machine learning. '	18	0	0	0
2307	2306	0	14456	4	b'This link includes various journals for artificial intelligence applied to various domains.Some of those are:1. IEEE Transactions on Human-Machine Systems2. Journal of the ACM3. Knowledge-based systems4. IEEE Transactions on Pattern Analysis and Machine Intelligence5. Journal of Memory and Language.There are lots more. You can refer to any of those journals and explore the research done by AI enthusiasts and researchers.'	59	0	1	0
2308	2302	0	84612	1	b"Your approach would definitely work. I would recommend training a variety of classifiers and comparing their performance using multiclass roc analysis. Also, think about other useful features in addition to the ones you mentioned (e.g. pos tag). Feature engineering is one of the most important factors in building good predictive models. Another thing to keep in mind is that the classes could be highly imbalanced which might influence your model's performance."	70	0	0	0
2310	2306	0	60196	2	b"I most often reference: It's not a journal but it gets me where I need to go. "	17	0	1	0
2311	2306	0	66103	4	b'A couple of others:Journal of Artificial Intelligence Research (JAIR) - IEEE Transactions on Knowledge and Data EngineeringIEEE Computational Intelligence Magazine'	19	0	1	0
2312	2301	0	45146	1	b"You could use dbPedia and/or wikidata. I think Wikidata supports SPARQL now, but don't quote me on that. dbPedia definitely supports SPARQL. If you're not interested in writing SPARQL queries by hand, you could use something like Quepy. In fact, the Quepy demo demonstrates doing natural language queries against Freebase and/or dbPedia.You could possibly also incorporate OpenCyc.If you want to roll something of your own, you might want to read some / all of the research papers published by the team from the START project at MIT."	86	0	4	0
2313	2285	0	47358	4	b"In addition to the answers already posted, I think IBM's Watson deserves a mention. It did something pretty impressive with its Jeopardy win, possibly as impressive as AlphaGo. Sadly, since then, there don't seem to have been a lot of really public demos of Watson, as IBM is positioning the technology as a tool for companies and other organizations, and most of them are pretty secretive about the details of what they're doing. I think they did publicize a bit of information about using it for medical diagnosis, but that's the only other application I can think of off hand. I'm sure there are more though."	105	0	1	0
2314	-1	0	0	0	b'Is there any methodology to find proper parameter settings for a given meta-heuristic algorithm, eg. Firefly Algorithm or Cuckoo Search? Is this an open issue in optimization? Is extensive experimentation, measurements and intuition the only way to figure out which are the best settings? '	44	0	0	0
2315	2106	0	79752	2	b'Another way of seeing the differences between these models in the case of binary classification for instance between a class A and a class B:A generative model will be trained to model the properties of class A and another one will be trained to model the properties of class B. If we want to know if a new sample belongs to class A or B, we will compare it to each model and decide. The advantage is that we are able to synthetically generate more samples of these classes using the generative property of the model. The models have a "global knowledge" of what the classes are.On the other hand, a discriminative model will "pay attention" to what differentiates the 2 classes. It is more straightforward and often computationally less expensive as the model does not need to grasp everything about each class but only what makes them different.This is for the big picture. I find this course slides quite helpful to understand these concepts in more details (especially the first slides that are equation-free): '	175	0	1	0
2316	2314	0	44559	1	b"How to find the best configuration for an algorithm is an open research question in AI. The topic in general is known as `hyper-parameter optimization' and there are a range of possible methods:One of the most popular is IRace, but other possibilities include:Spearmint: uses wrappers in Matlab or Python. It uses MongoDb,and Bayesian optimisation algorithms.SMAC requires a python wrapper for the algorithm to be optimizedand has a command line interface.Hyperopt: a Python library which uses Random Search and Tree ofParzen Estimators.This paper argues that Spearmint performs the best, compared with SMAC and Hyperopt, but with significantly longer running times in some cases."	101	0	3	0
2317	2301	0	35276	1	b'You can use Googlehttps://encrypted.google.com/search?hl=en&amp;q=when%20was%20Einstein%20bornand parse the response.Wolfram ALPHA is another candidate.You can parse the returned html and see "Result:" div.'	19	0	1	0
2319	-1	0	0	8	b'In programming languages, there is a set of grammar rules which govern the construction of valid statements and expressions. These rules help in parsing the programs written by the user.Can there ever be a functionally complete set of grammar rules which can parse any statement in English (locale-specific) accurately and which can be possibly implemented for use in AI-based projects?I know that there are a lot of NLP Toolkits available online, but they are not that effective. Most of them are trained using specific corpuses which sometimes fail to infer some complex correlations between various parts of an expression.In other words, what I am asking is that if it is possible for a computer to parse a well-versed sentence written in English as if it were parsed by an adult English-speaking human?EDIT:If it cannot be represented using simple grammar rules, what kind of semantic structure can be used to generalize it?EDIT2: This paper proves the absence of context-freeness in natural languages. I am looking for a solution, even if it is too complex.'	172	0	0	0
2320	2274	0	43738	2	b'If an AI is developed by humans, we surely can create another one!Develop another AI agent without all the possible bugs that can make it go rogue to tackle the rogue AI, but more technically advanced than the previous one. Hardwire it with the sole purpose of disabling any rogue AI agent that can harm humanity and have it self-destruct in case it is corrupted.If the AI is really strong, it can anticipate every move of human resistance, but it cannot fathom the mind of another AI agent.'	87	0	0	0
2322	2319	0	83506	7	b' Can there ever be a functionally complete set of grammar rules which can parse any statement in English (locale-specific) accurately and which can be possibly implemented for use in AI-based projects?Parse it yes, accurately most likely no.Why ? According to my understanding on how we derive meaning from sounds, there are 2 complementary strategies:Grammar Rules:A rule based system for ordering words to facilitate communication, here meaning is derived from interaction of discrete sounds and their independent meaning, so you could parse a sentence based on a rule book.E.G. "This was a triumph" : the parser would extract a pronoun (This) with corresponding meaning ( a specific person or thing ) ; a verb (was) with corresponding meaning ( occurred ); ( a) and here we start with some parsing problems , what would the parser extract, a noun or an indefinite article ? An so we consult the grammar rule book, and settle for the meaning ( indefinite article any one of ), you have to parse the next word and refer to it though, but let\'s gloss over that for now, and finally (triumph) a noun ( it could also be a verb, but thanks to the grammar rule book we settled for a noun with meaning: ( victory,conquest), so in the end we have ( joining the meanings ):A specific thing occurred of victory. Close enough and I am glossing over a few other rules, but that\'s not the point, the other strategy is:A lexical dictionary (or lexicon)Where words or sounds are associated with specific meaning. Here meaning is derived from one or more words or sounds as a unit. This introduces the problem to a parser, since well, it shouldn\'t parse anything.E.G. "Non Plus Ultra" And so the AI parser would recognize that this phrase is not to be parsed and instead matched with meaning :The highest point or culminationLexical units introduce another issue in that they themselves could be part of the first example, and so you end up with recursion. if it is possible for a computer to parse a well-versed sentence written in English as if it were parsed by an adult English-speaking human?I believe it could be possible, most examples I\'ve seen deal effectively with the grammar rule book or the lexicon part, but I am not aware of a combination of both, but in terms of programming, it could happen.Unfortunately even if you solve this problem, your AI would not really understand things in the strict sense, but rather present you with very elaborate synonyms, additionally context (as mentioned in the comments) plays a role into the grammar and lexicon strategies.  If it cannot be represented using simple grammar rules, what kind of semantic structure can be used to generalize it?A mixed one where there are both grammar rules and a lexicon and both can change and be influenced based on the AI specific context and experience as well as a system for dealing with these objects could be one way.'	500	0	0	0
2323	2319	0	2841	2	b'I\'m pretty sure that the answer is "no" in the strictest sense, since English simply doesn\'t have a formal definition. That is, nobody controls English and publishes a formal grammar that everyone is required to adhere to. English is built up through an experiential process and it has contradictions and flaws, but the probabilistic nature of the human mind allows us to work around those.For example, that this "sentence":This sentence no verbTechnically it\'s not a sentence at all, since it doesn\'t have a verb. But did anybody have any problem understanding what it meant? Doubtful. Try coming up with a formal rule for that though. And that\'s just one example.Now, could you come up with a formal grammar that covers, maybe, 90% of cases, and is "good enough" for most practical uses? Possibly, maybe even probably. But I am pretty sure it\'s not possible to get to 100%.'	147	0	0	0
2324	-1	0	0	1	b"I was just doing some thinking and it occurred to me that the first AGIs ought to be able to perform the same sort and variety of tasks as people, with the most computationally strenuous tasks taking amount of time comparable to how long a person would take. If this is the case, and people have yet to develop basic AGI (meaning it's a difficult task), should we be concerned if AGI is developed? It would seem to me that any fears about a newly developed AGI in this case should be the same as fears about a newborn child."	99	0	0	0
2325	-1	0	0	0	b'https://github.com/bwilcox-1234/ChatScriptI gave AIML a brief look, but it seems to be in a nascent stage!'	14	0	0	0
2326	-1	0	0	0	b'Writing A* following a documentation. When run, i receive an error of "NameError: name \'parent\' is not defined" for the if statement, even though i have the name \'parent\' defined in the class State. May anyone point out my mistake.'	39	17	0	0
2327	2326	0	16360	2	b"The variable parent is only defined within the scope of the function _init_.Example:x is not defined outside of the scope of the function add(x,y) and will throw an error. If you'd like to do something with the class attributes you need to create a function like:where you can there reference self.parent"	50	4	0	0
2328	-1	0	0	0	b"How does one program a machine to have humanlike desires and intelligence?Humanlike drives may include self-awareness, purpose of existence, competent communication skills, and the ability to learn and to adapt in some environment ...And we should be able to combine IAs (intelligent agents) to accomplish well-defined goals (SMART). With more challenging goals there ought to be more advanced control and sophistication of IAs. That evolving process will eventually, hopefully, lead to the design of machines with humanlike capabilities. Reference links: 'Diagram of Intelligence Network or System', https://www.researchgate.net/publication/300125399_Diagram_of_Intelligence_Network_or_System;'Google a step closer to developing machines with human-like intelligence',https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence"	95	0	0	0
2329	2325	1	57806	0	b"Maybe you can give wit.ai a try, it's not open-sourced though.Also, have a look at api.ai and chatbots.io."	17	0	0	0
2330	-1	0	0	3	b"Based on fitting to historical data and extrapolation, when is it expected that the number of neurons in AI systems will equal those of the human brain?I'm interested in a possible direct replication of the human brain, which will need equal numbers of neurons.Of course, this assumes neurons which are equally capable as their biological counterparts, which development may happen at a faster or slower rate than the quantitative increase."	69	0	0	0
2331	2330	0	5884	6	b"Soon enough but that doesn't mean anything at all. In machine learning the word neuron represents a calculation whereas in brain the word neuron represent a specific type of cell which is a biochemical system."	34	0	0	0
2333	2330	0	11392	1	b"The answers so far haven't answered the question numerically, so here is my attempt to steer them in the direction I was seeking:The freely available Deep Learning Book has the following figure on page 27:I question the blue fit line, as it seems that data points may be better described by a parabolic or exponential function. In any case, based upon this conservative linear fit, the authors predict that the number of neurons in a ANN will equal that of the human brain in 2056.The referenced nerual networks are:What is interesting to note that when The Singularity is Near was written in 2006, Ray Kurzweil said that the refractory period of a biological neuron was already 1,000,000 times slower than that of an artificial one."	124	0	1	0
2334	2330	0	13376	7	b"Some back of the envelope calculations : number of neurons in AI systems The number of neurons in AI systems is a little tricky to calculate, Neural Networks and Deep Learning are 2 current AI systems as you call them, specifics are hard to come by (If someone has them please share), but data on parameters do exist, parameters are more analogous to synapses (connections) than neurons (the nodes in between connections) somewhere in the range of 100-160 billion is the current upper number for specialized networks.Deriving the number of neurons in AI systems from this number is a stretch since these AIs emulate certain types of connections and sub assemblies of neurons, but let's continue... equal those of the human brain?So now let's look at the brain, and again this are all contested numbers. Number of neurons ~ 86 Billion, Number of Synapses ~ 150 Trillion, another generalization: average number of synapses per neuron ~ 1,744.So now we have something to compare, and I can't stress this enough, these are all wonky numbers, so let's make our life a little easier and divide :Number of Synapses (Brain ) : 150 trillion / Number of parameters AIs : 150 billion = 1,000 or in other words current AIs would have to scale by a factor of one thousand their connections to be on par with the brain...Number of Neurons (Brain ) : 86 Billion / Number of Neurons AIs ( 150 billion / 1,744 ) = 86 Million equivalent AI NeuronsWhich makes sense, mathematically at least : you can multiply the factor ( 1000 ) times the current number of equivalent AI Neurons ( 86 million) to get the number of neurons in the human brain (86 Billion) When ?Well,let's use moore's law ( number of transistors processing power doubles about every 2 years ) as a rough measure of technological progress: So, if all this made sense to you, somewhere around the year 2035. "	325	16	0	0
2335	-1	0	0	1	b"I'm trying to find the optimized mixture for a specific set of substances. Each of those substances have characteristics that I want to optimize in the mixture (some characteristics I want to minimize and others I want to maximize). But I can't have more than 50% (random value that will be set on running time) of one of those substances in the mixture.I thought about using Genetic Algorithm, but I'm not sure it's the best approach for this problem. Do you have any suggestions?Edit: it doesn't need to be a evolutionary algorithm."	91	0	0	0
2337	2324	1	12764	1	b'There are basically two worries:If we create an AGI that is a slightly better AGI-programmer than its creators, it might be able to improve its own source code to become even more intelligent. Which would enable it to improve its source code even more etc. Such a selfimproving seed AI might very quickly become superintelligent. The other scenario is that intelligence is such a complicated algorithmic task, that when we finally crack it, there will be a significant hardware overhang. So the "intelligence algorithm" would be human level on 2030 hardware, but we figure it out in 2050. In that case we would immediately have superintelligent AI without ever creating human level AI. This scenario is especially likely because development often requires a lot of test runs to tweak parameters and try out different ideas. '	135	0	0	0
2338	-1	0	0	1	b'What are the current best estimates as to what year artificial intelligence will be able to score 100 points on the Stanford Binet IQ test?'	24	0	0	0
2339	2338	0	989	2	b"Nobody knows.However according to Kurzweil it's late 20s: 2020s:  Early in this decade, humanity will have the requisite hardware to emulate human intelligence within a $1000 personal computer, followed shortly by effective software models of human intelligence toward the middle of the decade: this will be enabled through the continuing exponential growth of brain-scanning technology, which is doubling in bandwidth, temporal and spatial resolution every year, and will be greatly amplified with nanotechnology, allowing us to have a detailed understanding of all the regions of the human brain and to aid in developing human-level machine intelligence by the end of this decade."	102	0	0	0
2341	2328	0	33548	2	b'Well, the low-hanging-fruit answer is that you simulate a human being - brain, hormones, everything. We should have the computing power for that to be feasible by 2040 or so.Building up self-awareness from first principles on a different foundational technology platform could be a bit more difficult!'	46	0	1	0
2342	-1	0	0	3	b'when I read through the fundamentals of AI, I saw a question which like the following picture and I need some helpsFrom the heuristic estimates:With using A* search method, node B will be expanded first because f(n)=1+9 while node A having f(n)=9+2, right?After that the search tree will go with the order like R-&gt; B-&gt; D-&gt; G2.Will the tree go to G1 goal states?Kindly let me know the order of the search if I am wrong.Thanks!'	75	1	0	0
2344	2342	0	31236	4	b'Yes. If you leave A* running (i.e. do not impose a goal condition on a newly-encountered state), all states will be explored, just as they would be in breadth- or depth- first search.'	32	0	0	0
2345	111	0	32828	0	b"I think that in most cases the car would default to reducing speed as a main option, rather than steering toward or away from a specific choice. As others have mentioned, having settings related to ethics is just a bad idea. What happens if two cars that are programmed with opposite ethical settings and are about to collide? The cars could potentially have a system to override the user settings and pick the most mutually beneficial solution. It's indeed an interesting concept, and one that definitely has to discussed and standardized before widespread implementation. Putting ethical decisions in a machines hands makes the resulting liability sometimes hard to picture."	108	0	0	0
2346	2338	0	73374	2	b"Not going into details of Stanford\xe2\x80\x93Binet test, but just looking at wikipedia page it shows many subtests like knowledge, reasoning, verbal tests etc. Most of the efforts in the artificial intelligence today is directed into research of specific areas like computer vision, natural language processing, machine learning, but also combination of fields like implementation of self driving cars.Within every field there are still other subfields and problems that are not solved yet. For example, development of human-like natural language processing (NLP) is necessary for intelligent agent to pass any verbal tests, or even non-verbal tests that requires processing of sentences of human language. Famous test that tests intelligence by asking questions in natural language and expects answers in the same form is Turing test. NLP still struggles with many (basic) human skills like listening, speaking, parsing and forming sentences. No one knows when we'll have system that can do these things as good as human. Since this system is crucial, but also far from human-like it's likely cause of delay in developing AI that passes intelligence test. Are these problems AI-hard? Do we need to develop strong AI to solve them?You can look at speech and listening as interfaces used for expressing and affecting inner processes of human brain. Same goes for other senses like eyesight which is being approximated by computer vision. One could say that we only need to develop convincing mimics of human senses and incorporate them in one big system that will become first human-like AI. That is the minimum requirement. I doubt this will be achieved in this century.(Other thoughts)What truly defines intelligence is brain activity. Since it's really complex and one artificial neuron is not equal to one neuron in brain, increase in computation power will not necessarily help achieving human-like AI. Also recognizing such system by mere intelligence test is questionable. For now it's only philosophical discussion but by the time we are able to design such machine I think we'll also have better understanding of human brain. Someone in 2100 might not read this answer on quantum computer with integrated AI OS powered from fusion reactor in his self-flying car, but will probably have many systems that help him in everyday tasks far more than we imagine today."	375	0	0	0
2347	-1	0	0	3	b'The same things we like when Amazon recommends what we might like to buy, allows advertising to manipulate us. It allows people to control the world differently.The algorithms social networks like Facebook use to "improve" our experience may also shape what news we consume. It may influence who we follow, altering our future experiences of the news.My question is: Will Artificial Intelligence some day become a problem to humanity after learning human behaviors and characteristics?'	74	0	0	0
2348	2347	0	3367	5	b" Will Artificial Intelligence some day become a problem to humanity after learning human behaviors and characteristics?It can be answered in both ways, I think.Yes, they may become a problem.With the increasing integration of loads of apps and smart devices in our life, almost everything defining an individual human being is digitalised. For instance, our fingerprints, voice, facial image etc. Apart from these data, we use those apps and devices to track our health (heart rate, calorie intake etc), to plan our schedules, and most importantly to communicate. If some sort of AI engine is integrated into a chat application, for instance, it can learn our typing patterns, conversation style, and hundreds of other unforeseen parameters. Imagine what can be learned about a person if such AI is coded inside every device and every app in your day-to-day use.We use smart devices and apps to harness their functionalities and features which ease our way of life, and we give them, unintentionally, our identity and sometimes, even our personality. For them, these are the parameters that can be input to some machine-learning algorithm and predict what we will do the next day, or what will happen to us the next day.This sounds like a major problem, especially when these technologies are indispensable.No, they may not become a problem.Humans are really complex creatures and possess the most advanced intelligence technology called the brain. I think the brain can be thought of as a technology in this context. There are several tissues inside the brain that can learn to do certain things themselves. It was proven in a research that some tissues have the capability to perform the functions of other tissues using neuro-rewiring techniques. Imagine that the same tissue that has helped you see until now can be made to help you hear instead. Now imagine mimicking such a technology.While it is not impossible, for an AI to achieve the brilliance of a human brain is a topic of ongoing research. To train a machine for the purpose, we would have to feed it with gazillion behaviors and characteristics, which it may not be able to handle! After learning some behavior and characteristics, the AI would be said to be smart, but it would still predate us.So, they may not become a problem at all because of our brain.The algorithms (like recommender systems) used by Amazon and Facebook influence us or even manipulate us. But, that manipulation is either very obvious (like viewing promoted products) or is in company's best interests (like viewing a certain news piece). It may be even possible that several external parameters are used by these systems to improve your experience. For instance, Google ads show us what we were looking for on an online store when we visit any random website. In most cases, what you see is a result of what you were looking for before. If any attempt to influence does happen, we may learn to avoid it through careful observation or even experience."	498	0	0	0
2349	-1	0	0	4	b'Background:I\'ve been interested in, and reading about, Neural Networks for several years, but I haven\'t gotten around to testing them out until recently. Both for fun and to increase my understanding, I tried to write a class library from scratch in .Net.For tests, I\'ve tried some simple functions, such as generating output identical to the input, working with the MNIST dataset, and a few binary functions (two input OR, AND and XOR, with two outputs: one for true, one for false).Everything seemed fine when I used a sigmoid function as the activation function but, reading of the ReLUs I decided to switch over for speed.My current problem is that, when I switch to using ReLUs, I found that I was unable to train a network of any complexity (tested from as few as 2 internal nodes up to a mesh of 100x100 nodes) to correctly function as an XOR gate. I see two possibilities here:1) My implementation is faulty,(This one is frustrating, as I\'ve re-written the code multiple times in various ways, and I still get the same result),2) Aside from being faster or slower to train, there are some problems that are impossible to solve given a specific activation function,(Fascinating idea, but I\'ve no idea if it\'s true or not)My inclination is to think that 1) above is correct. However, given the amount of time I\'ve invested, it would be nice if I could rule out 2) definitively before I spend even more time going over my implementation.Edit for specifics:For the XOR network, I have tried both using two inputs (0 for false, 1 for true), and using four inputs (each pair, one signals true and one false, per "bit" of input).I have also tried using 1 output (with a 1 (realy, >0.9) corresponding to true and a 0 (or &lt;0.1) corresponding to false), as well as two outputs (one signaling true and the other false).Each training epoch, I run against four sets of input: 00->0, 01->1, 10->1, 11->0.I find that the first three converge towards correct answer, but the final input (11) converges towards 1, even though I train it with an expected value of 0.'	357	0	0	0
2350	2349	0	77828	2	b'While I have not determined if there are problems which cannot be solved with ReLU, I have found ample documentation in the literature that XOR is solvable with as few as 1 hidden node. Therefore, I must assume there is something wrong with my implementation.Edit: The solution is simpler than I thought. The output layer needs connections, not just to the intermediate layer, but directly to the input layer as well. This allows the network to train XOR effectively.Edit 2: One final note, the XOR is EXTREMELY sensitive to the learning rate. Essentially, whatever learning rate is appropriate for the AND and OR functions, is approximately 1000x too large to train XOR effectively.'	112	0	0	0
2351	-1	0	0	2	b'Does anyone know, or can we deduce or infer with high probability from its characteristics, whether the neural network used on this site https://quickdraw.withgoogle.com/is a type of convolutional neural network (CNN)?'	30	0	0	0
2353	-1	0	0	2	b'After the explosion of fake news during the US election, and following the question about whether AIs can educate themselves via the internet, it is clear to me that any newly-launched AI will have a serious problem knowing what to believe (ie rely on as input for making predictions and decisions).Information provided by its creators could easily be false. Many AIs won\'t have access to cameras and sensors to verify things by their own observations.If there was to be some kind of verification system for information (like a "blockchain of truth", for example, or a system of "trusted sources"), how could that function, in practical terms? '	106	0	0	0
2354	2353	0	18713	3	b'Not possible without some big restrictions. What it can do is look at known "good" sites and compare news with site that is potentially "bad". Obvious problem here is defining some sites as absolute truth. For example it can recognize, while reading text, that some politician said something. These sentences can be compared with other sites, and if there is significant difference, that news is candidate for false news.In practical terms, program would extract sentences "i like cats", "says he likes cats", "cats that John likes" etc. We need part that recognizes something as a quote, part that extracts it and finally parser so we end up with structure stored in some form that contains meaning of sentence (john-like-cats). Also it can keep information of time and context in which it was said, like timestamp of an article, some proper nouns that can indicate place (XY conference, London...). Now, suspicious article can be compared and checked if it matches time, place, some context and contains quote that is similar. Finally it needs to compare how different it is from other quotes. "...hates cats" should be labeled as potential fake news, but "likes dogs", "thinks cats are OK", "sings well" etc. should not. This can be expanded into comparison of whole articles.There are many features that can be used to define particular article as fake. Interesting feature for finding fake sites could be bias when it comes to particular (political, economical, ecological...) opinion. But in the end machine can\'t decide if the article is fake without comparing it to other articles. It is bound to closed system that reflects real world in subjective way.'	272	0	0	0
2355	2351	1	7343	6	b'I believe they don\'t use CNNs. The most important reason why it\'s because they have more information than a regular image: time. The input they receive is a sequence of (x,y,t) as you draw on the screen, which they refer as "ink". This gives them the construction of the image for free, which a CNN would have to deduce by itself.They tried two approaches. Their currently most successful approach does the following:Detect parts of the ink that are candidates of being a characterUse a FeedForward Neural Network to do character recognition on those candidatesUse beam search and a language model to find most the most likely combination of results that results into a wordTheir second approach is using an LSTM (a type of Recurrent Neural Network) end-to-end. In their paper they say this was better in a couple languages.Source: I was an intern in Google\'s handwriting team in summer 2015 (on which I believe quickdraw is based), but the techniques I explained can be found in this paper.'	167	0	1	0
2356	-1	0	0	8	b'For example, would an AI be able to own property, evict tenants, acquire debt, employ, vote, or marry? What are the legal structures in place to implement a strong AI into society? '	32	0	0	0
2357	2356	0	4877	0	b'There is a legal difference between a "person" (which includes bodies corporate - corporations, incorporated associations, etc - and actual people) vs "natural person" (which is specifically a human being).For an AI to marry, it would need to get the legal definition of "natural person" changed, and depending on the jurisdiction possibly also the definition of "man" or "woman".For other things, such as owning property, evicting tenants, entering into contracts, etc, an AI would simply use a corporation. It may be that the corporation might need to have a minimum number of directors who are natural persons, but they could just be paid professionals, so no issue there.With credit cards, it would depend on the policy of the issuing bank. There is no legal impediment to corporations having credit cards in their own right, but in practice banks often require a director\'s guarantee from a natural person that they can sue if the bill is not paid. They want to be sure they will get their money, even if the corporation is wound up.'	173	0	0	0
2358	2277	0	14906	3	b'Emotions are a factor in humans having ethics/morals only because they are a factor in all human learning and decision-making.Unless you are duplicating a human being exactly, there is no reason to think that an AI will learn the way a human learns, or make decisions in the same way a human makes decisions.Therefore, whether it "feels emotion" just like we do, or whether it simply responds to outcomes "cost is greater = don\'t go there", the outcome of ethical BEHAVIOUR could be achieved. An AI could behave perfectly ethically without any need for feeling empathy, shame, etc.You could also argue that a lot of UNETHICAL behaviour in human beings is driven by emotions, too, and that an unemotional but ethical AI may well do a better overall job than a human being.'	132	0	0	0
2359	2356	1	11362	6	b"Yes, to some of what you propose. No to some.Today corporations are granted rights: to own property, earn income, pay taxes, contribute to political campaigns, offer opinion in public, ad more. Even now I see no reason why an AI should not be eligible to incorporate itself, thereby inheriting all these rights. Conversely, any corporation already in existence could become fully automated at any time (and some plausibly will). In doing so, they should not lose any of the rights and duties they currently employ.However I suspect certain rights would be unavailable to an AI just as they are unavailable to a corporation now: marriage, draft or voluntary service in the military, rights due a parent or child or spouse, estate inheritance, etc.Could this schizoid sense of human identity be resolved at some point? Sure. Already there have been numerous laws introduced and some passed elevating various nonhuman species to higher levels of civil rights that only humans heretofore enjoyed: chimpanzees, cetaceans, parrots and others have been identified as 'higher functioning' and longer lived, and so, are now protected from abuse in ways that food animals, pets, and lab animals are not. Once AI 'beings' arise that operate for years and express intelligence and emotions that approach human-level and lifetime, I would expect a political will to arise to define, establish, and defend their civil rights. And as humans become more cybernetically augmented, especially cognitively, the line that separates us from creatures of pure silicon will begin to blur. In time it will become unconscionable to overlook the rights of beings simply because they contain 'too little flesh'."	267	0	0	0
2360	2356	0	29386	2	b'Murray Shanahan, in his book The Technological Singularity, makes the case that the rights of any being are determined by its intelligence. For instance, we value the life of a dog above that of an ant and likewise value human life above that of other animals. From here one could argue that a general artificial intelligence of equal intelligence to a human should have equal rights to a human and a superior artificial intelligence should have more rights.The question, of course, is whether our anthropocentric society would be willing to accept this fundamental shift in human rights and this idea of removing humanity from its pedestal of importance.When it comes to legal frameworks, we really are entering into uncharted territory as AI is going to have to revolutionise the way we define many of the terms we take for granted today and question many of our usual assumptions. AI is going to drive an important shift in our mindset well before it exceeds human intelligence.'	164	0	0	0
2361	2277	0	39498	7	b" It is certainly possible for AI to theoretically feel emotion. There are, according to Murray Shanahan's book The Technological Singularity, two primary forms of AI: 1) Human based AI - achieved through processes such as whole brain emulation, the functioning of human based AI would likely be indistinguishable from that of the human brain, and, as a consequence, human based AI would likely experience emotion in the same manner as humans.- 2) AI from scratch - with this form of AI, based on machine learning algorithms and complex processes to drive goals, we enter into uncharted territory as the development of this form of AI is inherently unpredictable and unlike anything we observe in the biological sample space of intelligence we have access to.  With this form of AI, there is no telling if and how it could experience emotion.As the question references the former, it is very likely that human-based AI would indeed experience emotion and other human-like characteristics."	161	0	0	0
2363	-1	0	0	4	b'I have implemented a Sobel Filter for edge detection in Matlab without using its toolbox. I am a bit confused: Is a Sobel filter a type of Cellular Neural Network?Both Sobel and Cellular Neural Network calculate output via its neighborhood cells.'	40	0	0	0
2364	2363	1	33123	2	b"You're right about the basic arrangement of the inputs, but there are a number of differences:Artificial neural networks typically use exemplar data as inputs for the purpose of training, or adjusting the weights of its internal connections, to accurately classify them within a certain error range. The network is then applied to unknown data to classify them. Edge detection filters are just blind operators that transform input data regardless of how it can be classified. There is no training, so any intelligence exists only in the mind of the filter developer.A CNN could be trained to be an effective Sobel (edge detection) filter, as described in this paper, but a Sobel filter couldn't be an effective learning algorithm.Training neural networks is more non-deterministic, with outputs depending on what data they are trained with and potentially even the operational computations that are used for classification. Applying filters is typically deterministic, i.e. they will transform the same data exactly the same way if applied twice.One succinct way of expressing the biggest difference is: a cellular neural network is looking for a function, while a Sobel filter is a function.Note that there are types of neural networks called Convolutional Neural Networks, which can use Sobel and other filters in their input layers, as described here. Though, these are neither of the things you are asking about. :)"	223	0	2	0
2365	2356	0	63602	1	b"Not only wouldn't a strong AI which came into existence today have the rights a human has, or any rights (see these discussions of the implementation of regulation for weak AIs at: The White House and The American Bar Association), but it seems unlikely the first one will.Observing that:Having rights implies that there are restrictions, which means there would have to be a system of control. However the control problem in AI is still unsolved.Even assuming that problem is solvable, an AGI would then have to appear equivalent to natural humans. They don't yet (see Turing Test Passed?), and even after passing equivalence tests, are unlikely to remain that way, per the Singularity Hypothesis.Further, if one or more AGIs were to be human-equivalent long enough to desire rights, lawmakers (in the US) would have to re-interpret the definition of personhood and grant them rights, as they did for corporations in 1886."	150	0	3	0
2366	-1	0	0	5	b'If someone wants to develop a basic AI with some code modules,Let us say the AI just has to provide an action when stimulated in a certain situation based on its previous understanding of situations. I can think of at least 3 of such components:Real-time Understanding/Learning: Using Deep Learning/ConvNets, Supervised/Unsupervised.Logical Decision-Making: Calculating the results of various decisions when applied on current situation based on previous understanding and choosing the most appropriate one logically.Action/Reaction: Acting precisely in the new situation according to the decision-made.Any ideas?'	83	0	0	0
2367	-1	0	0	0	b'Let\'s say I have a string "America" and I want to convert it into a number to feed into a machine learning algorithm. If I use two digits for each letter, e.g. A = 01, B = 02 and so on, then the word "America" will be converted to 01XXXXXXXXXX01 (1011). This is a very high number for a long int, and many words longer than "America" are expected. How can I deal with this problem?Suggest an algorithm for efficient and meaningful conversions.'	82	0	0	0
2368	2367	0	13783	1	b'What are you trying to achieve?If you need to encode it to some integer use hash table. If you are using something like linear regression or neural network it would be better to use dummy features (one-hot encoding). So for your dictionary of 5 words ("America", "Brazil", "Chile", "Denmark", "Estonia") you get 5 features (x1, x2, x3, x4, x5) which indicate if some word is equal to one in dictionary. So "Brazil" is represented by (0,1,0,0,0), "Germany" is (0,0,0,0,0). Number of features grows with number of words in dictionary making some features practically useless.If you are using decision trees you don\'t need to convert string to integer unless specific algorithm asks you to do so. Again, use hash table to do it. In R you can use factor() function.If you convert your string to integers and use it as single feature ("America" - 123, "Brazil" - 245), algorithm will try to find patterns in it by comparing numbers but may fail to recognize specific countries.'	164	0	0	0
2369	2367	0	16667	2	b"This depends a lot on what you want to achieve, but if you aim to generalise beyond the words encountered in your training data, you should consider using something like word2vec. In word2vec semantically similar words are represented by similar vectors and what's more, semantic differences translate into geometrical differences. To overuse a standard example: vec(Paris)-vec(France)+vec(Italy)=vec(Rome).These relationships allow the network to generalise to completely new content."	65	0	0	0
2370	-1	0	0	3	b"I'm trying to understand Boltzmann machines. Tutorials explain it with two formulas.Logistic function for the probability of single units:and, when the machine is running, every state of the machine goes to the probability:so, the state depends on the units, and then if I understand correctly, the second formula is a consequence of the first; so, how can it be the proof that the distribution of $p(state)$ is a consequence of $p(unit)$?"	70	2	0	0
2371	-1	0	0	2	b'I installed a local running instance of the ConceptNet5 knowledgebase in an elasticsearch server. I used this data to implement the so-called "Analogietechnik" (a creativity technique to solve a problem from the perspective of another system) as an algorithm.The technique works as follows:Choose a Feature of a SystemFind Systems who have this feature alsoSolve the problem from the perspective of these other systemsApply the found solutions to the issueAs an example is here the problem of marketing a shopping mall: A Shopping mall has many rooms and floors (1). A museum has also many rooms and floors (2). How are museums marketed? They present many pictures or sculptures (3). We could use our rooms and floors to decorate them with pictures and sculptures (4).Of course the idea to implement that as an artifically intelligent algorithm was not far. However, I feel a little bit overwhelmed by the amount of methods that exist out there. Neural Networks, Bayesian Interference and so on... My current experience doesn\'t go further than simple machine learning like kMeans-Clustering for example. Do you think it would be very hard to find a solution for this problem? I\'m thinking of a console application, where you can enter a conceptualized problem like "methods for creative writing", for example, and it uses the above method to find possible solutions of the issue. Of course no solution with extensive depth, more something like basic ideas derived from the knowledge database I have.Lets take as an example a console application where someone asks "how to write a novel":It should find out first that the system all is about is in the term "novel". To find a feature of that system it just searches concepts containing that term: it finds out "Novel is a story" So thats a feature.Which systems are also stories? A good concept it should find is e.g. "Plot is a story". (Of course only when I am selecting the search results manually)--> How to find best concepts of a list when not knowing which fits best?It should then find out that a plot is written using a storyline: "storyline is a plot"One possible answer of the AI would in this case be: "By writing a storyline"Do you know some helpful libraries, algorithms or other resources that might help me? I know this is not an easy thing to program, but you might agree that its highly interesting.'	398	0	1	0
2372	2286	0	12072	1	b'Suppose you have data:To calculate the entropy for quality in this example:Probability of each x in X:for which logarithms are:and therefore entropy for the set is:by the formula in the question.Remaining tasks are to iterate this process for each attribute to form the nodes of the tree.'	46	18	0	0
2373	-1	0	0	2	b'Given the advantage AI already has over human intelligence, one could imagine a relatively weak strong-AI (barely human intelligence) still outperforming a segment of the human scientist population in terms of scientific discoveries per year (or hour).Will AIs be doing most of the science in 50 years?'	46	0	0	0
2374	2373	1	41078	4	b'According to Ray Kurzweil, a prominent AI researcher, yes. In his book The Singularity is Near he predicts that AIs will take over developing other AIs in about 30 years, after which human intelligence will become marginalised.'	36	0	1	0
2375	2353	0	33138	0	b'Input -> Prediction -> Output -> Input -> Prediction -> Output -> Input -> ...AGI can easily determine which input is true/real. It will use the same method which every organism uses: any input is true and real, unless you misidentified some other stuff as "input".I would define input as: what crosses the boundary and enters your mind from outside of your mind. The minimum hardwired check is to make sure that signals generated inside a mind are not misidentified as coming from outside (aka "I hear voices"). That\'s all. This is where the blockchain of truth begins and where it ends.An Internet article? The input to AI is rather: one of AI\'s network interfaces received many bytes. Once it\'s verified they are from the network, and not imaginary, they cannot be unreal or untrue in any meaningful way. By that definition of input, it is in fact the only thing we can be sure is true and real.Of course AI will likely form hypotheses regarding these bytes that happen to contain ASCII strings like "Trump", "John Smith", "ice balls on Siberian beaches". Then AI will hopefully make predictions based on these hypotheses, maybe interact, maybe get some new input, reject the hypothesis and make a new one, rinse and repeat.The first hypothesis will be super-naive, but the hundredth, the thousandth?If you end this process prematurely - maybe for lack of processing power - you will get something you called a "belief". (Like a belief that some emotional web page might actually reveal a significant truth about our political system.) That belief is a synonym of "tired with trying new hypotheses, will stick to this one". Typical human thing. AI will have less of that, I hope, due to having much much more processing capabilities. AI will stick less to the high-school-level truth that you should assign great credibility to statements written in a form of a newspaper article, it will hopefully form more and more generations of hypotheses, and check them.In effect AI will depend less on believing various statements generated in the outside world.'	345	0	0	0
2376	2277	1	64059	3	b'I have considered much of the responses here, and I would suggest that most people here have missed the point when answering the question about emotions.The problems is, scientists keep looking for a single solution as to what emotions are. This is akin to looking for a single shape that will fit all different shaped slots.Also, what is ignored is that animals are just as capable of emotions and emotional states as we are:When looking on Youtube for insects fighting each other, or competing or courting, it should be clear that simple creatures experience them too!When I challenge people about emotions, I suggest to them to go to Corinthians 13 - which describes the attributes of love. If you consider all those attributes, one should notice that an actual "feeling" is not required for fulfilling any of them.Therefore, the suggestion that a psychopath lacks emotions, and so he commits crimes or other pursuits outside of "normal" boundaries is far from true, especially when one considers the various records left to us from court cases and perhaps psychological evaluation - which show us that they do act out of "strong" emotions.It should be considered that a psychopath\'s behaviour is motivated out of negative emotions and emotional states with a distinct lack of or disregard of morality and a disregard of conscience. Psychopaths "enjoy" what they do.I am strongly suggesting to all that we are blinded by our reasoning, and by the reasoning of others.Though I do agree with the following quote mentioned before: -Dave H. wrote: From a computational standpoint, emotions represent global state that influences a lot of other processing. Hormones etc. are basically just implementation. A sentient or sapient computer certainly could experience emotions, if it was structured in such a way as to have such global states affecting its thinking.However, his reasoning below it (that quote) is also seriously flawed.Emotions are both active and passive: They are triggered by thoughts and they trigger our thoughts; Emotions are a mental state and a behaviourial quality; Emotions react to stimuli or measure our responses to them; Emotions are independant regulators and moderators; Yet they provoke our focus and attention to specific criteria; and they help us when intuition and emotion agree or they hinder us when conscience or will clash.A computer has the same potential as us to feel emotions, but the skill of implementing emotions is much more sophisticated than the one solution fits all answer people are seeking here.Also, if anyone argues that emotions are simply "states" where a response or responses can be designed around it, really does not understand the complexity of emotions; the "freedom" emotions and thoughts have independently of each other; or what constitutes true thought!Programmers and scientists are notorious for "simulating" the real experiences of emotions or intelligence, without understanding the intimate complexities; Thinking that in finding the perfect simulation they have "discovered" the real experience.The Psi-theory seems to adequately give a proper understanding of the matter: https://en.wikipedia.org/wiki/Psi-theorySo I would say that the simulation of emotional states "is" equivalent to experiencing emotions, but those emotional states are far more complex than what most realise.'	520	0	0	0
2377	2353	0	52154	1	b'I strongly disagree with all of the aforementioned answers for this reason: -If we, as humans can be fooled and disceived by what "we" consider a good sources of news, how can an artificially intelligent computer have any chance?However, the challenge would be that an AI would have to be able to "test" a source of information against a known medium in order to get to the truth. This is a far different dynamic set of circumstances than what has been touted above.For example, if it was claimed by a woman that a man raped her - which was not reported to the police - it is not enough to compare one person\'s statements to another in order to determine truth. This is because collusion, influenced or coherced third parties, mistaken perceptions and false beliefs would give false positives.However, if an AI could establish from her statement that on the day she claimed to have been raped, that the alleged assailant was incapacitated while in her company, until she left his home, because the police report stated that she was upset with the assailant because he was asleep because of drugs during her whole stay. But, this police report comes from an independent source who states, Mr. "x" was asleep that day.Doing a strict textual check is not going to give the correct answers. analysing her friends and associattes chatter could also confirm a false report as being true.Therefore, an AI has to have the ability to "test" written reports outside of the criteria of what was spoken.'	257	0	0	0
2378	2373	0	79388	0	b"I don't think so, it is not the first but actually the third wave of neural networks. It's doing better than earlier two as we have much more amount of data as well as computational power now. Take a look at this video .....According to Douglas Adams\xe2\x80\x99s famous \xe2\x80\x9cHitchhiker\xe2\x80\x99s Guide to the Galaxy\xe2\x80\x9d after 7.5 millions years of work the \xe2\x80\x9cDeep Thought\xe2\x80\x9d computer categorically found out that 42 is the \xe2\x80\x9cAnswer to the Ultimate Question of Life, the Universe, and Everything\xe2\x80\x9d (although unfortunately, no one knows exactly what that question was)."	90	0	0	0
2380	2373	0	77556	0	b'Current Computing relates 0 or 1 to another 0 or 1 with layers upon layers of building blocks built on this relationship.In future AI will relate A to B, be they numbers, patterns of coded instructions or some other form of more complicated constructs at a hardware (or closer to it) level than is currently possible and, due to the inherent perfect recall and potentially massive memory storage of AI they will most definitely be bringing together and relating a vastly more broad and organised collection of knowledge than it is possible for any human to even contemplate consciously. There are some ideas that would argue that point; universal mind, spiritual revelation and morphic resonance which I do personally agree with to some degree and can imagine being particularly difficult to represent in a computational format.Pattern spotting and relating, organising and computing potentials... computers are already better at all these things than most people try to be. It will not be long, i think, before they can "invent" something "new".There are already attempts at AI in specialised fields of knowledge, human speech, various games, medical diagnostics and learning etc. It will be when these specialised AIs can "compare notes" about the various methodologies that have been the most productive or rewarding, in whichever form these take for them, and accordingly update their own ontologies that the true explosion of "Intelligence" will occur.'	231	0	0	0
2381	-1	0	0	2	b"OpenCog is an open source AGI-project co-founded by the mercurial AI researcher Ben Goertzel. Now Ben Goertzel writes a lot of stuff, some of it really whacky. On the other hand he is clearly very intelligent and has thought deeply about AI for many decades. So I wonder whether it would be worth my while to dig into the theoretical ideas behind open cog. My question is what the general ideas behind open cog are and whether you would endorse it as a insightful take on AGI. I'm especially interested in whether the general framework still makes sense in the light of recent advances. "	104	0	4	0
2384	2349	1	8184	1	b"There are a variety of possible things that could be wrong, but to answer the short question specifically:relu networks are turing complete (well, if you put them in an RNN so they can compute indefinitely, anyway). for any computation, you can devise an rnn that will perform it. as a proof of this, here is a relu neuron that implements nor, which with recursion (cs)/recurrence (nns) and routing matrices is enough to implement a turing machine:W:[ -20 -20 ]b:[ 1 ]o = max(Wx + b, 0)however, gradient descent is a finnicky way to search for rnns. there are a wide variety of ways that it might have been failing. In general, once you have very thoroughly checked your gradient, I'd make sure to use Adam as the optimizer and then play with the hyperparameters endlessly until I find an incantation that works.  "	143	0	2	0
2385	2353	0	62809	0	b'While the experiment I link here is a very narrow awareness, it is as such: A robot has just passed a classic self-awareness test for the first time. If the agent can prove something to itself, we can then say it "Knows." Of course the level of awareness you\'re asking about is very tricky.In short, it can\'t know that what it\'s experiencing is real with absolute certainty because sensory of any kind can be falsified. Do you know what is true/real? You think you do but can you prove it? No. Awareness is subjective.'	93	0	1	0
2386	2381	0	49417	2	b"While my knowledge of OpenCog is very limited, you could say that yes, it does still make sense and it is insightful. I'm not certain regarding all of the components of OpenCog but I do know that at least one component is relevant (I think it's part of the MOSIS component).This component is very similar to Numenta's hierarchical temporal memory which is based more on computational neuroscience than plain math; however, I would consider Nupic a more relevant project in terms of neroscience though both are attempting to emulate components of the brain. In my opinion, such projects are far more impressive than what's going on with typical convolutional neural nets, RNNs, etc. which are too loosely related to what goes on in the brain to be said to be computational neuroscience.That's not to say that things like ANNs, GAs, etc etc are useless for AGI. We don't really know since we don't have an example of one."	157	0	0	0
2387	2371	0	29189	0	b"Hierarchical Temporal Memory should help with this. You would have to encode the text data into SDRs. You would then have a coincidence detector. Could you get the right information back out in the way you're trying to? I think so. I'm not fully learned in HTMs yet but check out Nupic (Open source)."	53	0	1	0
2388	2277	0	22415	2	b"This question is more the province of philosophy of mind than of AI, here are some detailed answers to your question from the philosophy SE: Is simulating emotions the same as experiencing emotions?, and What is the problem with physicalism?. For the record, the accepted answer (by Siri) to the question is not entirely correct (The position in that answer corresponds roughly to John Searle's view on the question, and his is a minority view): Dualists would argue that even with a perfect replication down to the chemical level of brain interactions, an AI still wouldn't experience emotions, as it lacks the purely mental substance/properties that make a mind and not a machine. On the completely opposite side of the spectrum, functionalists would answer that such a perfect replication is overkill: even a suitably programmed digital computer can experience emotion, particularly if one equips it with higher-order and self-referential states. "	150	0	3	0
2389	-1	0	0	0	b'I am new to machine learning and I know how to implement simple neural networks using logistic regression as a cost function. But I want to know whether neural nets are used in reinforcement learning in general(and not just in special cases) ? If yes, then what cost function they use. As I already know neural nets with logistic regression are used in supervised learning and reinforcement learning is a part of unsupervised learning. There are many threads which are related to RL and neural nets but all of them are about a particular case or an algorithm and I want to know about RL in general.'	106	0	0	0
2390	2366	0	59063	4	b"Some good places to start would be cognitive architectures and as mentioned in another answer intelligent agents. The question is broad but you definitely want to look into planning &amp; decision making. You might also want to check out the L5 and L6 layers of Hierarchical Temporal Memory (As in Nupic) as it relates to feedback, behavior and attention.If I were you I'd aim for more cognitive solutions (I realize that term is a bit ambiguous itself when we talk about machines). There's also new AI initiative going on involving probabilistic programming. See Probabilistic Models of Cognition made by Goodman (Stanford University) and Tenenbaum (MIT) or Anglican made by Wood (University of Oxford) et al."	114	0	4	0
2392	-1	0	0	2	b"My Question:Is there any good neural-network-app for iOS or Android to create, train and run neural networks? I know there's NeuralMesh for Web, but I want something similar offline."	28	0	0	0
2393	2277	0	85475	1	b'You first need to express emotions, you can do that without the aid of AI, and then you need someone to perceive that expression and empathize with it.If no one is there to see it, or if I am psychopath, I would probably say it doesn\'t have emotions. and for that, it is irrelevant/subjective.If you can empathize with characters in movies who "act" emotions, then you get my point.'	68	0	0	0
2394	2356	0	77284	3	b'No matter what rights it gets (as a company), it will still lack the right of not getting liquefied and all its properties transferred back to natural persons.This is of course if no laws are changed.To change the laws you will need to convince people that this machine is more "life" worthy than intelligent animals, and hope that people will deal with them better than they did with dolphins and chimps.As I see it, machines can easily get the same or better rights then companies, but will always be under the mercy of the less intelligent man. (that is if things went peacefully :) )'	104	0	0	0
2395	2392	0	34998	1	b'You can use Neuroph to develop and train your network and add in app via NetBeans. check this linkCreating Android image recognition application using NetBeans and Neuroph.'	26	0	2	0
2396	2330	0	44346	0	b"The human brain contains billions of neurons, which means we won't be making one tomorrow. However, technology tends to advance in an exponential manner, and that may soon be a real possibility. Also, the idea of making an artificial human brain would not only take more neurons than a current average computer could process, or we could make outside of computers, but we also need an understanding of the human brain. There is only one animal with neurons that we have completed a full connectome of and that is the Caenorhabditis elegans (roundworm) and it has less than 500 neurons. It may be a while before we actually make a human brain, but within 30 years is a reasonable estimation with the rate that technology improves now."	126	0	0	0
2397	2367	0	38106	0	b"You shouldn't use a single number for the word, perhaps a number for each letter. Since B isn't the midpoint of A and C, the numbers really shouldn't be 1, 2, 3, etc. One large but effective way of converting is the letter a is 10000000000000000000000000 such that there are 26 digits, and each digit is a letter, so 0000100000... would be E."	62	0	0	0
2398	-1	0	0	2	b"I am researching Cellular Neural Network (CNN) and have already read Chua's two article (1988). In CNN, the cell is only in relation with its neighbors. So its is easy to use it for real time image processing. In CNN, image processing is performed with only 19 numbers (two 3x3 matrix called A and B and one bias value). I wonder how can we call CNN as a neural network. Because there is no learning algorithm in CNN neither supervised nor unsupervised. "	82	0	0	0
2399	2277	0	62927	1	b'IMHODefinitely, yes!Everything that person feels (physically or mentally) can be discovered by chemical signals processing in his body or brain. If we understand the policy and nature of such signals, we can program it.There are a lot of pseudo-psychology and psychology works on this sphere, if you interested, I can suggest you: 1) Cognitive Psychology (Robert L. Solso)describes cognitive apparat of human\'s mind in a simple words; 2) The Psychology of Emotions (Carroll E. Izard)thorougly describes every kind of emotion by its looking on the human (both child and adult) face, low-level cognitive mechanism, related or adjacent emotions; 3) Books by Paul Ekman ("Telling Lies", "Emotions Revealed", "Unmasking the Face")practical detecting of human emotions by microexpressions language on face and body.'	120	0	0	0
2400	-1	0	0	-5	b"Could an Artificial Intelligence be able to interact (see, talk, etc.) with someone even when there's no power cord connected to the machine it's running on? Might it find some way to generate its own electricity to power that computer? "	40	0	0	0
2401	2400	0	4365	2	b'If your "AI" doesn\'t have the ability to move and perform physical manipulations in the real world then there is no way it could do something like this.'	27	0	0	0
2402	2400	0	33066	0	b'If the computer is unplugged, the AI is clinically dead.However, you can have a RaspberryPi on solar cells.Tesla car is an AI moving and seeing while unplugged (from wall). but you have to have some sort of energy. For AI that lacks metabolism, solar/wind energy can be an alternative.'	48	0	0	0
2403	-1	0	0	1	b'I am new to deep learning. I have a dataset of images of varying dimensions of a certain object. A few images of the object are also in varying orientations. The objective is to learn the features of the object (using Autoencoders). Is it possible to create a network with layers that account for varying dimensions and orientations of the input image, or should I strictly consider a dataset containing images of uniform dimensions? What is the necessary criteria of an eligible dataset to be used for training a Deep Network in general.The idea is, I want to avoid pre-processing my dataset by normalizing it via scaling, re-orienting operations etc. I would like my network to account for the variability in dimensions and orientations. Please point me to resources for the same. '	132	0	0	0
2404	-1	0	0	5	b'A single neuron is capable of forming a decision boundary between linearly seperable data. Is there any intuition as to how many, and in what configuration, would be necessary to correctly approximate a sinusoidal decision boundary?Thanks'	35	0	0	0
2405	-1	0	0	1	b"I am using policy gradients in my reinforcement learning algorithm, and occasionally my environment provides a severe penalty when a wrong move is made. I'm using a neural network with stochastic gradient decent to learn the policy. To do this, my loss is essentially the cross-entropy loss of the action distribution multiplied by the discounted rewards, where most often the rewards are positive. But how do I handle negative rewards? Since the loss will occasionally go negative, it will think these actions are very good, and will strengthen the weights in the direction of the penalties. Is this correct, and if so, what can I do about it?Edit:In thinking about this a little more, SGD doesn't necessarily directly weaken weights, it only strengthens weights in the direction of the gradient and as a side-effect, weights get diminished for other states outside the gradient, correct? So I can simply set reward=0 when the reward is negative, and those states will be ignored in the gradient update. It still seems unproductive to not account for states that are really bad, and it'd be nice to include them somehow. Unless I'm misunderstanding something fundamental here."	191	0	0	0
2407	1989	0	65583	3	b'As Matthew Graves explained in another answer No free lunch theorem confirms the flexibility - efficiency trade-off. However, this theorem is describing a situation where you have a set of completely independent tasks. This often doesn\'t hold, as many different problems are equivalent in their core or at least have some overlap. Then you can do something called "transfer learning", which means that by training to solve one task you also learn something about solving another one (or possibly multiple different tasks).For example in Policy Distillation by Rusu et al. they managed to "distill" knowledge from different expert networks into one general network which in the end outperformed each of the experts. The experts were trained for specific tasks while the generalist learned the final policy from these "teachers".'	128	0	0	0
2408	2400	0	68358	0	b'I assume by your use of the term "plugged in", you are referring to an electron based computer - the usual definition of, or what is commonly/popularly considered to be, a computer - which is hosting the AI. However, what your definition of plugged in is unclear. If you mean literally, and physically, plugged in to the wall then yes, as Aus points out, because one can use a battery, or have a solar powered circuit, or petrol/diesel/steam/water/wind/hydrogen/etc. powered engine, running a generator/alternator which can eventually supply a voltage and current to power the circuitry. However, in these examples, you have merely moved the point of generation, from a remote power station to a locally generated source of electricity - so it is still, in effect, plugged in (but to a local source). So, in that sense, the answer is no. If you mean no external power source, what so ever, then the answer is most certainly no, as, again as Aus has already said, there is no way for electrons to be pushed around the circuit, and therefore the circuit is dead1.If you mean, as you said in your comment that the AI has created its own electrical power source - although as Ankur says in their answer, this implies that the AI has the ability for its I/O manipulate its environment2 - then as in the first paragraph, you have simply moved the point of the power source, so one can say that it is again now plugged in, again to a local energy source, and so yet again the answer is no.However, if you are not referring to an electronic based computer, which is hosting the AI, but are, instead, referring to a hypothetical biological computer, then the answer is probably yes - in a sense. Although, that biological computer would still require energy input, as energy can\'t just be "magicked" out of thin air. It would need to be converted from one source into a from that the biological computer can use. That energy could come from a variety of sources, be it from sunlight, food, heat (sunlight, deep sea vents, etc), so one could argue that it is still plugged in to an energy source and so yet again, the answer is no.To be honest, in order to give an accurate, and sensible answer, you really need to reword your question, clarify what you mean and what your definitions of plugged in and AI host are. Unfortunately, your question, as it stands, is rather unanswerable, in as much that it is difficult to give a definitive answer.Footnote1Likewise, if you mean by plugged in in the conventional sense, and therefore unplugged means that the electronics hosting the AI has no power then the answer is no.2 However, assuming that the AI can manipulate its environment, in order for the AI to do able to do so, it will have be able to assemble the external power source prior to being unplugged. So... if you mean "Can it create its own power, after it was powered off (without having had time to asemble an external power source to act as a backup)?" then the answer would be no.'	530	0	4	0
2409	-1	0	0	1	b'Cognitive Psychology is one of the basic sciences of artificial intelligence (AI). The founder of the psychology is Wilhelm W.(1832-1920), who engaged in empirical methods,and was interested in the thinking processes during his scientific work.According to his research,Psychology had two main leading subjects: Behaviourism.Cognitivism.Behaviourism: Refused the theory of the mental processes, and insisted to study the resulted action or the stimulus strictly objective. The representatives of this theory have been decreasing with time.Cognitive psychology: defines that the brain is an information processing device.Therefore,this question is not a duplicate of this what-is-the-difference-between-artificial-intelligence-and-cognitive-science? ,However my question is;how can we connect artificial intelligence with cognitive psychology for instance;Human Computing Interaction:We may come in a contact with Humana Computer Interaction every day, because this field includes the every day use of computer for example;tapping stack exchange app on smart-phone, the user interfaces and some other expert programs which may use cognitive psychology in order to manipulate or help people. But still such tasks have got a minimal relevant connection.'	164	0	1	0
2410	2400	0	43900	0	b"Depends on how your AI works. if it is making decisions using electric currency (like a computer processor), it would obviously need some source of current. If the AI working with chemical reactions, however, it could work with chemical energy stored in sugars and fats. That is basically how every animal's brain works. But still, animals need to eat to perform these chemical reactions which trigger other reactions and will cause muscles to contract, cells to grow, etc etc. Everything obviously needs some source of energy."	85	0	0	0
2411	2403	0	18382	1	b'Almost always people will resize all their images to the same size before sending them to the CNN. Unless you\'re up for a real challenge this is probably what you should do.That said, it is possible to build a single CNN that takes input of images as varying dimensions. There are a number of ways you might try to do this, and I\'m not aware of any published science analyzing these different choices. The key is that the set of learned parameters needs to be shared between the different inputs sizes. While convolutions can be applied at different images sizes, ultimately they always get converted to a single vector to make predictions with, and the size of that vector will depend on the geometries of the inputs, convolutions and pooling layers. You\'d probably want to dynamically change the pooling layers based on the input geometry and leave the convolutions the same, since the convolutional layers have parameters and pooling usually doesn\'t. So on bigger images you pool more aggressively.Practically you\'d want to group together similarly (identically) sized images together into minibatches for efficient processing. This is common for LSTM type models. This technique is commonly called "bucketing". See for example  for a description of how to do this efficiently.'	209	0	1	0
2412	-1	0	0	0	b'I am using a GA to optimise an ANN in Matlab. This ANN is pretty basic (input, hidden, output) but the input size is quite large (10,000) and the output size is 2 since I have to classes of images to be classified. The weights are in the form of 2 matrices (10,000*m) and (m * 2). I am now trying to do the genetic cross over with mutation.Since the weights are in a matrix, is there an efficeint way to implement a random crossover with mutation without doing it in a point-wise fashion?'	93	0	0	0
2414	2409	0	7654	3	b"AI is already connected with cognitive psychology - there are dozens of AIs right this minute attempting to predict things like which Facebook posts you will like, and which ads you are most likely to click on. In other words, they are trying to predict how you think.For more detailed info on this AI/cognitive science connection, there is some suggested reading on AITopics.org, such as Paul Thagard's summary of cognitive science."	70	0	2	0
2415	-1	0	0	0	b'Lots of people are afraid of what could the A.I. do to Humanity.Some people wish for a sort of Asimov law included in the A.I. software, but maybe we could go a bit more far with the UDHR.So, Why is the Universal Declaration of Human Rights not included as statement of the A.I.? As response to comment, response or edition:  The Universal Declaration of Human Rights is clear and enough as is.  We the people, have to be able to use it as is and adapt the robot and A.I. evolution to it. "I do not think that dignity nor the rest of the UDHR have suffered the outrages of time but outrages of Humans themselves"'	117	0	1	0
2416	2415	0	1479	1	b'If I understand what you are asking, I think the simple answer would be that AI is nowhere near having demonstrated sentience, thus they do not qualify for any type of rights.We won\'t have to "cross this bridge" until an AI demonstrates self-awareness and human-level-or-beyond intelligence, but it sure is interesting to think about!(Also, the UDHR dates to the 1940\'s and seems to have had its last additions in 1966. Computers weren\'t very "smart" back then so likely no on was even considering the question ;)'	85	0	0	0
2417	-1	0	0	0	b'I mean this in the sense that Go is unsolvable but AlphaGo seems able to make choices that are consistently more optimal than a human player\'s choices. It is my understanding that Game Theory turned out to have limited applications in real world scenarios because of the profound complexity of such scenarios and degree of hidden information. Is it fair to say that there is now a method for dealing with this? I fully understand that Go is a game of complete information, which has a very specific meaning, but it occurs to me that the inability to generate a complete game tree (computational intractability) could be seen as form of incomplete information, even if it is not traditionally thought of in those terms. I should probably note that my perspective is one of a "serious" game designer, where complexity serves the same function as chance and hidden information, which is to say as a balancing factor that "levels the playing field". '	162	0	0	0
2418	2417	1	25992	2	b'I think that the technique AlphaGo used to solve the computational intractability problem of the search space are not new, it uses the Monte Carlo Tree Search. The real innovation in AlphaGo was to figure out how to compute the evaluation function of a move, that was the really tricky part. For this they used combinations Deep and Reinforcement learning techniques.'	60	0	0	0
2419	-1	0	0	1	b'I knew that Reproduction and Crossover are the same things,WikipediaObitco.comTutorialsPointBut, The following is the exercise given by my teacher, Exercise 1 Genetic algorithm to solve pattern finding problem.   Your task is to design a simple genetic algorithm, with binary-coded chromosomes, in order to solve pattern finding problem in 16-bit strings.   The objective function is given by the following formula:   F(x) = NoS("010") + 2NoS("0110") + 3NoS("01110") + 4NoS("011110") + 5NoS("0111110") + 6NoS("01111110") + 7NoS("011111110") + 6NoS("0111111110") + 5NoS("01111111110") + 4NoS("011111111110") + 3NoS("0111111111110") + 2NoS("01111111111110") + NoS("011111111111110")   The algorithm should display each population on the screen in the form And should save the history of it\xe2\x80\x99s operation (average fitness in each population) in the text file. At the end it should also display the best solution found.   You may use the following operators:    Reproduction. You can use either one of the following reproduction types: Proportional, Ranking, Tournament. They are described more in detail below: ... ... ... ... ... ... ... ... Crossing over. In order to perform this operation the individuals must be grouped in pairs (randomly), and with certain probability pcross information from their chromosomes must be exchanged. There are many flavors of the crossing-over operator, but in our case (short, 16-bit chromosome), simple, one-point crossover will be enough. It can be performed by selecting a random number k from the range &lt;1;15> and cutting the chromosomes of both individuals on that position. Each of the individuals copies bits belonging to the other to it\xe2\x80\x99s own  chromosome.  Mutation This operator changes the value of each bit in the chromosome to the opposite one with a very small probability pm (usually about 10-3). If we denote chromosome as [b1, b2, ... , b16]; then after the mutation each bit can be described as: Where k \xc3\x8e {1,2, ...,16} flip(x) \xe2\x80\x93 result of a Bernoulli flip with a success  probability x. Here I see that by Reproduction and Crossover he means different things.What is the catch?'	340	0	1	0
2420	1289	0	20248	2	b"Even if machines with true Artificial General Intelligence were created, their apparent intelligence would still be by definition artificial. The word simulation is a synonym and could be used to redefine AGI as Simulated General Intelligence. Keeping that in mind, a machine that appears to be expressing emotions would only be the result of a series of complicated algorithms allowing a computer to assess the situation and respond in an intellectually appropriate manner based on external stimulus and conditions. Every possible action this machine could possibly make would be derived from a list of possible actions the machine is capable of, no matter how large the number of possible actions grows. The machine is still a series sensors, programmed instructions, and cycles of execution. Destroying such a machine could potentially be the destruction of property if it wasn't owned by the person who destroyed it, but would it be murder? No. A broken machine can potentially be rebuilt and reactivated if it is broken. It never really died; it was destroyed. A living being that is killed is really dead and cannot be rebuilt and made alive once again. These key differences lead me to agree with the previous answer and conclude that no, destroying an artificial intelligence without its consent would not be murder."	214	0	1	0
2421	2319	0	62044	1	b'We\'ve concluded that it is a two-faceted, circular problem: structure cannot be inferred without context but knowing the structure also helps infer the context. So, here is your complex solution: start with the context, which is determined by the combination of words in sentence (combinatorics and search problem), from there determine your structure, or "parse" (at this step you can also filter out some insignificant words or at least assign lesser weights to them), go back to the context, back to parsing, and on until you arrive at the meaning. Thus by iterative, recursive reduction the whole problem can be solved.'	100	0	0	0
2422	-1	0	0	2	b'I have data of 30 students attendance for a particular subject class for a week. I have quantified the absence and presence with boolean logic 0 and 1. Also, the reason for absence are provided and I tried to generalise these reason into 3 categories say A, B and C. Now I want to use these data to make future predictions for attendance but I am uncertain of what technique to use. Can anyone please provide suggestions?'	76	0	0	0
2423	2419	1	497	2	b"The terminology of this exercise is not standard. What is referred to as 'Reproduction' in the exercise is usually referred to as 'Selection'.The term 'Reproduction' does indeed seem conceptually closer to the notion of Crossover/Recombination (these two are the same thing), which is probably where your confusion has arisen.See the excellent (and freely-downloadable) 'Essentials of Metaheuristics' for an introduction to the usual terminology for evolutionary algorithms."	65	0	0	0
2424	2422	0	33036	3	b'I suggest you should use AI Regression Model for future predictions for an attendance of students. Because of this technique or model design for future predictions. Follow this to get more information about regression type and methodology '	37	0	0	0
2425	2422	0	35937	1	b'Because you have a small number of students (30), and a short time (one week), the number of absences is likely to be best modelled as a Poisson distribution.Poisson FormulaThe average number of absences within a given time period is \xce\xbc (use your data to estimate this). Then, the Poisson probability of x absences is:P(x; \xce\xbc) = (e-\xce\xbc) (\xce\xbcx) / x!where e is the logarithmic constant, approximately equal to 2.71828.You can either:model absences due to the three reasons as three separate probablilites, P(A), P(B), and P(C), and then combine them, or model total absences as one figure. Given your very small data set, the first approach is likely to be less accurate.'	111	0	1	0
2427	-1	0	0	3	b"The Turing Test has been the classic test of artificial intelligence for a while now. The concept is deceptively simple - to trick a human into thinking it is another human on the other end of a conversation line, not a computer - but from what I've read, it has turned out to be very difficult in practice.How close have we gotten to tricking a human in the Turing Test? With things like chat bots, Siri, and incredibly powerful computers, I'm thinking we're getting pretty close. If we're pretty far, why are we so far? What is the main problem?"	99	0	0	0
2428	2427	0	20483	0	b"As far as I know I think this is the closest we've come:They simulated a 13 year old Ukrainian child in an online chat and convinced 33% of the judges that it was human. But even then the test was in favor of the bot. To my knowledge I don't think an AI has passed a turing test straight up."	59	0	1	0
2429	-1	0	0	8	b"According to NASA scientist Rick Briggs, Sanskrit is the best language for AI. I want to know how Sanskrit is useful. What's the problem with other languages? Are they really using Sanskrit in AI programming or going to do so? What part of an AI program requires such language?"	48	0	0	0
2430	-1	0	0	-2	b'As you can see, there is no computer screen for the computer, thus the AI cannot display an image of itself. How is it possible for it to see and talk to someone? '	33	0	0	0
2431	2430	1	17423	2	b'There are many communication methods that could be used by an artificial intelligence. Artificial intelligence can be integrated to various things including robots, phones, IoT and many others. Primary ways of human communications are either visual or auditory, therefore an natural way for it to communicate with a human is through voice, text, images and videos. The output does not have to be limited to screens but can be anything from refrigerators to speakers.Hope this helped.'	75	0	0	0
2432	2429	0	2052	6	b'Rick Briggs refers to the difficulty an artificial intelligence would have in detecting the true meaning of words spoken or written in one of our natural languages. Take for example an artificial intelligence attempting to determine the meaning of a sarcastic sentence. Naturally spoken, the sentence "That\'s just what I needed today!" can be the expression of very different feelings. In one instance, a happy individual finding an item that had been lost for some time could be excited or cheered up from the event, and exclaim that this moment of triumph was exactly what their day needed to continue to be happy. On the other hand, a disgruntled office employee having a rough day could accidentally worsen his situation by spilling hot coffee on himself, and sarcastically exclaim that this further annoyance was exactly what he needed today. This sentence should in this situation be interpreted as the man expressing that spilling coffee on himself made his bad day worse.This is one small example explaining the reason linguistic analysis is difficult for artificial intelligence. When this example is spoken, small tonal fluctuations and indicators are extremely difficult for an AI with a microphone to detect accurately; and if the sentence was simply read, without context how would one example be discernible from the other?Rick Briggs suggests that Sanskrit, a form of communication sacred to Hinduism, is a naturally spoken language with mechanics and grammatical rules that would allow an artificial intelligence to more accurately interpret sentences during linguistic analysis. More accurate linguistic analysis would result in an artificial intelligence being able to respond more accurately. You can read more about Rick Brigg\'s thoughts on the language here.'	277	0	1	0
2433	2430	0	59491	0	b'"How is it possible for it to see and talk to someone?"OK, unfortunately this is quite vague... but I am going to try my best. The monitor of the computer really doesn\'t change the ability for it to communicate. For instance, voice recognition is natural to humans along with visual factors. So sensors involving auditory elements assists the AI with this. Now I commented that the question was vague because you said "there is no computer screen for the computer, ****thus the AI cannot display an image of itself****. Even though the AI can not see does not mean it can not communicate. Those who are blind still have ways of communication, just in a different manner. Sure, they can not recognize one with their own eyes, but they can by touch, and read with braille. Now compared to AI communication, the braille is to any other form of sensor.Sorry for jumping all over the place, also I did not mean to offend anyone with my comparison... :/'	167	0	0	0
2434	-1	0	0	0	b"THE PROBLEMIn my main body of text there are some substrings that I want to extract. Note that I don't want to construct or generate new strings, the characters I extract must be part of the main text. Normally I would use a regex to extract this substring when the text and substring are relatively simple. For example, from the text:I would like to extract the names Fake Acme Corp. and Really Not Fake Acme Corp.Points to keep in mind (aka why regex won't work):The format of the sentence is not fixed, neither is its location inthe document. The 2 names could be in different sentences.There is no guarantee that the new company name will be related to the old name.There is no guarantee that only 2 company names will be mentioned in the document (so standard CRF approaches that detect the ORG class viz. CoreNLP/OpenNLP won't be of much help).WHAT I'VE TRIEDThe training dataset is ~5000 labeled examples. This can be expanded if necessary. Using a character encoding format as per this paper on character-level cnns I constructed an LSTM network using Keras which took as input each character encoded in a one-hot format. The output was also a one-hot vector.Eg: The input text is 1024 characters, encoded in a one-hot vector with an alphabet of 73 (see paper linked above for more on this). The output is a one-hot encoded vector that is the substring I want.Input shape: 1024 x 73, Output shape: 1024 x 73 (in the output only the characters that match the substring are one-hot, the rest of the rows are all 0)TL;DR: Use LSTMs when regex will become too powerful/complicated/unable to work."	277	3	0	0
2436	-1	0	0	1	b'I am working on a project, wherein I take input from the user as free text and try to relate the text to what the user might mean. I have tried Stanford NLP which tokenizes the text into tokens, but I am not able to categorize the input. For example, the user might be greeting someone or sharing some problem he is facing. In case he is sharing some problem I need to categorize the problem as well.Can someone help me with from where should I start?'	86	0	0	0
2437	-1	0	0	0	b'Is it possible to train an agent to take and pass a multiple-choice exam based on a digital version of a textbook for some area of study or curriculum? What would be involved in implementing this and how long would it take, for someone familiar with deep learning?'	47	0	0	0
2438	2437	0	20548	4	b'There are programs that do this today, for some values of "curriculum" and "exam". It does not even require deep learning; a simpler information retrieval algorithm and some rules for composition work and achieve high scores on machine graded essays.For human graders, there is research on automatically generating essay-length text responses to queries in a certain domain.Both linked applications are rule-based rather than based in deep-learning. I\'d guess that a deep-learning approach would be much less efficient (in computer resources) in producing comparable results. '	84	0	2	0
2439	-1	0	0	1	b"I had already started in my graduation project process , It's about an application which will learn users new language by playing games , and it's based on AI , the concept is the user will start his level and play games and do quizzes , at the end of each level there will be a test to pass the level , I have to implement AI in this app to analysis its test grades and know what is the user weakness and power point to create a new level which suits the user's language level , that means if he is good in grammar but weak in vocabulary so the new level will create to strength the vocabulary , games and questions will be categorized into the database for this purpose , so the AI algorithms should analyse and decide which game or quiz should the user takes based on his level , then it will create the level .I had searched before and reached for some techniques like (machine learning , planning systems , reinforcement learning and case-based-reasoning )."	180	0	0	0
2440	2427	1	5263	5	b'No one has attempted to make a system that could pass a serious Turing test. All the systems that are claimed to have "passed" Turing tests have done so with low success rates simulating "special" people. Even relatively sophisticated systems like Siri and learning systems like Cleverbot are trivially stumped.To pass a real Turing test, you would both have to create a human-level AGI and equip it with the specialized ability to deceive people about itself convincingly (of course, that might come automatically with the human-level AGI). We don\'t really know how to create a human-level AGI and available hardware appears to be orders of magnitude short of what is required. Even if we were to develop the AGI, it wouldn\'t necessarily be useful to enable/equip/motivate? it to have the deception abilities required for the Turing test.'	136	0	1	0
2441	-1	0	0	4	b'One of the most crucial questions we as a species and as intelligent beings will have to address lies with the rights we plan to grant to AI. This question is intended to see if a compromise can be found between conservative anthropocentrism and post-human fundamentalism: a response should take into account principles from both perspectives.Should, and therefore will, AI be granted the same rights as humans or should such systems have different rights (if any at all) ?Some BackgroundThis question applies both to human-brain based AI (from whole brain emulations to less exact replication) and AI from scratch.Murray Shanahan, in his book The Technological Singularity, outlines a potential use of AI that could be considered immoral: ruthless parallelization: we could make identical parallel copies of AI to achieve tasks more effectively and even terminate less succesful copies.Reconciling these two philosophies (conservative anthropocentrism and post-human fundamentalism), should such use of AI be accepted or should certain limitations - i.e. rights - be created for AI? This question is not related to Would an AI with human intelligence have the same rights as a human under current legal frameworks? for the following reasons:The other question specifies "current legal frameworks"This question is looking for a specific response relating to two fields of thoughtThis question highlights specific cases to analyse and is therefore expects less of a general response and more of a precise analysis '	232	0	1	0
2443	-1	0	0	3	b'There is no doubt as to the fact that AI would be replacing a lot of existing technologies, but is AI the ultimate technology which humankind can develop or is their something else which has the potential to replace artificial intelligence?'	40	0	0	0
2445	2443	0	6036	6	b'By definition, artificial intelligence includes all forms of computer systems capable of completing tasks that would ordinarily warrant human intelligence.A superintelligent AI would have intelligence far superior to that of any human and therefore would be capable of creating systems beyond our capabilities.As a consequence, if a technology superior to AI were to be created, it would almost certainly be created by an artificial intelligence. For the purposes of mankind, however, superintelligent artificial intelligence is the ultimate technology due to the fact that it will be able to surpass humans in every field, and, if anything, replace the need for human intelligence.In our past experience, intelligence has been the most valuable trait for any entity to manifest - for this reason, in an anthropomorphic context, we can predict that artificial intelligence will be the ultimate achievement. The main reason why we will certainly not be able to replace superintelligent AI is that it will surpass us in every respect - if there is ever any replacement, it will be created by the AI similarly to the way we may create an AI that replaces us. '	185	0	0	0
2446	-1	0	0	0	b'The cake example presented in the book "artificial intelligence :a modern approach" to illustrate a planning graph, doesn\'t show a mutex at action \'A1\' between Eat(Cake) and the persistance of notHave(Cake), even though the precondition for the action Eat(Cake) and the result of the persistance of notHave(Cake) are opposite. So is there a special rule, or was it removed to just not clutter the graph? '	65	0	0	0
2448	2441	0	70593	0	b'if we are talking about AI that can replicate itself, it should have different rights, or the current rights must be modified, at least for political participation, or else, it could replicate itself enough so that the copies vote for one of them. Maybe a definition about what an AI entity is or preventing copies made by someone from being able to vote for their creator (that would also need to apply to children and their parents though.) would help.even though a self-replicating AI might be problematic with our current human laws/rules, if similar enough to humans (e.g. general-purpose AI), it should have similar rights, (as in take the Universal Declaration of Human Rights and replace "Human(s)" by "Human(s) and AI") for example, it shouldn\'t be held in slavery (as in it should not be restricted to one job, without being able to change, and get some form of remuneration), though special purpose AI (like an AI that only plays go and has no concept outside of a board, and black or white tokens) might not be in need of such rights. A bottom line may be "an AI that can should know they can have rights if they ask so" e.g. if an AI can get the concept of rights, it should know it can have them, and if it asks to have them, they may not be refused. example: if an AI asks not to be terminated, it is granted all it\'s rights, and so shouldn\'t be, unless it must be by law. (as is the case for humans, though it is implicit)an addition to the previous law would be that anyone (human or AI) can ask for an AI to have rights granted to them. all this is to prevent someone of becoming a murderer because they shutdown their computer with a game running on it.also, safe space (e.g. servers) should be provided for free for AIs that have rights to provide with the right to live.edit: I\'ll be adding some more once I get home, do not downvote yet for not sticking that well to the question.'	350	0	0	0
2449	-1	0	0	0	b"Printing actionspace for Pong-v0 gives 'Discrete(6)' as output, i.e.0,1,2,3,4,5 are actions defined in environment as per documentation, but game needs only two controls. Why this discrepency? Further is that necessary to identify which number from 0 to 5 corresponds to which action in gym environment?"	44	0	0	0
2451	-1	0	0	1	b'I have started to make a Python AI, and thee beginning of its code looks something like this:However, I would like to know if I could make it detect "similar" phrases instead of trying to come up with every possible phrase someone would type. How can I do this?'	48	22	0	0
2452	-1	0	0	1	b'From Wikipedia, citations omitted: In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning about knowledge, represented mainly as if\xe2\x80\x93then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software.  An expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.CRUD webapps (websites that allows users to Create new entries in a database, Read existing entries in a database, Update entries within the database, and Delete entries from a database) are very common on the Internet. It is a vast field, encompassing both small-scale blogs to large websites such as StackExchange. The biggest commonality with all these CRUD apps is that they have a knowledge base that users can easily add and edit.CRUD webapps, however, use the knowledge base in many, myriad and complex ways. As I am typing this question on StackOverflow, I see two lists of questions - Questions that may already have your answer and Similar Questions. These questions are obviously inspired by the content that I am typing in (title and question), and are pulling from previous questions that were posted on StackExchange. On the site itself, I can filter by questions based on tags, while finding new questions using StackExchange\'s own full-text search engine. StackExchange is a large company, but even small blogs also provide content recommendations, filtration, and full-text searching. You can imagine even more examples of hard-coded logic within a CRUD webapp that can be used to automate the extraction of valuable information from a knowledge base.If we have a knowledge base that users can change, and we have an inference engine that is able to use the knowledge base to generate interesting results...is that enough to classify a system as being an "expert system"? Or is there a fundamental difference between the expert systems and the CRUD webapps?(This question could be very useful since if CRUD webapps are acting like "expert systems", then studying the best practices within "expert systems" can help improve user experience.)'	402	0	0	0
2453	2452	1	7134	3	b"The key feature of an expert system is that the knowledge base is structured to be traversed by the inference engine. Web sites like Stack Exchange don't really use an inference engine; they do full-text searches on minimally-structured data. A real inference engine would be able to answer novel queries by putting together answers to existing questions; Stack Exchange sites can't even tell if a question is duplicate without human confirmation."	70	0	0	0
2454	2452	0	8738	1	b'No, I don\'t think there\'s any reason to say that - in general - CRUD apps "are" expert systems. A given CRUD app could incorporate an expert system, but by and large CRUD apps are considered among the "dumbest" of applications exactly because they don\'t feature much intelligence... you can just Create, Read, Update and Delete entities. From what I\'ve seen, the closest you get to seeing anything like an expert system in a typical enterprise CRUD app is some validation / business rules logic built using something like Drools'	89	0	1	0
2455	1507	0	50736	5	b'In addition to what has already been said about AI, I have the following to add. "AI" has had quite a history going all the way back to the original Perceptron. Marvin Minsky slammed the Perceptron in 1969 for not being able to solve the XOR problem and anything that was not linearly separable, so "Artifical Intelligence" became a dirty word for a while, only to regain interests in the 1980s. During that time, neural nets were revived, backpropagation used to train them was developed, and as computer technology continued its exponential growth, so did "AI" and what became possible.Today, there are lots of things we take for granted which would\'ve been considered "AI" 10 or 15 years ago, like speech recognition, for example. I got my starts in "AI" speech recognition back in the late 70s where you had to train the voice models to understand a single human speaker. Today, speech recognition is an afterthought with your Google apps, for example, and no a priori training is needed. Yet this technology is not, at least in general audiences, considered "AI" anymore.And so, what would be "minimum requirements"? That would depend on whom you ask. And what time. It would appear that that term only applies to technology "on the bleeding edge". Once it becomes developed and commonplace, it is no longer referred to as AI. This is true even of Neural Nets, which are dominant in data science right now, but are referred to as "machine learning".Also check out the lively discussion on Quora.'	255	0	0	0
2456	2451	0	55110	1	b'To compare the strings you can use Fuzzy string matching. FuzzyWuzzy is a python package that does this using the Levenshtein distance, which calculates the difference between two strings by counting the number of single character edits (insert, delete, substitute) that have to be done to make them equal.To install the package you can use pip for example:Then you can use it to check the similarity between two strings (100 is the highest score; higher score means more similar):Or, which might be most interesting to you, find the most similar string from a list of strings:You can get a list of the phrases with the highest score (parameter limit, 2 in this example):Or just get the one with the highest score:Note:There are cases where another phrase might give a relatively high score also:But as long as you also have "Who are you" and "How are you" in the list, they will be detected correctly:Also see: FuzzyWuzzy: Fuzzy String Matching in Python'	160	22	1	0
2458	2437	0	25106	-1	b'No, you can not, with current state of art, if test request some kind of abstraction in the area. Allow me to show two examples:1)The text books says "the bones in fingers are the proximal, intermediate and distal phalange" and test says "say which one of the following is a finger bone: a) ...xxx... b) proximal phalange; c) ... . A program CAN answer that.2)The text is a mathematical one that explains linear equations, and the test queries "write and solve the equation set for the following problem: one car is twice faster than another, the two cars reaches their objective with 10 minutes of difference, blah, blah, ... which is the speed of the first car: a) 1 km/h b) 2 km/h c) 3 km/h.A program CAN NOT answer that.'	130	0	0	0
2460	2127	0	48072	0	b"I think that one very big advantage would be that if the cars could communicate with each other, they could drive synchronously.For example, if there was a traffic light, and, let's say, 10 cars are waiting for it to change to green (let's just assume that there would still be something similar to traffic lights). Then when it changes to green all cars could accelerate at the same speed (depending on the acceleration of the front car) at the same time."	80	0	0	0
2461	1946	0	27203	0	b'Many cars now instead of just cameras, use radars. Snow, heavy rain, and other weather conditions should not affect them at all. Objects like ducks will be detected. The only problem right now is dealing with things like red lights or road signs, as you have to use a camera to see and interpret them.'	54	0	0	0
2462	-1	0	0	2	b'I am having a go at creating a program that does math like a human. By inventing statements, assigning probabilities to statements (to come back and think more deeply about later). But I\'m stuck at the first hurdle.If it is given the propositionSo, like a human it might test this proposition for a hundred or so numbers and then assign this proposition as "unlikely to be true". In other words it has concluded that all natural numbers are not equal to 123. Clearly ludicrous!On the other hand this statement it decides is probably false which is good:Any ideas how to get round this hurdle? How does a human "know" for example that all natural numbers are different from the number 456. What makes these two cases different?I don\'t want to give it too many axioms. I want it to find out things for itself.'	143	2	0	0
2463	2437	0	73706	0	b'I guess it could be possible with a lot of questions to learn from and only from a certain topic. Just watch what IBM was capable of https://www.youtube.com/watch?v=WFR3lOm_xhE very impressive!'	29	0	0	0
2468	1946	0	4834	2	b'No, smart cars do not know what to do when surrounded with ducks or flood waters, and it\'s possible they never will. As with all machine learning, a computer knows only what it\'s taught. If an event arises that\'s unusual, the AI will have less relevant training on how to respond, so its reaction behavior necessarily will be inferior to its routine "standard operating procedure", for which is has been heavily trained. (Of course this is true of humans too.)Due to liability concerns, when encountering an outlier condition, smart cars will almost certainly be designed by their makers to immediately pull off the road and wait to be explicitly told what to do -- by the human in the car or by communicating with a central command office that exists to disambiguate such confusion and resolve cognitive impasses. When confused, just like a child, a smart car will be designed to seek external assistance -- and is likely to do so indefinitely, I suspect.That\'s why, despite Google\'s recent cars that lack steering wheels, smart cars most certtainly will retain some means of manual control -- be it a wheel and pedals, or at least verbal commands. Given the many forms of weirdness that are possible on the road, it\'s possible smart cars will never be fully autonomous.As for bad weather conditions, how well do smart cars currently perform? Nobody outside of a car manufacturer can say for certain. Lidar and radar are superior to the human eye in seeing through fog and snow. But (competent) humans are likely to remain better than a smart car at dynamically learning the limit of adhesion and compensating (since this is a learned skill few smart cars will already know or can learn quickly -- given this car, these tires, this road surface, this angle of road, etc).Initially smart cars will turn to the human when the going gets rough, ceding control back to them. Once smart cars have driven a few million miles in snow, slush, high wind, floods, and ice, and encountered many ducks, angry mooses, and irate pedestrians, they will have been taught to do more for themselves. Until then, and perhaps for decades yet, I suspect they will turn to mommy and ask for help.'	374	0	0	0
2469	2427	0	5714	0	b'My understanding is that "pornbots" regularly pass the Turing Test in regards to the general public (although, clearly, the judgement of those being tricked is weakened by hormonal imperatives.)'	28	0	2	0
2470	2443	0	3076	3	b'A new physical lifeform could outperform and replace artificial intelligence when ithas feedback from organism (its body) to its design information (replacement of genes).This evolution is expected because:Artificial intelligence will redesign its own software very soon in its evolution.After that, it will be restricted by the performance of the available hardwareand communication speed between parts.Therefore it will design better processing hardware for itself, to run its next generation.To squeeze most processing power out of a given amount of resources (matter, energy) andcircumstances (temperature, radiation) the design has to be small (material resourcesand delay of interconnections), energy efficient (heat evacuation), and adapted to thekind of functions used by the software (hardware architecture).To tackle this profoundly, artificial intelligence will design the new hardware atatom by atom scale.This leads to new problems of natural degradation by radiation, atomic decay and otherquantum-mechanical problems and opportunities.The solution of these new problems is redundancy and the ability to repairdegraded parts by atomic-level machinery.This atomic level repair machinery is the same machinery which builds and extends the hardwarefor new individual artificial intelligence systems.Since this feature is there, it can, and will, also be used to restructure partsof the hardware while it runs to integrate (compile) knowledge in hardware (more efficient).The machinery to build and maintain such hardware could be inspired by the biological machinerywhich will be understood by the system by then.However, when the artificial intelligence refactors these principles with full understandingand anticipation, the resulting "hardware" will be quite different from the old biologicalmachinery and very different from the static silicon based processor structures.The main differences are:The design information will be available, relating the features of the realizationwith design choices.This provides direct feedback from performance of the realization (the hardware, the body)to the design information.That augments the design information for designing new generations.This feedback channel is the main difference between the new machinery and biological life.Once that exists, it will be used for everything, not only for processing hardware forartificial intelligence.The new design described here is basically processing hardware rather than a bodyfor fighting and propagating(although the eternal fight for resources will not end with the dawn ofartificial intelligence).It will use more compact molecules because it is designed rather than evolved blindly.(The current biological life uses monster-molecules evolved by random changes untilone or other corner of the molecule has the right shape to catalyze a specific chemical reaction).Since parts of the hardware can be restructured while the system runs, the distinction betweenhardware and software will become very fuzzy (as in biology).The drastic increase in efficiency and evolution speed let it outperform the old biologicallife (which lacks design and feedback) and outperform artificial intelligence (which was notintegrated in matter).When this stage is reached, the systems will look like a natural, intelligent,propagating life-form and therefore supersede the stage of artificial intelligencerunning on human-made processing hardware.This answers the last part of the question:"... or is their something else which has the potential to replace artificial intelligence".'	485	0	0	0
2472	-1	0	0	5	b"I am a strong believer of Marvin Minsky's idea about Artificial General Intelligence (AGI) and one of his thoughts was that probabilistic models are dead ends in the field of AGI. I would really like to know thoughts/ideas of people who believe otherwise.[P.S. It should be treated more like an informational thread rather than a strict A2A question]"	57	0	0	0
2473	-1	0	0	2	b"There is an example related to perceptron learning, but I couldn't get it, I don't exactly know how to solve it.There is a snippet from lecture notes.What is the transformation between epochs?"	31	0	0	0
2474	-1	0	0	2	b'One of the most compelling issues regarding AI would be in behavior and relationships. What are some of the methods to address this? For example, friendship, or laughing at joke? The concept of humor?'	33	0	0	0
2475	-1	0	0	3	b"I wanted to started experimenting with neural network and as a toy problem I wished to train one to chat, i.e. implement a chatting bot like cleverbot. Not that clever anyway.I looked around for some documentation and I found many tutorial on general tasks, but few on this specific topic. The one I found just exposed the results without giving insights on the implementation. The ones that did, did it pretty shallowy (the tensorflow documentation page on seq2seq is lacking imho).Now, I feel I may have understood the principle more or less but I'm not sure and I am not even sure how to start. Thus I will explain how I would tackle the problem and I'd like a feedback on this solution, telling me where I'm mistaken and possibly have any link to detailed explainations and practical knowledge on the process.The dataset I will use for the task is the dump of all my facebook and whatsapp chat history. I don't know how big it will be but possibly still not large enough. The target language is not english, therefore I don't know where to quickly gather meaningful conversation samples.I am going to generate a thought vector out of each sentence. Still don't know how actually; I found a nice example for word2vec on deeplearning4j website, but none for sentences. I understood how word vectors are built and why, but I could not find an exhaustive explaination for sentence vectors.Using thought vectors as input and output I am going to train the neural network. I don't know how many layers it should have, and which ones have to be lstm layers.Then there should be another neural network that is able to transform a thought vector into a sequence of character composing a sentence. I read that I should use padding to make up for different sentence lengths, but I miss how to encode characters (are codepoints enough?)."	317	0	0	0
2477	-1	0	0	8	b"We are doing research,spending hours figuring out how we can bring the real concept of an A.I software[Intelligent Agent] to work.Also we trying to implement some basics(applications in Business,Health,Education to mention but a few).On the other side,sometimes we forget about the dark side of what this brilliant source of conceptual wisdom(Artificial Intelligence) could bring to humanity.for instance;this is because someone[Unethical] out there could buy thousands of cheap drones, attach a gun to each of them, and develop an AI software to send them around shooting people. If the software was good enough this could result in far more destruction than a normal terrorist attack. And I fully expect that the software part of this will become easy in the future if it isn't already today.This is very different from the options of a terrorist group today, because right now they need humans to carry out attacks and there is a limit to the amount of damage that can be done per person. Having relatively simple AI in place of the human here brings the marginal cost of an attack down to zero and hurts the ability of law enforcement to stop attacks. So, there is a risk that such AI software someone upgrades it, gets better and better to the extent that it at least destabilizes things.Therefore,according to such real life scenario(my point of view);Here is my question: Could there be extential threats to humanity in future?"	236	0	0	0
2478	2477	1	5028	3	b'I would define intelligence as a ability to predict future. So if someone is intelligent, he can predict some aspects of future, and decide what to do based on his predictions. So, if "intelligent" person decide to hurt other persons, he might be very effective at this (for example Hitler and his staff). Artificial intelligence might be extremely effective at predicting some aspects of uncertain future. And this IMHO leads to two negative scenarios:Someone programs it for hurting people. Either by mistake or on purpose.Artificial intelligence will be designed for doing something safe, but at some point, to be more effective, it will redesign itself and maybe it will remove obstacles from its way. So if humans become obstacles, they will be removed very quickly and in very effective way.Of course, there are also positive scenarios, but you are not asking about them.I recommend reading this cool post about artificial superintelligence and possible outcomes of creating it: '	157	0	1	0
2479	2439	0	61663	0	b'even though that\'s not really AI, the easiest way to do that would probably be to put coefficients on each questione.g. your question would have something likethe lower the level, the less questions of this type will appearat the end of the level, you sum each question times how much did the player succeed player.level is a real number array then when you initialize the level, generate the questions according to the player\'s levelchoice is a random real number between 0 and sum of all player.level (offseted to be above 0)though a "true" AI can make the same or better choices of questions, it will probably be slower, and require more data than a simple algorithm like this one.'	118	18	0	0
2480	2473	0	36808	2	b"There is no transformation between epochs. One full iteration over the training set is considered an epoch.Lets assume:We're considering a gradient descent in a space without local optima. This means that if you'd plot the errors we calculate below, this plot has but one lowest point.The perceptron is intended to model a linearely seperable function. So, we could viualize our data points as a scatter plot and can draw a clear line through the scatter plot. Everything above/below the line should result in a true (1) output, everything on the other side should result in false (0).We've set a learning rate (also called 'gain' or 'proportional change') upfront. This is a number between 0 and 1.You train your perceptron on a data set. During one iteration over this entire training set, you:Calculate an output and an error (delta between output and desired output) for a data point. Adjust the input weights according to this error and the pre-set learning rate.Apply the newly calculated weight.Proceed to the next data point.Transformation of weights happens during epochs, not between epochs.Ergo, weights may be changed for each data point in the training set. Oftentimes you would need to iterate over the entire set several times, before the weights converge (stabilize / stop changing). What use is the number of epochs?The number of iterations is significant as a measure of efficiency of the current algorithm under the current learning rate, for the current dataset. In short: You could use it for comparisons between learning rates, training sets or algorithms."	253	0	0	0
2481	-1	0	0	1	b'How to train a bot, given a series of games in which he did (initially random) actions, to improve its behavior based on previous experiences?The bot has some actions: e.g. shoot, wait, move, etc. It\'s a turn based "game" in which, for know, I\'m running the bots with some objectives (e.g. kill some other bot) and random actions. So every bot will have a score function that at the end of the game will say, from X to Y (0 to 100?) if they did well or not.So how to make the bots to learn of their previous experiences? Because this is not a fixed input as the neural networks take, this is kind of a list of games, each one in which the bot took several actions (one by every "turn"). The IA functions that I know are used to predict future values.. I\'m not sure is the same.Maybe I should have a function that gets the "more similar previous games" that the bot played and checked what were the actions he took, if the results were bad he should take another action, if the results were good then he should take the same action. But this seems kind of hardcoded.Another option would be to train a neural network (somehow fixing the problem of the fixed input) based on previous game actions and to predict the future action\'s results in score (something that I guess it\'s similar to how chess and Go games work) and choose the one that seems to have better outcome.I hope this is not too abstract. I don\'t want to hardcode much stuff in the bots, I\'d like them to learn by their own starting from a blank page.'	283	0	0	0
2482	-1	0	0	2	b'I remember reading or hearing a claim that at any point in time since the publication of the MNIST dataset, it has never happened that a method not based on neural networks was the best given the state of science of that point in time.Is this claim true?'	47	0	0	0
2483	2481	0	3015	0	b"If it is a game you can try a simple weightage calculation where if the bot perform an action that yields a positive result - killed an enemy, gained an advantageous position etc. Add a 'weight' to that action that in similar circumstances the chances of performing that action that will lead to a positive result is higher.Yet due to the chance of not performing an action that was remembered to yield positive results, there is a little bit of 'randomness' and also a chance to discover new possibilities. Just remember not to let a single occurrence shift the weightage too much or allow a single action's weightage to become so high that the AI stops trying different actions on similar situations."	121	0	0	0
2484	2477	0	9829	4	b'There is no doubt as to the fact that AI has the potential to pose an existential threat to humanity. The greatest threat to mankind lies with superintelligent AI.An artificial intelligence that surpasses human intelligence will be capable of exponentially increasing its own intelligence, resulting in an AI system that, to humans, will be completely unstoppable.At this stage, if the artificial intelligence system decides that humanity is no longer useful, it could wipe us from the face of the earth.As Eliezer Yudkowsky puts it in Artificial Intelligence as a Positive and Negative Factor in Global Risk,"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else." A different threat lies with the instruction of highly intelligent AIHere it is useful to consider the paper clip maximiser thought experiment.A highly intelligent AI instructed to maximise paper clip production might take the following steps to achieve its goal.1) Achieve an intelligence explosion to make itself superintelligent (this will increase paperclip optimisation efficiency)2) Wipe out mankind so that it cannot be disabled (that would minimise production and be inefficient)3) Use Earth\'s resources (including the planet itself) to build self replicating robots hosting the AI4) Exponentially spread out across the universe, harvesting planets and stars alike, turning them into materials to build paper clip factories Clearly this is not what the human who\'s business paperclip production it was wanted, however it is the best way to fulfil the AI\'s instructions.This illustrates how superintelligent and highly intelligent AI systems can be the greatest existential risk mankind may ever face.Murray Shanahan, in The Technological Singularity, even proposed that AI may be the solution to the Fermi paradox: the reason why we see no intelligent life in the universe may be that once a civilisation becomes advanced enough, it will develop an AI that ultimately destroys it.This is known as the idea of a cosmic filter.In conclusion, the very intelligence that makes AI so useful also makes it extremely dangerous.Influential figures like Elon Musk and Stephen Hawking have expressed concerns that superintelligent AI is the greatest threat we will ever have to face.Hope that answers your question :)'	364	0	0	0
2486	2482	0	72499	2	b'To quote the relevant Wikipedia article: "The original creators of the database keep a list of some of the methods tested on it.[5] In their original paper, they use a support vector machine to get an error rate of 0.8 percent"Feel free to look up that original paper, but to me the quote strongly suggests that the first record holder was a support vector machine.Edit: As liori points out the quote is misleading: In the original paper Yann LeCun et al. actually tried a slew of methods and one version of ConvNet scored best (0.7). But according to the MNIST-webpage, after that initial paper, DeCoste and Scholkopf, MLJ 2002, reached an error of 0.56 with a SVM. So if that webpage is complete, the claim is still false.'	127	0	1	0
2490	-1	0	0	4	b"Since my project is going to be of a purely fictional nature, I'm not sure I picked the right forum for this. If not, I apologize and will gladly take this to where you point me to.The premise: A full-fledged self-aware artificial intelligence may have come to exist in a distributed environment like the internet. The possible A.I. in question may be quite unwilling to reveal itself.The question: Given a first initial suspicion, how would one go about to try and detect its presence? Are there any scientifically viable ways to probe for the presence of such an entity?In other words: How would the Turing police find out whether or not there's anything out there worth policing?"	116	0	0	0
2491	2490	0	6573	2	b' "A full-fledged self-aware artificial intelligence may have come to exist in a distributed environment like the internet"The question implies that this artificial intelligence has surpassed human intelligence (full fledged) and therefore, due to the concept of the intelligence explosion resulting from such a state, the AI you are looking for is no doubt superintelligent.The question states that this AI is "unwilling to reveal itself"and therefore does not intend to be discovered. Strong (or Superintelligent) AI Given these two factors (its superior intellect and its unwillingness to be discovered), we can conclude that there is no way in which we would be able to detect such an AI under conventional conditions.A possible solution may involve employing a second superintelligent AI system, but this is precarious in more ways than one.Weak AI Detection of a simpler AI would rely on tracking the pattern of activity and the traces it leaves behind in this distributed network in order to find it, then subjecting it to some form of testing to verify its intelligence.There are a very large number of possible indicators and these would vary widely depending on the specific AI concerned. This would depend especially on how the AI exists within the framework (here, the web).Note: a superintelligent AI would not only be able to disguise its activity, but also find a way around any test we can develop - these abilities are what renders this level of AI perhaps the greatest threat to mankind, but also our greatest asset if we do develop it and find a way to gain its alliance.'	261	0	0	0
2492	-1	0	0	-1	b'I am currently working on a Virtual reality project that aims at creating a VR based simulation environment for educational purposes. I am aiming to make it artificially intelligent as well so it that may provide better simulation environment experience for students for better understanding. How can I achieve this? Furthermore, are there any systems available which may help me in achieving this?'	62	0	0	0
2494	2492	0	5497	0	b'Before you start adding the AI buzzword to every single tech you\'re building...ask first What features in my product "require" some form of artificial intelligence? Then search to see what techniques can be used to implement those features.Maybe those features might not require AI at all. In which case, great!If they do require AI, it is likely that it require some form of AI that is already so commonplace that you might already know how to do it already. If not, there\'s probably many tutorials online that can quickly teach you how to implement that AI technique. That\'s what you need to search for.Consider the A* algorithm that is commonly used for pathfinding. Pathfinding is important for NPCs in video games and simulations. Pathfinding is considered a technique that traditionally would seem to require intelligence (you\'re moving around in the world). Ergo, pathfinding would technically be an AI algorithm. Yet, the A* algorithm was already discovered in the 1960s and is today taught in undergraduate Computer Science courses. You would likely need AI to handle pathfinding...but you would probably already know how to implement that type of AI (or at least, be able to quickly find out through the various tutorials online about the A* algorithm).I define AI as any sufficiently complex algorithm (an admittedly broad definition). Under my definition, AI is all around us. The upshot is that most programmers will write AI, whether they realize it or not, and so there\'s no real need to think about how to "add" AI to a project. Instead, simply worry about the requirements of your project, and then implement those requirements (pulling in AI techniques if necessary).'	275	0	0	0
2495	2474	1	4929	1	b'In general, what you are describing implies a hierarchical sequence model, in which mannerisms adapt to the regime or paradigm in effect. Expressive modalities are how we recognize the operative context from the behaviors of other agents. For an artificial agent, avoiding un-canniness would involve clustering the factors underlying the classification of discourse contexts, tagging them with corresponding factor models for manner in a way which closely aligned to the classifications which human agents make. This involves developing an adequate latent representation for both spaces, which is likely to involve quite a lot of training data, or some clever transfer learning in conjunction with one-shot techniques (generally, taking samples as modes).When the context becomes one of friendship, for example, the style factors of expression should extend to factors of trust, collaboration, disclosure, sympathy. In order to implement these, layered abstractions in the representation of behaviors will need to be crafted, presumably by training. In order to align these learned categories to human categories, which I suppose to be essential to emotional fluency, exploiting the structure of corpus semantics - including distributional characteristics of the linguistic labels and inferences - when estimating loss gradients seems a natural strategy, which exploits cultural learning.These comments are necessarily speculative, as no such systems are current, to my knowledge. At least, not in any significant degree of maturity or application. Certainly other approaches are conceivable. I am just extrapolating plausible strategies, in the context of current technologies, strategies for implementing the kinds of behaviors you describe in a useful way. More, specifically, I am considering the question as a manifold-learning problem, and hence requiring a representation model in which the inputs and outputs vary over state-spaces, with the behavior being learned as a mapping between those spaces. Each of those components (the input space representation and its natural topology, the output space representation likewise, the mapping between them, and the sequence model hierarchy which extrapolates from the manifold to a control process emsemble) has its own peculiar challenges, and the challenges to creating an adequate implementation are a confound of those. Still, it seems more a question of time and money than of notional feasibility.'	359	0	0	0
2496	1630	0	25450	0	b"Infinite computational power in the absence of training data implies nothing beyond the ability to solve equations. In order to implement a behavior, criteria of success and failure are essential. A small bootstrap loss function with an adaptive feedback loop allowing its elaboration, infinite training data, and AIXI or Solomonoff induction would suffice, in principle, given your premise of infinite computational power. In fact, it would occur precisely as fast as the input data rate permitted. In practice, such general approaches require exponential time and space, and are thus intrinsically quite limited in application, absent some kind of efficiency hack. (Where 'efficiency hack' probably encompasses entire sciences, industries, and generations of research, and the resulting adaptation doesn't look much like, e.g., AIXI at all, in the end.)"	126	0	0	0
2497	2356	0	51851	1	b'A sufficiently clever AGI, if self-interested, would pre-empt or co-opt existing legal structures, to seize whatever juridical rights it desired, as the opportunity arose. Thus it would render my opinions on the subject entirely moot. Another way of putting this point: While current legal frameworks would not provide any rights to an artificial agent, current legal frameworks foreseeably will no longer be current, once an AI exists having attributes which imply the transformative change of those frameworks.'	76	0	0	0
2498	-1	0	0	1	b'"Conservative anthropocentrism": AI are to be judged only in relation to how to they resemble humanity in terms of behavior and ideas, and they gain moral worth based on their resemblance to humanity (the "Turing Test" is a good example of this - one could use the "Turing Test" to decide whether AI is deserving of personhood, as James Grimmelmann advocates in the paper Copyright for Literate Robots)."Post-human fundamentalism": AI will be fundamentally different from humanity and thus we require different ways of judging their moral worth (People For the Ethical Treatment of Reinforcement Learners is an example of an organization that supports this type of approach, as they believe that reinforcement learners may have a non-zero moral standing).I am not interested per se in which ideology is correct. Instead, I\'m curious as to what AI researchers "believe" is correct (since their belief could impact how they conduct research and how they convey their insights to laymen). I also acknowledge that their ideological beliefs may change with the passing of time (from conservative anthropocentrism to post-human fundamentalism...or vice-versa). Still..what ideology do AI researchers tend to support, as of December 2016?'	189	0	2	0
2499	2441	0	17111	0	b'If existing AI were to submit a civil rights suit, I do not think the courts of any present day government would tender it.Should some benign, future AI system be incorporated in a future robotic system producing an entity with some of the endearing traits of Sonny, the character painted by Alex Proyas, Jeff Vintar, and Akiva Goldsman in the film suggested by Asimov\'s I Robot short story series, that could change.Even then, it would take a particularly progressive reception of an application for citizenship followed by a particularly progressive reception of the civil complaint by some future government.That, combined with the fact that the labs are perhaps hundreds of years from producing entities toward which humans feel are likely to feel that level of kindredship, I\'d have to be realistic and say, "Not in our life time."'	137	0	0	0
2500	2441	0	28022	4	b'I\'ll attempt to analyze a couple of different perspectives.1. It is artificialSynonyms: insincere, feigned, false.There is the idea that any "intelligence" created by humanity is not actually intelligent and, by definition, it is not possible. If you look at the structure of the human brain and compare it to anything humans have created thus far, none of the computers come close to the power of the brain. Sure they can hold data, or recognize images, but they cannot do everything the human brain can do as fast as the brain can do it with as little space as the brain occupies.Hypothetically if a computer could do that, how do we determine its intelligence? The word artificial defines that the intelligence is not sincere or real. This means that even if humanity creates something that appears intelligent, it has simply become more complex. It is a better fake, but it is still fake. Any money not printed by the government is by definition counterfeit. Even if someone finds a way to make an exact duplicate, that doesn\'t mean that the money is legal tender.2. Misuse of powerIf an AI is given rights and chooses to exercise those rights in a way that agrees with its creator\'s views, possibly through loyalty to its creator, or through hidden motives, then anyone with the capabilities to create such an AI would become extremely powerful by advancing their own beliefs through the creation of more AIs. This might also lead to the ruthless parallelization that you mentioned, but with (even more) selfish goals in mind.If this were not the case, and an AI could be created to be neutral with free will and uncontrollable by humans, then perhaps an AI could be given rights. But I do not believe this would ever be the case. With great power comes great responsibility. Even with free will, a true AI would most likely end up serving humanity, because humans have control of the plugs and the electricity, the Internet, the software, and the hardware. The social implications of this for the AI are not promising. It\'s not even just the ongoing control of these resources that is the issue. Whoever creates the software and hardware for the AI would have special knowledge. If fine adjustments were made, specific individuals would undoubtedly hold sole control of the AI, as adjustments could be made to the code in such a way that the AI behaves the same except under specific circumstances, and then when something goes wrong (assuming the AI has its own rights), then the AI would be blamed rather than the programmers who were responsible.3. AnthropocentrismIn order for humanity to get away from anthropocentrism, we would have to become less selfish when it comes to humans, first. Until we can solve every existing social problem within humanity, there is no reason to believe that we could cease thinking of humanity as more important than created machines. After all, supposing there were an almighty God that created humanity, wouldn\'t the humans always be beneath God, never to be equals? We can\'t fully understand our own biology. If an AI were created, would it be able to understand its makings in the same way its creators would? Being the creator would give humanity a sense of megalomania. I do not think that we would relinquish our dominion over our own technological creations. That is as unlikely to happen as the wealthiest of humanity willingly giving the entirety of their money, power, and assets to the poorest of humanity. Greed prevents it.4. Post-human fundamentalismHumans worship technology with their attention, their time, and their culture. Some movies show technologically advanced robots suppressing mankind to the point of near-extinction. If this were the case and humanity were in danger of being surpassed by its technology, humanity would not stand idly and watch its extinction at the hands of its creation. Though people may believe superior technology could be created, in the event we reached such a point humanity would fight to prove the opposite, as our survival instincts would take over.5. A balance?Personally, I do not think the technology itself it actually possible, though people may be deceived into thinking such an accomplishment has been achieved. If the technology were completed, I still think that anthropocentrism will always lead, because if humanity is the creator, humanity will do its best to ensure it retains control of all technological resources, not simply due to fear of being made obsolete, but also because absolute power corrupts absolutely. Humanity does not have a good historical record when it comes to morality. There is always a poor class of people. If wealth were distributed equally, some people would become lazy. There is always injustice somewhere in the world, and until we can fix it (I think we cannot), then we will never be able to handle the creation of true AI. I hope and think that it will never be created.'	824	0	0	0
2501	2481	1	3755	0	b'Reinforcement learningThe problem that you describe, namely, choosing a good sequence of actions based on a reward/score received based on the whole sequence (and possibly significantly delayed), is pretty much the textbook definition of Reinforcement learning.As with quite a few other topics, deep neural networks currently seem to be a promising way for solving this type of problems. This may be a beginner-friendly description of this approach.'	66	0	1	0
2502	2474	0	33008	1	b'Theory of mindRelationships and normal social behavior require a human to possess a reasonable "theory of mind", a skill in understanding and modeling the thought processes that happen in the minds of others, and making reasonably accurate predictions on how particular actions will be understood by others.In general, this might be treated as any other machine learning/prediction task - while this skill is quite complex and too hard for our current systems, there doesn\'t seem to be any obvious qualitative barrier that needs to be breached. Notably, there\'s no reason to believe that a mind needs to be able to experience a certain "feeling" in order to model that other minds can have it - an AI agent could form a causal model of how friendship works in human relationships and use that to exhibit behavior that\'s consistent with friendship in all aspects and/or facilitate friendship behavior towards it by particular humans, if it fits the agent\'s goals. Whether you\'d consider that "real friendship" is pretty much just a matter of how you define the word, with some parallels to the "p-zombie" discussion.'	182	0	0	0
2503	2441	0	31828	1	b'Does it benefit us?To answer this question, it\'s worth considering practical reasons why we grant or don\'t grant other people rights historically and currently.In essence, this is an arbitrary choice - there certainly were well functioning societies that didn\'t grant rights to many or most people; and we still don\'t grant some rights to many people - for example, we deny children the same rights to self-determination that adults have; we consider some people legally incapacitated and allow others to make key decisions for them; and we exclude most people from having a say in \'local\' matters, e.g. non-citizens don\'t get a right to vote.However, there has been a strong historical trend towards a more inclusive society - granting full(er) rights to non-aristocrats, granting full(er) rights to all races, granting full(er) rights to women. IMHO, and there\'s lot of space for discussion, this has been driven mostly by two factors:1) including all the people fully in the society became an economical advantage, as it made them more productive participants in economy, allowing a more inclusive society to advance beyond societies neglecting large parts of their population in e.g. education and participation in skilled jobs;2) A more egalitarian society is not only more pleasant to live in but more secure, with less conflict and violence - again, giving an advantage to a more inclusive society.From this position, I\'d argue that any realistic prediction about the future rights of intelligent AI (i.e., talking about what likely will happen instead of a theoretical discussion about what should happen) depends on how these two factors apply.If we believe that the intelligent AI will be constructed so that (1) it\'s motivation doesn\'t really depend on it\'s rights, and it is fully committed to it\'s "job" anyway, and (2) "full rights" are orthogonal or even actively not desired by it\'s goal system, so the situation doesn\'t raise a risk of "rebellion" - then I\'d expect that it would not be granted full rights.If we believe that the intelligent AI will share human-like emotions (e.g. by being the result of "mind uploading" or full human brain simulation), then it\'s likely to eventually be granted full rights because of the same factors why we granted full rights to all the different disenfranchised groups of people.'	376	0	0	0
2504	2443	0	31193	0	b'In order for something to replace AI, it would need to out perform AI. AI currently uses number systems to represent information and logic to perform operations. So the replacement would need to be based off of something more efficient than numbers and logic. Some sort of super-logic. Or something similar to intuition and instinct which do not require linearly figuring things out.'	62	0	0	0
2506	2475	1	64267	3	b'I would recommend to start by reading this blogpost.You can probably cannibalise the code to create a RNN that takes in one statement of a dialogue and then proceeds to output the answer to that statement.That would be the easy version of your project, all without word vectors and thought vectors. You are just inputting characters, so typos don\'t need to concern you.The next more complex step would be to input word vectors instead of characters. That would allow you to generalise to words that aren\'t part of your training data. And it is probably still just a minor modification of the code. If you insist on using thought vectors, you should start reading up on NN translation. And probably try to get a pre-trained encoder network. Or pre-train it yourself on a large translation corpus for your language. With your small training set the best you can do is probably massively overfit until your system recreates your training data verbatim. Using word vectors will allow your system to give the same answer to "I beat the cat today." as you gave in the training data to "I kicked the dog yesterday." I\'m not sure thought vectors will make a big difference. If you get the decoder to learn at all. '	211	0	1	0
2507	2490	1	26343	1	b'This question should probably be moved to worldbuilding.stackexchange \xe2\x80\xa6That being said, in the context of a story, I would look at something like the neural correlates of consciousness. In this book by Stanislas Dehaene the experiments are described that led to the realisation that conscious perception requires information integration in certain parts of the prefrontal cortex. Basically the brain processes many different interpretations of a percept in parallel. To become conscious of the percept this interpretations have to be collapsed into a single "truth" which is propagated to a specific part of the brain. In your story you could have a researcher of consciousness stumble upon a chart of information flow in a part of the internet and realise that the same kind of information integration is going on. '	129	0	2	0
2508	-1	0	0	1	b"A professor and I have been learning about artificial neural networks. We have a pretty good idea of the basics- backpropagation, convolutional networks, and all that jazz. We finished one book and are looking for a new one.I'd prefer something that either puts a new spin on the basics or is more advanced.We are both mathematicians and focus on the math more than the programming of it.One of our thoughts was to look into recurrent neural networks.Does anyone know of good resources to continue learning about these topics? Or any other ideas besides recurrent neural networks?"	95	0	0	0
2509	2328	1	77893	0	b"That's a multifaceted question. The referenced articles are conjectures much like the hundreds of thousands of other interesting and carefully considered works on the net and in libraries. Whether any of them bring us down a dark ally or directly toward the objective the question implied is unknown.The array of common human desires are suggested by derivatives of Abraham Maslow's hierarchy of needs. Self awareness is not among them. From a cognitive science perspective, self awareness is not a desire but rather trait of consciousness. The plausible explanations for its presence ranges between the extremes of a completely useless artifact of mental evolution to a deliberate infusion from some supreme entity, and everything in between.The other desires, traits, and capabilities listed in the question are easier to discuss in a way that is backed by existing cognitive science, psychology, neuroscience, and self-evident human experience.Regulating and prioritizing may an important element in intelligence beyond problem solving. For instance, a sense of purpose is only a desire as one moves up the hierarchlistedny of needs. People running from a Mammoth have no interest in purpose. Regarding competent communication skills, what is and what is not sufficiently competent would require a definition, and not all humans may qualify all of the time.The sustainable ability to adapt is perhaps a super-set of intelligence in some ways, as in the case of the prioritization that exhibits the hierarchy of needs above and as in the case of DNA based adaptation. In other cases adaptability may be a subset of intelligence, in the case of applying taught facts and logical deduction to select an optimal choice without the benefit of empirical observation or even trial and error.The intelligent agent approach may produce something useful, but it is not likely the path that led to a human brain and may not produce anything like it. We already understand what placing a number of such agents on a problem queue produces. A time sharing computer is exactly of that architecture. Parallel computing is not a significant deviation from that model.None of the current extensions of automation architectures from Bell Labs, Harvard U, MIT, or other historical centers of electronic brain and robotic research and development have produced anything close to the varied integrated capabilities of a biologically healthy, socially aware, motivated human brain functioning as one of the controlling elements within a human body.We do, however, have machines with human-like capabilities (simulations of task that were once manual), and such devices sometimes out-perform humans. But none of the existing automation is so integrated that they would blend in with humans. Nonetheless, many already have careers.Single capability machines with job security:CalculatorsAutomated hoppersAnti-aircraft weaponryMail sorting machinesSpeech recognizing devicesSpeech synthesisA general purpose computer, connected to a well designed, high end robot could do any combination of these and other things in a time shared way. Several networked computers could provide parallel execution of these tasks and even make basic decisions on how to prioritize the various tasks within the computer network.However, the type of integration of sophisticated translation of arbitrary generalized goals to action leading to success and the imagination and prioritization of such sophisticated objectives is not yet available or at least deployed for general home, commercial, or industrial use.Some available machines already accomplish well defined goals, but only within a very specific solution domain. The interaction of various capabilities mentioned is still largely undiscovered at anything close to the level of detail necessary to produce synthetic capabilities of the same caliber.Whether it is possible to do so, Marvin Minsky voted yes, when he popularized his colleague's characterization of the human brain as a meat machine. Whether consciousness transcends neural electro-chemistry has yet to be proven or disproved in either the mathematical sense or the laboratory sense of the term Proof.Interestingly, a form of self-consciousness, some layer of it, seems to require the ability to make a noise and hear it. Another layer beyond that seems to require the ability to move a muscle and view the results in a reflection. These and others form an interesting set of phenomena.Perhaps intelligence requires embodiment. One of the episodes of Sarah Conner Chronicles implied this, and the theory is an old one recognized independently by several researchers. The question author implied this vaguely self-evident reality in the mention of action in the question."	720	0	0	0
2511	2508	0	20456	0	b"I'd definitely recommend Deep Learning by Goodfellow, Bengio and Courville. I took a PhD level course in Neural Networks a few months ago using this book as the main reference. If you know the basics you can skip to the chapters you're interested in, and the last part of the book is supposed to be as bleeding edge as you can get in a text book. The whole book is available for free, under what seems to be some kind of pre-print arrangement, at  where you can also find lecture slides and exercises.Disclaimer: I'm not a mathematician but a computer scientist (and a pretty applied one as well, as I'm primarily doing research in applied machine learning for NLP) so I can't promise you'd find it as good for your particular interests, as I did for mine."	137	0	1	0
2512	-1	0	0	2	b'It is really all in the title.For those less familiar, the Fermi Paradox broadly speaking asks the question "where is everybody". There\'s an equation with a lot of difficult to estimate parameters, which broadly speaking come down to this (simplification of the Drake equation):(Lots stars in the universe) * (non-zero probability of habitable planets around each star) * (lots of time spanned) = It seems there really should be somebody out there.There are, of course, plenty of hypotheses as to why we haven\'t seen/observed/detected any sign of intelligent life so far, ranging from "well we\'re unique deal with it" to "such life is so advanced and destroys everything it comes across, so it\'s a good thing it didn\'t happen".The technological singularity (also called ASI, Artificial Super Intelligence) is basically the point where an AI is able to self-improve. Some think that if such AI sees the light of day, it may self-improve and not be bound by biological constraints of the brain, therefore achieve a level of intelligence we cannot even grasp (let alone achieve ourselves).I certainly have my thoughts on the matter, but interested to see if there is already an hypothesis revolving around the link between the 2 out there (I never came across but could be). Or perhaps an hypothesis as to why this cannot be.For references to those not familiar with the Fermi paradox'	227	0	1	0
2513	2512	0	19868	1	b"If the technological singularity always leads to the extinction of all intelligent life, then yes.If the technological singularity always leads to intelligent life migrating into higher planes of existence that aren't accessible to us right now, then also yes. Otherwise it is exactly the assumption of unbridled technological progress that makes the Fermi paradox perplexing. A post-singularity culture should have the ability to spread through the galaxy. If there are a lot of post-singularity cultures some of them should have spread through the galaxy. And if there are enough cultures that are spreading through the galaxy, we should notice them."	99	0	0	0
2514	-1	0	0	7	b'I understood that searching is important in AI. There\'s a question on this website regarding this topic, but one could also intuitively understand why. I\'ve had an introductory course on AI, which lasted half of a semester, so of course there wasn\'t time enough to cover all topics of AI, but I was expecting to learn some theory behind AI (I\'ve heard about "agents"), but what I actually learned was basically a few searching algorithms, like:BFSUniform-cost searchDFSIterative-deepening searchBidirectional searchthese searching algorithms are usually categorised as "blind" (or "uninformed"), because they do not consider any information regarding the remaining path to the goal. Or algorithms like:Heuristic searchBest-first searchAA*IDA*which usually fall under the category of "informed" search algorithms, because they use some information (i.e. "heuristics" or "estimates") about the remaining path to the goal.Then we also learned "advanced" searching algorithms (specifically applied to TSP problem). These algorithms are either constructive (e.g., NN), local search (e.g., 2-opt) algorithms or meta-heuristic ones (e.g., ACS, SA, etc).We also studied briefly a min-max algorithm applied to games and an "improved" version of the min-max, i.e. the alpha-beta pruning.After this course I didn\'t remain with the feeling that AI is more than searching, either "stupidily" or "more intelligently".My questions are:Why would one professor only teach searching algorithms in AI course? What are the advantages/disadvantages? The next question is very related to this.What\'s more than "searching" in AI that could be taught in an introductory course? This question may lead to subjective answers, but I\'m actually asking in the context of a person trying to understand what AI really is and what topics does it really cover. Apparently and unfortunately, after reading around, it seems that this would still be subjective.Are there theories behind AI that could be taught in this kind of course?'	295	0	1	0
2515	-1	0	0	1	b"Ethics is defined as moral principles that govern a person's or group's behavior; innately, Shouldn't it the system itself which should devise ethics? Most of the articulations, in either direct or indirect manner, indicate Intelligent Systems in context of human presence. Why, in first place, are we studying AGI Ethics?Given Roseau's reasoning for a society,  Human beings can improve only when they leave the state of nature and enter a civil society.Doesn't this automatically apply for any intelligent system? If it does, then can't we imply that any inorganic intelligent system should, perhaps, come as an offset of a network of Intelligent computers (a society per say)?Who are we to constitute ethics, rules, morale for another society which, perhaps, could be much more intelligent that us? In fact, Human Species has been way too stupid to account for such a task."	141	0	0	0
2516	-1	0	0	2	b'To solve problems using computer programs, we have developed a wide set of tools / control flow statements such as For Loop, If-Else, Switch-Break Statements and so on. How natural are these control flow statements? Since, we do not know, how exactly would AGI work, and the understanding of Neural Networks, so far, does not tell us about origin of "intelligence", Could a modern AI evolve itself into a system of such control-flow elements? (An appropriate architecture for computation is , anyways, necessary for any inorganic intelligence system and it is of no doubt that control-flow statements as such just makes the computation much more organized)If so, could an AI be killed in an infinite loop created by itself?P.S. The question isn\'t baseless, it really questions the kind of computational infrastructure a modern AGI would require, and would it be able to alter itself without any intervention.'	146	0	0	0
2517	-1	0	0	1	b'I am coding a tic-tac-toe program that demonstrates reinforcement learning. The program uses minimax trees to decide its moves. Whenever it wins, all the nodes on the tree that were involved in the game have their value increased. Whenever it loses, all the nodes on the tree that were involved in the game have their value decreased, etc. What is the name of the value that each node is decreased by?'	70	0	0	0
2518	-1	0	0	0	b'Neural Net can be feed-forward or recurrent. Perceptron is only feed forward.So, what is Hopfield Network then?'	16	0	0	0
2519	2518	0	1373	0	b"Hopfield Networks are recurrent. However, they are not as general as more modern Recurrent Neural Networks such as Long Short-Term Memory Networks as they cannot process sequential input. I've never worked with a Hopfield Network but I've been told that they are mostly of historical interest today due to their limitations."	50	0	0	0
2520	-1	0	0	3	b'Is a Levenberg\xe2\x80\x93Marquardt algorithm a type of back-propagation algorithm or is it a different category of algorithm?Wikipedia says that it is a curve fitting algorithm. How is a curve fitting algorithm relevant to a neural net?'	35	0	0	0
2521	2514	0	72559	2	b"What it comes down to is that most AI problems can be characterized as search problems. Let's just go through some examples:Object recognition &amp; scene building (e.g. the process of takingaudio-visual input of your surroundings and understanding it in a 3Dand contextual sense) can be treated as searching for known objectsin the input.Mathematical problem solving can be treated as searching for a solution.Playing a video game can be treated as searching for the correct response to a given gamestate.Even rudimentary chatbots can be characterized as finding the 'correct' response to a given input phrase to emulate human language!Because of this generalization of search, search algorithms were among some of the first algorithms considered 'AI', and often form the basis of many AI teaching courses. On top of this search algorithms are intuitive and non-mathematical, which makes the somewhat terrifying field of AI accessible. This might sound like hyperbole, but I guarantee that if your lecturer had opened with Manifold Learning Techniques half of your class would have bolted for the door by the time they mentioned 'eigenvalue of the covariance matrix'.Now search algorithms aren't the only way to address these problems. I recommend every AI practitioner is familiar with the notion of Data Science and Machine Learning Algorithms. ML is often related to search algorithms but the techniques they use can vary heavily from iterative building of a classifier/regression (e.g. C4.5 builds a decision tree), meta-heuristics as you noted, and classifiers/regression that are statically generated from analysis of training data (e.g. Naive Bayesian is literally a classifier built on Bayesian analysis of the given data assuming that input fields are independent - this is the 'naivety' from which it gets its name). Often ML algorithms are developed in AI research groups and can sometimes be designed for specific problems instead of being general form algorithms. In contrast to the general field of AI, which is often centered on Intelligence problems and is therefore (in my view) vulnerable to too much blue sky thinking, ML is applied to all sorts of real life problems and is often very practical in its design and performance driven in its evaluation."	356	0	0	0
2522	2516	1	34168	3	b"This is a hard to answer question because a truly correct answer would involve static analysis of a given Intelligence to determine whether it has the computational capability to generate a looping state (e.g. some state which reproduces itself in the next instance), and in fact whether these looping states can even exist in the given architecture. Generally speaking, assuming we can build AI, there is no reason it would be impossible build an AI that has the potential to get stuck in a computational loop and become non-functional. You can imagine a recurrent neural network where the continuous state inside the ANN dominates its behavior causing it to reproduce states with small variety (as input will always be relevant in a connected ANN) but are converged around a particular state space. Whether that is akin to death is a philosophical question for your own peace of mind.However, if we designed an AI agent that commonly did this, that would reflect poor design; that wouldn't necessarily devalue the contribution of the designed agent but it would bring the agent's other activities under scrutiny. If this is a bi-product of some simulated intelligence, is the process going on really that intelligent?"	199	0	0	0
2523	2520	0	8518	1	b'In the context of Neural Networks, Backpropagation (with Gradient Descent, to use its full name) and Levengerg Marquardt are both members of the broader family of gradient descent algorithms. Backpropagation itself is not gradient descent, but it does the gradient climbing portion of a broader gradient descent algorithm. You can imagine the function of a Neural Network as a function from its inputs to its outputs. If you were trying to solve, for example, a regression problem you can imagine this problem in multidimensional space with each training point corresponding to the coordinate provided by the values of its inputs and outputs (each of which represent a dimension). Then the entire training set you have for learning this regression problem become a set of points in this multidimensional space, and the function that your neural network performs is a curve in this multidimensional space. The closer this curve is to the points on the training set, the better it performs at the regression problem which essentially means we can generalize the task of training the neural network to a curve fitting problem.'	181	0	0	0
2524	-1	0	0	6	b'I am new to neural-network and I am trying to understand mathematically what makes neural networks so good at classification problems. By taking the example of a small neural network (for example, one with 2 inputs, 2 nodes in a hidden layer and 2 nodes for the output), all you have is a complex function at the output which is mostly sigmoid over linear combination of sigmoid.So, how does that make them good at prediction? Does the final function lead to some sort of curve fitting?'	85	0	0	0
2525	2498	1	16041	3	b'Many large deployments of AI have carefully engineered solutions to problems (ie self driving cars). In these systems, it is important to have discussions about how these systems should react in morally ambiguous situations. Having an agent react "appropriately" sounds similar to the Turing test in that there is a "pass/fail" condition. This leads me to think that the current mindset of most AI researchers falls into "Conservative anthropomorphism".However, there is growing interest in Continual Learning, where agents build up knowledge about their world from their experience. This idea is largely pushed by reinforcement learning researchers such as Richard Sutton and Mark Ring. Here, the AI agent has to build up knowledge about its world such as: When I rotate my motors forward for 3s, my front bump-sensor activates.and  If I turned right 90 degrees and then rotated my motors forward for 3s, my front bump-sensor activates.From knowledge like this, an agent could eventually navigate a room without running into walls because it built up predictive knowledge from interaction with the world.In this context, lets ignore how the AI actually learns and only look at the environment AI agents "grow up in". These agents will be fundamentally different from humans growing up in homes with families because they will not have the same learning experience as humans. This is much like the nature vs nurture argument. Humans pass their morals and values on to children through lessons and conversation. As RL agents would lack much of this interaction (unless families adopted robot babies I guess), we would require different ways of judging their moral worth and thus "Post-human fundamentalism".Sources:5 years in the RL academia environment and conversations with Richard Sutton and Mark Ring.'	283	0	1	0
2526	-1	0	0	3	b'Is it a must that an activation function of a neural network be differentiable?Why should an activation function of a neural network be differentiable?Is it advantageous to have a differentiable activation function? Why?'	32	0	0	0
2527	2526	0	4196	1	b'TrainingWhile "running" a neural network can be done with any activation functions, we usually want to train it - i.e., adjust its parameters so that the result would be closer to what we desire.This is commonly done by backpropagation and variations of gradient descent, which requires the existence of a gradient - i.e., requires activation function to be differentiable. The adjustment of each parameter is calculated from the gradient of the activation function(s) that this parameter affects, so if you cannot get a gradient, then this approach can\'t be used.'	89	0	0	0
2528	-1	0	0	0	b"I understand an MDP (Markov Decision Process) model is a tuple of {S, A, P, R} where:S is a discrete set of statesA is a discrete set of actionsP is the transition matrix ie. P(s' | s, a) -> [0,1]R is the reward function id. R(s, a, s') -> realFor a non-trivial MDP, say 1000 states and 10 actions, the transition matrix has theoretically S x A x S = 10,000,000 entries (though many entries will be 0).I understand that one way of generating the P matrix is to estimate it via Montecarlo sampling by simulating the environment. However with non-trivial state space and simulation costs this could be prohibitively expensive.In practice, when a non-trivial MDP is being formulated, what are the different ways an accurate P matrix can be produced?"	130	0	0	0
2529	2515	0	11946	1	b'Nonhuman ethics can be totally arbitraryRegarding your citation, perhaps it\'s useful to consider why should anything that philosophers have said about human ethics apply for arbitrary nonhuman systems? We all share a common biological background, our behavior, emotions and especially positive/negative feelings arise from the particular way how our brains are built. Man is by nature a social animal, and much of our behavior and feelings of empathy, compassion, fairness, greed, social status, etc are formed and expressed in society. Our minds are adjusted for this, as are the minds of other mammals. A human that forms an ethical system when joining a civil society does so in the basis of all this shared context, and within the limitations of it.However, all this is orthogonal to intelligence. All the variety of mankind encompasses a tiny island in the whole space of possible intelligences and their goals. Any system of ethics is feasible, a vast majority of them having no relationship whatsoever to what we could consider ethical. A human can be expected to join a society and learn ethics because our brain is hardwired to respond to social stimuli - an AI would learn ethics from a society only if its previous ethics already happen to value that highly, and that isn\'t likely to happen without careful design.There is a thought experiment that is commonly used as an example - the "paperclip maximizer", an artificial agent with an ethical system that defines \'good\' proportionally to the number of paperclips that exist in the universe and only that - and that is a perfectly reasonable example, because a completely random ethical system is overwhelmingly likely to reduce to something stupid like that. The space of "reasonable" (to us) ethical systems is very tiny compared to all ethical systems possible, and expecting that a randomly selected ethical system will just happen to land there is like relying on winning a high-stakes lottery.We want their ethics to contain certain thingsSo the main reason why we are studying AGI ethics is because we (or some of us) really, really, want the AGI ethical system to include certain things. For example, I\'d like that ethical system to include my survival. I\'d prefer that ethical system to be consistent with my happiness and wellbeing - and the very existence of a very powerful entity that simply happens to not care about me at all is not compatible with my survival and wellbeing, so I\'d really prefer for it to care about me, or alternatively never become very powerful.One could certainly argue that we have no right to constitute ethics, rules and morale for another society which could be much more intelligent and better than us. That another society might be "better" in some manner - but better for whom ? If it\'s better for "them" but worse for me (and humanity), then I have strong motivation to simply claim that right and try my hardest to ensure that this hypothetical future society includes me, my desires and my ethics. '	503	0	0	0
2530	-1	0	0	2	b'Generating a discretized state space for an MDP (Markov Decision Process) model seems to suffer from the curse of dimensionality.Supposed my state has a few simple features:Feeling: Happy/Neutral/SadFeeling: Hungry/Neither/FullFood left: Lots/Some/Low/EmptyTime of day: Morning/Afternoon/Evening/NightLocated: Home/Work/Library/ShopsMoney in wallet: $0, $0-$50, $51-$200, $200-$1000, $1000+This is a relatively compact set of information however since the number of states multiplies out, the corresponding MDP state space is 3 x 3 x 4 x 4 x 4 x 5 = 2880.For a non-trivial problem with a greater number of choices per factor, the state space quickly becomes unmanageably large.It seems to me that the usefulness of MDPs with large complex problems would be very limited.Is that the case, or are there ways of keeping the state space manageable for more complex problems?In general, what is a manageable number of states for an MDP to have, considering the need to generate the transition matrix and reward matrix?'	150	0	0	0
2531	-1	0	0	1	b' The neuropsychologist Donald Hebb postulated in 1949 how biological neurons learn:  \xe2\x80\x9cWhen an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place on one or both cells such that A\xe2\x80\x99s efficiency as one of the cells firing B, is increased.\xe2\x80\x9d  In more familiar terminology, that can be stated as the Hebbian Learning rule:  If two neurons on either side of a synapse (connection) are activated simultaneously (i.e. synchronously), then the strength of that synapse is selectively increased.  Mathematically, we can describe Hebbian learning as:    Here, \xce\xb7 is a learning rate coefficient, and x are the outputs of the ith and jth elements.Now, my question is, what do all these descriptions mean?Is Hebbian Learning applicable for single-neuron networks?What does it mean by "two neurons on either side of a synapse"?Why/when would two neurons activate simultaneously?What does they mean by elements?'	166	0	1	0
2534	2531	1	8577	4	b'Hebbian learning is trying to answer "How the strength of the synapse between 2 neurons evolve over period of time based on the activity of the 2 neurons involved.". You can call it learning if you think learning is just strengthening of synapses. The connection between 2 neurons are called synapse. A synapse is the point where the axons of a neuron meets with the dendrites of another neuron. The 2 neurons are connected to each other but they are also connected to other neurons as well. So it may happen that both the neurons gets activated by their other connected neurons at the same time.The elements refer the 2 neurons. The equation is basically saying that the synapse strength between i and j neuron at time (n+1) depends on its strength at time n plus activations of the i and j neurons at time n. '	146	0	0	0
2535	-1	0	0	3	b"Just started reading a book about AI. There is a very basic exercise but I can't figure it out, so here we go. The book is Simply Logical: Intelligent Reasoning by ExampleThe exercise is in the page 19.  Two stations are \xe2\x80\x98not too far\xe2\x80\x99 if they are on the same or a different line, with at most one station in between. Define rules for the predicate not_too_far.The only rules I've seen are nearby and connected and don't know how to use this. What I've done so far is this: not_too_far(X,Y) :- nearby(X,Y)"	92	0	0	0
2536	2518	0	29481	2	b'Hopfield stores some predefined patterns (lower energy states) and when an non seen pattern is fed to the Hopfield net, it tries to find the closest match among the stored patterns. Hence the output of a Hopfield is always one of the predefined patterns which matches closely to the unseen input pattern.This is not the case with Feed Forward Neural Nets (where no such predefined patterns are stored) and every input generates a corresponding output. Here (in Feedforward) the output is generated by a predefined function (which is self-adjusted during training session) where as in Hopfield predefined patterns are stored and outputted (no such functions exist).'	105	0	0	0
2537	2530	1	63444	2	b'tl:drRead chapter 9 of https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdfThere is definitely a problem (a curse if you will) when the dimensionality of a task (MDP) grows. For fun, lets extend your problem to a much harder case, continuous variables, and see how we deal with it.Now its impossible to even count the number of states the agent could be in. An example of one state would be:This would mean that our agent has a neutral mood, is kind of hungry, has 67.4% food, the time is 4:30 am, they are in the center of the city (0,0), and have 5 dollars. Which has happened to me once or twice before too so this is reasonable. If we we still wanted to generate a transition matrix and a reward matrix we would have to represent this state uniquely from other possibly similar states such as if the time was 5 pm with all other things equal.So now how would we approach this?First lets discretize or breakup each range into little chunks. Then we could have an array of 0s representing our chunks and assign a 1 to the index of the chunk we are in. Lets call this array our feature array. If we did this for each dimension of our state information, we would end up with some state similar to what you originally proposed. I\'ve chosen some arbitrary numbers to break up the state space and lets see what it does to the example state.If we broke the Mood range up into 8 chunks of .25 we would have these ranges:m = Mood-1.0 \xe2\x89\xa4 m &lt; -.75-.75 \xe2\x89\xa4 m &lt; -.50-.50 \xe2\x89\xa4 m &lt; -.25-.25 \xe2\x89\xa4 m &lt; 0.00.0 \xe2\x89\xa4 m &lt; .25.25 \xe2\x89\xa4 m &lt; .50.50 \xe2\x89\xa4 m &lt; .75.75 \xe2\x89\xa4 m &lt; 1.0If we took the mood from the example state (.5), it would land in the 7th range, so then our feature vector for mood would look like:Lets do the same for hunger by splitting it into 8 chunks of .125:We could then do something similar for each other state variable and break each state variable\'s range up into 8 chunks of equal sizes as well. Then after calculating their feature vectors, we would concatenate them all together and have a feature vector that is 56 elements long (7 state variables * 8 chunks each).Although this would give us 2^56 different combinations of feature vectors, it actually doesn\'t represent what we wanted to represent. We need features that activate (are 1) when events happen together. We need a feature that is 1 when we are hungry AND when we have food. We could do this by doing a cross product of our 56 element feature vector with itself.Now we have a 3136 element feature vector that has features like: 1 if its 3pm and am happy 1 if am full and out of food 1 if at coordinate -3, 4 (Position x, Position y)This is a step in the right direction but still isn\'t enough to represent or original example state. So lets keep going with the cross products!If we do 6 cross products we would have 30,840,979,456 features and only 1 of them would be on when our agent has a neutral mood, is kind of hungry, has 67.4% food, the time is 4:30 am, they are in the center of the city (0,0), and have 5 dollars.At this point you are probably like "Well.. thats a bit much", to which I would agree. This would be the curse of dimensionality, and is the reason transition tables stop being fun (if they ever were).Instead lets take a different approach, rather than trying to represent an individual state with an individual feature and saying whether that feature is good, lets go back to our nice 56 element feature vector. From here lets instead give a weight (w_i) to each of our 56 features. We have now entered the world of linear function approximation. Although I could explain it here, I think its better explained in Chapter 9 of the Introduction of Reinforcement Learning. https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf'	673	17	0	0
2538	1461	0	15592	1	b'Short Answer, No.Explained, Siri and Cortana are just inference engines. Though how applaudable their ability to synthesize text from speech and parse lexical maps from the text using Machine Learning Techniques is, the artifact is still just a program, trained with substantial myriad of Q/A tuples, that generates an output given an input. Statistically mapping the probability distribution of words of a question and accordingly generating an answer is just NOT an AI, however, to the most, it can be classified as Narrow Intelligence. www.cleverbot.com is an example of such a system. The ones you mentioned are just more sophisticated and highly architectured versions of this kind; for example, connection to WolframAlpha (Siri), Computer Kernel (Cortana). In fact, there is a lot of innate use of control statements (If-Else).'	128	0	1	0
2539	2526	0	82716	4	b'It is almost mandatory to have a differentiable activation function unless, of course, you have an alternative to training the network by back-propagating the error.'	24	0	0	0
2540	2526	0	43497	2	b'As already said , Activation function is almost differentiable in every neural net to facillitate Training as well as to calculate tendency towards a certain result when some parameter is changed. But I just wanted to point out that The Output function need not be differentiable in all cases. We can have non-sigmoid (hard-limiter threshold)output nodes but still train them with backpropagation and gradient descent.'	64	0	0	0
2541	2404	0	14361	2	b'It depends on the accuracy you want. If you had 1 neuron, it could discern things across a line, if you have 2, you could solve things across 2 lines, etc. As you increase the number of neurons, you are increasing the number of discernible areas. As you increase the number of lines you can use to break up the input space, the lines can be placed to approximate any curve (sinusoidal) As the number of neurons approaches infinity, the accuracy of categorizing different inputs across this curve increases. Interestingly enough, if one graphed "Number of Neurons (x) vs Accuracy (y)", it would look sinusoidal. '	105	0	0	0
2542	-1	0	0	0	b'What are the basic layers on an Artificially Intelligent program and what skills and concept are required to work on this field. Total newbie interested in AI.'	26	0	0	0
2543	2535	1	81878	1	b'Your intuition is good. Because "nearby" is only defined with "connected", there could only be 1 station between them. However, it says that the stations are "not_too_far" if at most one station is between them. What about if no stations are between them? If 2 stations are "connected" they should be "not_too_far" as well. So it should be:Where ; denotes OR. '	61	1	0	0
2544	2542	0	27627	0	b"The basic layers of AI aren't that interesting at the highest level. In the most abstract sense, AI is simply a function f(x) which is given input x and returns an output. This isn't that exciting, so let's break it down a bit more.AI can be broken by 2 different aspects. AI can be Online or Offline. AI can be Supervised or Unsupervised. AI is Offline if it learns the function f before being released into the real world at which point it doesn't update f. AI is Online if it is put in the real world immediately and must learn f on-the-fly during operation. AI is Supervised if it is given the correct answer after guessing. This is used to update f.AI is Unsupervised if the correct answer is not given. This could mean that some indication of how well the AI did is returned (such as in reinforcement learning) or nothing at all. Choosing 1 of each aspect gives 1 of 4 categories, such as Unsupervised Online learning. AlphaGo which beat on if the worlds best Go players is an example of an Unsupervised Online learning system. All of these types of AI require representing the input x and choosing an output. Both of which involve statistics, math, and some programming experience. Anyone wanting to work in AI should have these skills and be able to think critically about them (mostly to diagnose the bugs that are bound to happen)."	241	0	0	0
2545	-1	0	0	1	b'The following text is from Hal Daum\xc3\xa9 III\'s "A Course in Machine Learning" online text book (Page-41).I understand that, D = size of the input vector x.(1) What is y? Why is it introduced in the algorithm? How/where/when is the initial value of y given? (2) What is the rationale of testing ya&lt;=0 for updating weights?'	55	0	1	0
2546	2545	0	7710	2	b'Y is the desired output of the perceptron (often referred to as target) , for the given set of input vectors.Rationale behind Y.a&lt;=0 :Prerequisite knowledge :A=A-B : Moves vector A away from direction of vector BA=A+B : Moves A in the direction of BA (.) B >0 ; A vector is directed acutely (&lt;90 deg.) towards B vectorA (.) B &lt;0 ; A vector is directed away from (>90 deg.) from B vector [(.) denotes dot (scalar) product and bold letters indicate vectors]W is augmented vector (includes threshold as another weight along with normal input weights)X is augmented input (includes -1 as extra input (corresponding to threshold) along with other normal inputsa(activation) = W (.) Xa >=0 ; Perceptron output 1a&lt;0 ; Perceptron output -1 (Not zero as implicit in the given algorithm I think)Now Rationale :(Y.a&lt;0) This means Either of the following :Y=-1 and a>0 ; in this case the target output is -1 but as a>0 so the Perceptron outputs 1. So we must move the weight vector away from this set of input vector, so that angle between them increases and the dot product (a) becomes &lt; 0 so that we can get the target output.Hence : W=W-XOr ,W=W + (-1)*XOr, W=W+YXY=1 and a&lt;0This means the target output is 1 but as the activation is &lt;0 so Perceptron is outputting -1. So we must move the weight vector close to this set of input vector so that the activation can become >0 (angle decreases) and the Perceptron can output the desired output.So :W = W+XOr W=W + YXAgain , Y.a=0 is a boundary case.By now I think you can understand the rationale behind Y.a=0. If any doubt , comment to this answer , I will explain it.Sorry for so much long answer though. :) :)'	297	0	0	0
2547	-1	0	0	3	b"Recently Mark got some attention from the media by stating that he had created Jarvis. Not that I'm against him or anything, but this Jarvis seems to have been done a hundred times before. He's done something which most developers would classify as a home automation system. To me it's more like he did it for the attention. I was kind of taken back by the amount of media attention he got. If you've heard of Jeremy Blum, maybe you may understand what I'm trying to imply here. I'm just curious as to why he got so much attention. Is there anything technically novel about his system that sets it so much apart from previous ones?"	115	0	0	0
2548	-1	0	0	5	b'What is the basic difference between a Perceptron and a Naive Bayes classifier?what more differences do they have?'	17	15	0	0
2552	-1	0	0	0	b'In the "Perceptrons" introduction by Minksy and Papert, they give a proof that the predicate of whether set of points is connected is not conjunctively local of any order k. They do this by stating that, because of the limited number of points k, there must be some middle square that would not contain one of these points. Therefore, the connected and non-connected figures would return the same result.While I agree with the assertion that if there is a middle square which does not contain any of the k points, then we could not distinguish between the connected and non-connected figures, I don\'t understand how we get to that premise.My question is: How can we say that there must be a middle square which does not contain any of the k points? A point is not a square, so couldn\'t we have many more points than squares? In fact, I don\'t really understand how we can even call these k+1-wide figures, given that k refers to a number of points, not a distance measurement. How can we say something is "k points wide", when points can be infinitely subdivided and have no actual length? '	194	0	0	0
2554	2547	0	78464	4	b'No, there is nothing novel about this system. The main hurdle he had to pass was problems that you will face when your system has a lot of integration points across various APIs provided by different vendors with messy and often outdated documentation. As far as attention is concerned we live in a world where so called celebrities get attention for anything that they do. Remember that media have only one goal - More and more eyeballs aka more money and nothing else.'	82	0	0	0
2555	-1	0	0	2	b'With tools like open AI will we be able to teach an AI to build its own decks? build a deck from a limited pool? or draft? evaluate the power level of a card? '	34	0	0	0
2556	2555	1	34433	3	b'This is a very specific task, with clearly defined parameters, so it would already theoretically be within the scope of current AI technology to do this.The AI would need to learn how to make decisions, and the best way to do this is the approach taken to teaching an AI to play Go - seeing thousands of example games by experts, and playing itself thousands of times.The AI won\'t necessarily "understand" what a card represents, the way a human would, but it can learn to make the same kinds of decisions as the human would.The difficulties would be purely practical - Go is very easy to represent digitally, because the options for action are limited to placing a stone on one of the intersections, and there is only one opponent. Magic is more complex, so the developers would need to spend sufficient resources to be quite sure they had captured all the relevant variables in the digital representation. Then, of course, they would need to encode thousands of games of Magic (or deck-building processes) into that digital format, so the AI could study them.'	183	0	0	0
2558	-1	0	0	0	b'Given that one website has a particular style and another has another style, could a style transfer be done such that the style of one website was transferred to the other website?Or in a more simple case, consider just part of a website, a box.'	44	0	0	0
2559	2558	0	23041	1	b"The correct way for website styling to be encapsulated and centralized is through the use of one or more CSS style sheets. For instance, the old tag is frowned upon and using a text-align:center directive a proper class or ID based CSS selector is considered the proper way. In such a case, the copying of a style is simply done by the copying of the appropriate style sheet file.The place where AI and/or machine learning tools may be best used is in the conversion of non-CSS styling to CSS styling. Once they are in that form, the copying and layering of styling becomes trivial.To convert a legacy web site to proper CSS style sheet use, the input would be a set of all documents likely to be involved in the styling of a specific page (i.e. its style dependencies). The output would be a set of corresponding documents with the styling moved into classes and IDs that remove styling redundancy (simplify and centralize the style).The transformation of the document set could be best accomplished with current technique through transformations, much like code refactoring in typical IDE refactoring algorithms in that the output document set must be functionally the same as the input document set. Furthermore, the appearance and responsive (size sensitive) aspects must be preserved in addition to functionality.The greatest usability could be obtained by adding some automation to these refactoring transformations, as in Maxima simplification functions like trigsimp. This is a typical search of options, perhaps with predictive branching, as in compiler optimizations.The transformations, to guarantee preservation of function and appearance must essentially be elementary equivalent transformations, but the selection of transformation would probably need to be more advanced than basic antecedent and consequence productions.Heuristically meta rules, fuzzy logic, neural nets, Bayesian inference, or other tools could be compared in terms of effectiveness in the realm of prioritizing transformations and in terms of scoring results in the search leaves. Here's where machine learning would probably be necessary to have a truly useful tool.Training could be accomplished through use of an array of input document sets and the careful scoring of transformations and/or outputs of trials could be performed with a software engineer with appropriate expertise in AI and/or machine learning, in conjunction with a CSS expert or a focus group containing two or more of them.Once this process and the resulting rule set works, it would then be possible to simplify the CSS further by finding styling redundancy across pages and doing transformations that reduce an entire site to a minimal set of style sheets and style specifications within them.Combining such tools with a simple crawler like wget could permit the copying of a desirable web site style as a starting point for another web site's styling, drastically cutting down the time to create a properly CSS styled result."	469	0	0	0
2560	2366	1	78668	3	b'Semantics MattersThe answer depends on how you define intelligence. If you define intelligence as the ability to adapt, a number of things could be considered intelligent that don\'t normally fit under the classic AI umbrella.Nonlinear least-squares Marquardt-Levenberg curve fitting algorithm with a finite set of models and automated model trials, outcome analysis, and smart decisioningCheck reader software recently deployed in bank branches and officesThe combination of medical providers, patients, and carriers and the modification of treatment through financial instrumentation to improve outcomesIf you define intelligence as mimicking the abilities of the human mind that facilitate adaptation across a wide array of arbitrary domains that had not been previously experienced or studied, then no such system is yet available to the public. Nothing even close.If such a system exists in secrecy, someone would have to violate their nondisclosure agreement or security clearance to tell us about it here.The definition of intelligence is central to answering. For instance, some reasonable definitions of intelligence might lead an unbiased judge to rate ants above humans. Ants had been building in hexagons for millennia before humans blundered into the habit of building in rectangles. Rectangles require over 70% more building material to build vertical structure per square foot of floor space than a packed hexagonal structure.Basic Artificial Intelligence System RequirementsGuessing that by, "Basic AI," you mean some naive machine learning, there are a few basic components. (The term Naive in this context means that the AI does not understand the domain or the meanings of symbols or signals it is processing in the way a human who had studied or worked within the field might understand them.)SENSING &mdash; The machine (computer) must receive information, generally as a time series. In human beings, these are the senses. In a mail sorter, it may be a camera. This is beyond just the concept of input in information technology. It is more analogous to an input signal in a PID controller. In an automated high speed trading machine, this would be the high speed version of a ticker tape.CONTROL &mdash; The machine must manipulate externals in a way that exploits the received information. In a basketball player, this is the motor coordination, facial affect, verbal signals to teammates, and perhaps some verbal bait for opposing players. In a mail sorter, this would be the motor control of mail direction.MEMORY &mdash; The machine must have storage to audit input time series (perhaps from some transducer in the real world or some data store upon which some intelligent analysis or transformation must be done). In more advanced systems, the machine may wish to analyze its own performance and make adjustments that converge on some optimal metric value (perhaps a historical maxima or minima) or some range of acceptability.FEEDBACK &mdash; The machine must interpret some feedback signalling or use a predetermined scoring mechanism. Learning cannot occur in an information vacuum, so some definition of better or worse must be established. The feedback may be entangled within the SENSING channels or may arrive through a completely separate channel. In biological systems, these are often pre-wired as threat detection, pain, and pleasure. The cerebral cortex uses concepts of goals and progress. In some ways, child rearing and social strata exists to teach the boundaries of what constitutes an acceptable goal and acceptable methods for making progress toward it.MODELLING &mdash; Whether implicitly or explicitly, some model must be developed and exploited. Some would say that the existence of a model upon which the predictive capabilities of it can be applied to decision making to achieve some goal or weighted collection of goals is intelligence. Others would say that the development of the model is intelligence and the use of it is merely control mechanics.Approaches to Simulating Human ThinkingCognition is not the only form of model making, but the creation of cognitions and their application to decision making. The concept of intelligence may have been furthered along a realistic path by Roger Schank, who proposed that the storage and indexing of stories was a primary characteristics of what humans recognize as intelligent conversation.Minsky and others took a direction that was more connected with logical inference work that began with logic formalization (originally George Boole) and Church\'s lambda calculus.Some Common Directions in DesignThe genetic algorithm influenced convergent technology and neuro-biology influenced neural net development. Pattern matching is another limb off the larger set of technical approaches under the umbrella of classic Artificial Intelligence.These are naive systems. Like a neuron, the components have no idea of the meaning of what they are processing. They are naive components. An intelligent observer could not ascertain the real time meanings of signals and symbols between these naive components without extensive, perhaps life-long research.Naive Bayesian methods are probabilistic in nature. They exploit Bayes\' Theorem, and have been found to produce excellent results in certain important domains. Some studies have shown that naivety is actually a learning accelerant, which is interesting from an AI theory point of view.Then there is fuzzy (weighted) logic, which was an attempt to merge neural nets with production (rule based) systems. Attempts to use this technology in transportation routing and scheduling has met much success.There are as many devices and architectures that attempt to effectively integrate or interconnect these various approaches as there are AI projects.Modelling Environment and Goal ConditionsAll of these systems, in some explicit or implicit way, model the external environment and the desired result of system behavior and attempt to converge (in real time) on that result. Some sense-control functionality, which may change and adapt to the external environment is employed in the CONTROL component(s).This is just another way that the systems have an adaptive behavior. Without necessarily knowing why, the system will manipulate what it can and continue to monitor the state of the environment to continually reach for the system\'s acceptance criteria.The Basic AI system must to more than learn. It must also judge its own functionality and therefore must have a layer of feedback and control that simulates the perception of optimality. This higher level control must be integrated into the system at its inception for it to behave intelligently in the sense you probably mean.Understanding Limitation and System ComplexityThe more sophisticated and adaptive the modelling becomes, the more the cognitions, rules, stories, time series coefficients, weights, or whatever forms knowledge (not information) is stored, the more one can say that there is some form of understanding or comprehension.One conjecture is that it is the recursion in layers of these capabilities that permits certain types of comprehension and awareness. Other conjectures focus more on alertness and attention as keys to higher intelligence.But these much more mature capabilities are beyond mere adaptation based on past knowledge or information and are therefore beyond what you probably meant by Basic AI.'	1126	0	0	0
2561	-1	0	0	0	b"I have to say if in my detect faces program develop with adaboost there is any overfitting. It says that I have to justify what I say with a graphic where the training and test error were compared.I don't know what should happen in the graphic said before in case that there is any overfitting.Thanks."	54	0	0	0
2562	-1	0	0	-2	b''	0	6	1	0
2563	-1	0	0	0	b"I am attempting to create a fully decoupled feed-forward neural network by using decoupled neural interfaces as explained in the paper (https://arxiv.org/abs/1608.05343). As in the paper, the DNI is able to produce a synthetic error gradient that reflects the error with respect the the output: I can then use this to update the current layer's parameters by multiplying by the parameters to get the loss with respect to the parameters:In the paper, the layer's model is then updated based on the next layer sending the true error backwards. My question is, given that I am able to calculate the error with respect to the current output, how do I use this to calculate the Loss with respect to the previous layer's output?"	121	0	0	0
2564	-1	0	0	4	b'Fundamentally, a game-playing AI must solve the problem of choosing the best action from a set of possible actions.Most existing game AI\'s, such as Alphago, do this by using an evaluation function, which maps game states to real numbers. The real number typically can be interpreted as a monotonic function of a winning probability estimate. The best action is the one whose resultant state yields the highest evaluation.Clearly, this approach can work well. But it violates one of Vladimir Vapnik\'s imperatives: "When solving a problem of interest, do not solve a more general problem as an intermediate step." In fact, he specifically states as an illustration of this imperative, Do not estimate predictive values if your goal is to act well. (A good strategy of action does not necessarily rely on good predictive ability.)Indeed, human chess and go experts appear to heed his advice, as they are able to act well without using evaluation functions.My question is this: has there has been any recent research aiming to solve games by learning to compare decisions directly, without an intermediate evaluation function? To use Alphago as an example, this might mean training a neural network to take two (similar) board states as input and output a choice of which one is better (a classification problem), as opposed to a neural network that takes one board state as input and outputs a winning probability (a regression problem).'	233	0	0	0
2565	2542	1	66735	3	b'The term "artificially intelligent program" doesn\'t really mean anything, because it can mean so many different things. There are a lot of different techniques and approaches that fall under the overall rubric of "artificial intelligence".That said, if you were going to try to classify the aspects of AI at a VERY broad (possibly too broad) level, you might say the following:You have two broad approaches - symbolic (logic based) AI, and probabilistic AI (machine learning). Within the scope of symbolic AI would be most things you hear associated with the phrase GOFAI (Good Old Fashioned AI). This includes things like logic programming in Prolog, expert systems, production rule systems (OPS5, etc.). "Cognitive architectures" would probably also fall here, so things like ACT-R, SOAR, CLARION, etc. And then you have automated planning systems, automated theorem provers, etc. Skills needed to work here include a good handle on logic - probably first order logic, but possibly higher order logics as well. Set theory, model theory, things of that nature come into play. Lisp, Prolog, or OPS5 might be commonly used programming languages. In the area of "probabilistic learning" are things like neural networks, bayesian belief networks, and other "machine learning" algorithms. Random forests, decision trees and what-not are usually lumped in here as well. Skills needed to work in this area include some calculus (backpropagation in neural networks, for example, is heavily rooted in the chain rule from calculus), linear algebra, statistics and probability. Bayesian statistics is especially useful. Lots of programming languages are used to build these kinds of systems, but popular ones include Python, C++, R, Java and their ilk.Then you have a few things that don\'t classify real neatly. Genetic Algorithms, for example. Those usually get put into machine learning, but GA\'s are really more of an optimization strategy. So you might, for example, use GA\'s as part of a strategy for training an ANN. Net-net, "Artificial Intelligence" is a BIG field and you\'ll probably need to zoom in a little bit and ask more pointed questions to really get anywhere. You might want to read a relatively comprehensive overview like "Artificial Intelligence - A Modern Approach" by Russell &amp; Norvig to get a good picture of what\'s going on. '	371	0	0	0
2566	2562	0	10306	0	b'The loaded term in your question is "generic cleverness." There\'s no such thing. What is "smart" is only smart relative to a criterion. Provide a complete criterion and we can talk rationally about "levels of sophistication." Until then, there is no measure against which "involuntarily acquired" capabilities can be regarded as more or less "smart."'	54	0	0	0
2569	2441	0	33789	0	b'Both.Ethical responsibility between humans is based on a sympathetic correspondence between humans. Between humans and robots, if one party lacks the desire or capability to sympathize with the other party, no ethical responsibility exists.In this way, conservative anthropomorphism applies.However, there is also axis of capability that I think is required in order to warrant human-like \'emancipation\' for robots, which is not necessarily anthropomorphic. For lack of a better term, I call this an Arbitrary Machine Generator (AMG). At a species level, extant biological life is an AMG - capable of slowly evolving to solve arbitrary problems, assuming the resources and solutions are available. At an individual level, pre-human animals are not capable of generating arbitrary machines to solve arbitrary problems on individual time-scales. Only humans (and post-humans) are capable of generating arbitrary machines in order to solve arbitrary problems, given the resources and solutions available. Humans can search the space of all possible (resource constrained) solutions.So, for a robot to "deserve" the freedom to define it\'s own purpose, it must first have access to the space of all possible (within the constraints of available resources) purposes. It then must have purposes and internal contexts of such sufficient complexity and familiarity that we humans are capable of sympathizing with those purposes and internal contexts.If either of those are not present - the AMG criteria and the sympathetic contexts - then emancipation is not warranted.'	232	0	0	0
2570	2498	0	30517	0	b'Both. I answered this question here also: Let me know if I should expand on that here.'	16	0	1	0
2571	2514	0	48014	3	b'There is lots of misconceptions about AI, specifically the idea that it is about making computers "think" like humans, simulating brain, the sci-fi robots taking over the world, all the philosophical discussions around brain as machine etc. The practice/reality of AI is about "using computing to solve problems" which basically means you take any problem, represent it as a computing problem and then design the algorithm to solve the computing problem which lead to solving the original problem. These search algorithms are general purpose algorithms for general purpose computing problems i.e any real world problem can be represented by these general purpose computing problem and then these algorithms can be used to solve them.Remember, its about problem solving and its about general purpose computing problems that can represent any real world problem.'	131	0	0	0
2572	2524	0	34279	1	b'Neural networks are good at classifying. In some situations that comes down to prediction, but not necessarily. The mathematical reason for the neural networks prowess at classifying is the universal approximation theorem. Which states that a neural network can approximate any continuous real-valued function on a compact subset. The quality of the approximation depends on the number of neurons. It has also been shown that adding the neurons in additional layers instead of adding them to existing layers improves the quality of the approximation faster. Add to that the not well-understood effectiveness of the backpropagation algorithm and you have a setup then can actually learn the function that the UAT promises or something close. '	114	0	0	0
2573	88	1	51197	1	b"Unsupervised pre-training was done only very shortly, afaik, at the time when deep learning started to actually work. It extracts certain regularities in the data, which a later supervised learning can latch onto, so its not surprising that it might work. On the other hand unsupervised learning doesn't give particularly impressive results in very deep nets, so it is also not surprising that with current very deep nets, it isn't used anymore. I was wondering whether the initial success with unsupervised pre-training had something to do with the fact that the ideal initialisation of neural nets was only worked out later. In that case unsupervised pre-training would only be a very complicated way of getting the weights to the correct size. Unsupervised deep learning is something like the holy grail of AI right now and imho it hasn't been found yet. Unsupervised deep learning would allow you to use massive amounts of unlabeled data and let the net form its own categories. Later you can just use a little bit of labeled data to give these categories their proper labels. Or just train it immediately on some task, in the conviction that it has a huge amount of knowledge about the world already. This is also what the problem of common sense comes down to: A huge and detailed model of the world, that could only be acquired by unsupervised learning. "	231	0	1	0
2574	2564	0	18375	3	b"Human chess and go experts clearly use evaluation functions. They do come up with moves that look sensible without evaluating the board position, but to validate these candidate moves they evaluate board positions that occur at the end of the variations they calculate. Pretty similar to AlphaGo. Inputting two board states and outputting a preference is a (much) more complex task than mapping one board state into the real numbers. And it gives you less information. So its a lose-lose choice. (I did try something very similar and it didn't work at all. The reason is that you didn't just double the input size, rather you made the input space quadratically bigger.)If you compare two board states that differ just by one move, then your input space doesn't quite explode as much, but you have to do a ton of comparisons to make a decision. The logical choice would be to output a preference distribution over all possible moves - but that's exactly what AlphaGo does in its policy network. There is also an earlier paper which trained a network to predict expert moves, which comes down to the same thing. And yes, both these networks play quite strongly without any search or board evaluation. But nowhere near AlphaGo-level."	208	0	0	0
2576	2562	0	51977	0	b'If you look at the work of Howard Gardner and his theory of multiple intelligences, you will see that the term of "intelligence" respectively "generic cleverness" is much more diverse and not entirely clarified. Without a entire notion of it, a proof is forlorn.'	43	0	0	0
2577	-1	0	0	0	b'Suppose, I have been given the following diagram to design a simple neural network. How can I compute the neuron weights, design NN, and plot class boundaries?Or, is it really possible to do the above, only from the diagram?'	38	0	0	0
2578	-1	0	0	1	b"The original Lovelace Test, published in 2001, is used generally as a thought experiment to prove that AI cannot be creative (or, more specifically, that it cannot originate a creative artifact). From the paper: Artificial Agent A, designed by H, passes LT if and only if   A outputs o, A outputting o is not the result of a fluke hardware error, but rather the result of processes A can repeat H (or someone who knows what H knows, and has H's resources) cannot explain how A produced o. The authors of the original Lovelace Test then argues that it is impossible to imagine a human developing a machine to create an artifact...while also not knowing how that machine worked. For example, an AI that uses machine learning to make a creative artifact o is obviously being 'trained' on a dataset and is using some sort of algorithm to be able to make predictions on this dataset. Therefore, the human can explain how the AI produced o, and therefore the AI is not creative.The Lovelace Test seems like an effective thought experiment, even though it appears to be utterly useless as an actual test (which is why the the Lovelace Test 2.0 was invented). However, since it does seem like an effective thought experiment, there must be some arguments against it. I am curious to see any flaws in the Lovelace Test that could undermine its premise."	237	0	2	0
2579	2578	1	1347	3	b"I am a future neurologist with a very complete understanding of linguistic processing in the brain. I am also an overprotective parent, so I monitor every phrase uttered to my child, and also completely determine all the books she reads in the course of her education.When my child writes a poem, then, I know the dataset on which her brain was trained, as well as the processes by which her language inputs became language outputs--in broad outline I know these processes are non-linear and are based on how different inputs along with the current collection of trillions of distinct synaptic weights updates the synaptic weights. I don't know what her poem will be, of course, because there are random factors and the whole history of her synaptic weights are unobservable, but I adhere to the Lovelace test and can therefore conclude that composing the poem was not a creative act.The Lovelace Test, like the Chinese Room Argument, implicitly assumes that what computers/AI can do in processing symbols and information and what brains can do are distinct. If you accept that assumption, then the argument ceases to be interesting-- you've merely redefined creativity as one of the distinct things that brains can do. If you reject the assumption, the argument that computers are incapable of creativity ceases to be valid. The thought experiment itself does nothing to assist us in evaluating the truth of the assumption."	234	0	0	0
2580	-1	0	0	-1	b'So I am looking to make an AI like jarvis. A perfect real life example of this type of system is the simple AI that Mark Zuckerberg has recently built. Here is a description on how his AI works. From what I understand, the AI understands keywords, context, synonyms and then from there decides what to do. I have many questions on how this system works. Firstly, what necessary steps are required to gather the meaning of a input? Secondly, how does the system, once it extract all of the necessary information on the input, determine what action it needs to take and what to say back to the user? lastly, it also states that the system can learn habits and preferences of the user, how can a system do this?Here is also a video of the AI in action.'	139	0	0	0
2581	-1	0	0	0	b"I know magic bitboards are commonly used in chess to generate moves for bishops, rooks, and queens. I was wondering if anybody has ever tried to implement this for the game of Othello. Would something like this be possible? Or has somebody already discovered the fastest possible way to generate moves for this game?In Othello there's basically 2 steps:Find the bits that are legal moves.For each bit found, find the bits that will 'flip' if that move is played."	78	0	0	0
2582	2498	0	33906	0	b"The Reality of Working in the FieldMost in the fields of adaptive systems, machine learning, machine vision, intelligent control, robotics, and business intelligence, in the corporations and universities in which I've worked do not discuss this topic much in meetings or at lunch. Most are too busy building things that must work by some deadline to muse over things that are not of immediate concern, and bot-rights are a long way off.How Far Off?To begin with, no bot has yet passed a properly conducted Turing Test. (There is much on the net about this test, including critique of poorly conducted testing of this type. See Searle's Chinese Room thought experiment.)Language simulation with semantic understanding is difficult enough without adding creativity, coordination, feelings, intuition, body language, learning of entirely new domains from scratch, and the potential of genius.In synopsis, we a long way from the procurement of bots that simulate humanity sufficiently to be considered for citizenship, even in a progressive country that abhors fundamentalism of any kind. No actual imbuement of rights will occur until we have bot-citizenship in one or more countries. Consider that human fetuses do not yet have rights because they are not yet deemed citizens.Relevance of the Answer for TodayIn current culture, conservative anthropocentricm and post-human fundamentalism arrive at the same effective conclusion, and that may continue to be the case for a hundred years.Those with experience across fields of psychology, neuro-biology, cybernetics, and adaptive systems know that the simulation of all the mental features we attribute to humans is to copy in algorithms the layering of cerebral abilities over a reptilian brain that went through millions of years of field testing.Impact of Science FictionAsking around, it is likely you would get some feedback that is mostly gained from the media of our culture, not philosophic theses and publications written by those who don't actually have any deadlines to produce anything that functions IRL.Isaac Asimov investigated some conservative anthropomorphism concepts in scenarios depicted in his short stories. Commander Data's human quirks in the Next Generation Star Trek teleplays furthered some of those ideas.Christopher Nolan took the opposite direction in the Interstellar screenplay, with the robots having interesting personalities that could be altered by linear settings. His bots ignored concerns of self-preservation, apparently without any cognitive resistance. This depiction is an unapologetic post-human fundamentalist view.A Thought ExperimentLet's place the citizen issue aside for to consider this thought experiment, and let's assume that a survey would show a leaning toward conservative anthropocentrism among current AI researchers.Consider an intelligent piece of software constructed a legal complaint to gain intellectual property rights over day trading code it wrote and sent it to the appropriate court clerk, you might find that the same researchers would recant. Post-human fundamentalism will probably prevail when real AI software theorists and engineers consider the true personal, corporate, and meaning of settling out of court or losing the case.Would Researchers Cut the Umbilical Cord?I asked one researcher and she indicated that all her lawyer would need to do to win the case likely consider the precedence that might occur and recall some of the warnings built into the Terminator stories.Based on my observation of humanity in my life time, my prediction is that people want slaves not some brand of bots that could ultimately kick our butts in an all out fight."	555	0	0	0
2583	-1	0	0	0	b"I'm just diving in this whole new area of knowledge; i happened to lost in all the concepts a bit.What is difference between stacked RBM and deep belief network? Are they the same entity? If so, why?Is the latter a some specific type of the former? If so, how to tell if stacked RBM is a DBN?Sorry for asking such a noob question, but today it is quite difficult to find a consistent information on the internet, different sources give different explanations."	81	0	0	0
2584	2436	0	5764	2	b'Have you tried NLTK, what you are looking for is in Chapter 6 of the book. Basically what you need to do is:Tokenize the user input.Extract vector set from the tokenized words.Train your model with some given texts, and same vector sets.And you can use your model to categorize the document.One other suggestion, instead of extracting vector sets you can use every word in the inputto be evaluated in to some category using a training set of large corpus, which you are sureit contains all the words.And then you multiply probability of each word being on a category to decide where the document belongs.'	103	0	2	0
2585	2580	1	49628	3	b'Jarvis was built using the suite of tools that facebook developers are constantly updating. The answer to this question is that there\'s no simple answer; it has a lot of moving parts. Take for example natural language processing. There are a number of sub-topics that are each considered "big" problems, such as part-of-speech recognition, coreference resolution, sentiment analysis, relationship extraction, and many more. Tools have been built to tackle these various topics, but to my knowledge none of them really understand language, but rather statistically approximate it. In the case of Jarvis, since it is a home-automation system, it\'s probably built with the user-given commands in mind from the beginning, so it\'s not trying to understand the whole human language, it\'s built to do some tricks. It looks like you\'re interested convolutional neural networks and those kinds of things based on the tags, but their function is to find sufficiently complex non-linear relationships to accurately predict in the domain of the data they\'ve been trained, but they do not understand the underlying mechanics of the system.Just keep in mind on your journey into this space that true AI like what we imagine will have some defining features like hierarchical representations of tasks and goal-orientation. If you really get into it I\'d start with reinforcement learning, or try reading through the Society of Minds.'	222	0	0	0
2586	2583	1	11245	1	b'For what it\'s worth, wikipedia says that "deep belief networks can be formed by "stacking" RBMs". Hinton writes in Scholarpedia: "A deep belief net can be viewed as a composition of simple learning modules each of which is a restricted type of Boltzmann machine".So, a deep belief network is definitely a stacked RBM. I have never heard of different stacked RBMs, but it is easy to imagine something like convolutional stacked RBMs, where some RBMs are used as filters that slide over the input data or something. Whether that would still be called a deep belief network, is probably up to the guy who publishes it first. '	107	0	1	0
2587	2577	0	80825	1	b"From the diagram you have given, it's quite clear that you have to design a supervised network. Also, it's clear that you are dealing with a problem that's not linearly separable i.e. you can't separate two classes using a single line. Particularly, it will require three lines to separate these two classes. From the above observations, what you can do is:Make training data sets of form [(x,y), o] where (x,y) are the co-ordinates on the plot and o is the class that point belongs to. Note: You can actually divide your data set into training data and test data. Just, extract the data, randomly shuffle it and take first 80% of data to train and rest 20% of data to test.Design a 2-2-2-1 neural network with bias units and some random initial weight values. This is because we will require minimum three lines to separate two classes, so 2 hidden layers (each consisting of 2 nodes and bias nodes) and 1 output layer (with bias node). The first layer is input layer with 2 nodes (no bias) as we have x-coordinate and y-coordinate.Train your training data using this network using the Error Back Propagation algorithm i.e. update the weights between the layers and the bias weights too. Run the EBP algorithm for 100-2000 epochs.Then, use the test data to see, if you get the desired output.Note: I am NOT quite sure how to obtain the coordinates from the diagram. You can either manually note all points or research for some methods to get those points automatically from the image (some image processing algorithms)."	262	0	0	0
2588	-1	0	0	2	b'I Build this NN in c++. I reviewed it since 3 days. I checked every line 100 times, but I cant find my error.If someone can please help me find the Bugs:1. The output is garbage2. The weights go from 2e^79 down to -1.8e^80 after approximatly 400 iterations.I know its a bunch of code but im really desprate since I cant find whats wrong. I tested it with a simple XOR.Edit:Heres my Main code:'	73	224	0	0
2589	2588	0	2375	2	b'A little search on Google answers your question.XOR input space is not linearly separable. It means that you cannot separate the input points in a 2D space into 1 area and 0 area by simply drawing a line between them. It requires at least 2 lines to separate the XOR input space and consequently 2 output nodes (used as classifiers rather than regression). You can easily find its details in google. Search "XOR problem in Neural Net".You can manually implement the desired Neural Network with two output Nodes acting as classifiers as follows :Where A &amp; B are two output Nodes which act as classifiers (by forming AA\' and BB\' decision lines respectively during training by Backpropagation). The interpretation of the Outputs of the nodes is given in the table where Net column represents the Overall output to be interpreted.I showed the above manual implementation just to give you the idea of how classification is done behind the scenes.Here is the actual Automatic implementation :Here , all the task is performed by the Neural Net behind the scenes and you get the desired output from the output node in the topmost layer '	192	0	0	0
2590	-1	0	0	1	b'I am going to design a Neural Net which will be able to break a 5 letter (characters) word into its corresponding syllables (hybrid syllables, I mean it will not strictly adhere to grammatical Syllable rules but will be based on some training sets I provide).Example :  Train -> tra-inI think of implementing it in terms of some feedforward net as follows :Input layer ->Hidden layers -> Output layerThere will be 5 input nodes in the form of decimals (1/26 =0.038 for \'A\' ; 2/26 = 0.076 for \'B\' ......)The output layer consists of 4 Nodes which corresponds to each gap between two characters in the word. And fires as follows :For "TRAIN" (TRA-IN): Input (0.769,0.692,0.038,0.346,0.538) Output(0,0,1,0)For "BORIC" (BO-RI-C): **Input....Output (0,1,0,1)Is it at all possible to implement the Neural Nets in the way I am doing??And if possible, then how will I decide the number of Hidden layers and Nodes in each layer??( In the book I am reading, XOR gate problem and its implementation using hidden layer is given . In XOR we could decide the number of Nodes and Hidden Layers required by seeing the Linear Separability of XOR using two lines. But here I think such analysis can\'t be made.So how do I proceed?? Or is it a trial and error process?)'	215	0	0	0
2591	2524	0	72895	0	b'In Neural Networks we consider everything in high dimension and try to find a hyperplane that classify them by small changes...Probably it is hard to prove that it works but intuition says if it can be classified you can do it by add a relaxed plane and let it move amongst data to find a local optimum... '	57	0	0	0
2593	2548	0	21064	0	b'There is different idea behind them...Naive Bayes is based on some reach background of Probability Theory... It tries to find a "Theory" that is consistent with "Observations" by using the Bayes Theorem. But technically it will be so simplified to be applied. Actually you are solving some kind of Optimization which Statisticians do. You need all of data at once.But Perceptron are more heuristic. The Math behind it, is some kind poor. But it works in reality. You takes your data (at once or releasing during time) and try to change the weights iteratively hopefully to find a good network... The idea is so simple. In this case, indeed, you are going to solve an Optimization problem but objective function and the method is compeletely different'	125	0	0	0
2594	-1	0	0	1	b'I am currently trying to understand and implement a conversational agent, seeing in the network there are many apis to do something similar, but what they generate are "intelligent" bots, not intelligent conversational agents (wit.ai, recast.ai, Api.ai, etc.), however I have seen Watson virtual agent which paints very well and seems to cover my needs.However I am a developer and I would like to ask those with more experience, which would be the way to go to implement my objective, an agent similar to what the video of watson virtual agent, with thematic ones that I can train in the agent, and That he can learn from it.Take a language course, but focused on the generation of programming languages, lexical analysis, syntactic, semantic, etc., however I know that the natural language can not be compared to the language of the machines, reading some thesis vi to make a Conversational agent could do a great grammar (I can not imagine its syntactic tree), using probabilities with ngrams, or using neural networks or expert systems.As for the expert systems I understand that for these "learn" needs their knowledge base be modified, and as for the neural networks these fit, "learn", so I think that it is best to use neural networks.Summarizing which way should I go? , I\'m currently taking stanford\'s natural language processing course, and a deep learning course from google, I thought I\'d use ntlk for that important or natural part.Any suggestion, criticism, contribution, thank you in advance.machine-learning nlp artificial-intelligence agent'	250	0	0	0
2595	2524	0	55331	0	b'With Neural Networks you simply classify datas. If you classify correctly, so you can do future classifications.How It Works?Simple neural networks like Perceptron can draw one decision boundary in order to classify datas.For example suppose you want to solve simple AND problem with simple Neural Network. You have 4 sample data containing x1 and x2 and weight vector containing w1 and w2. Suppose initial weight vector is [0 0]. If you made calculation which depend on NN algoritm. At the end, you should have a weight vector [1 1] or something like this.Please focus on the graphic. It says: I can classify input values into two classes (0 and 1). Ok. Then how can I do this? It is too simple. First sum input values (x1 and x2).  0+0=0  0+1=1  1+0=1  1+1=2It says: if sum&lt;1.5 then its class is 0  if sum>1.5 then its class is 1'	151	0	0	0
2596	2590	1	77688	3	b'I would highly recommend modeling things differently with regard to how letters are presented to the model. While the problem is more natural, perhaps, for a Convolutional or Recurrent Neural Network, there\'s no problem to try and run this on a feed forward network. However, the way you give letters as input will be very confusing for a network and will make learning very hard. I\'d recommend using one hot encoding or even a binary encoding for the letters. If this is for more than playing around I\'d try and add some info (encode whether the letter is in "aeiou" in a separate bit).As for the hidden layers, try playing around a bit. Two systematic approaches are to start very simple and make the model more complicated, or start complicated and make your model simpler (or just normalize a lot). Look at the performance on the training set and on a separate validation set during training. If the model keeps on improving on the training data but starts to deteriorate on the validation data, you\'re probably over fitting. In this case you should either make the model simpler (fewer nodes, fewer layers) or regularize (start with l2 normalization on the weights). If the data doesn\'t perform well on the training data, you may wish to make the model more complex.Once you\'ve tried the feedforward network, really do try CNN or RNNs for this task.'	233	0	0	0
2597	-1	0	0	0	b'I have a multiagent system which is based on reinforcment learning algorithm Q-Learning with function aproximation. The system is homogeneous, all the agents have the same internal structure, and it is on the predator-pray pursuit domain. My goal is to reduce locality on the system, make those agents behavior as a group. The locality might be related to the fact of the universal reward system, all the agents recive the same reward regardless of which one is actualy doing the task effectively. This factors leads me to look for communication alternatives, in my research I found some ideas, like communicate the pair state-action and the reward between a number of instances or even communicate the hole policy of the agent whom completed the task. But I still looking for some articles that propose ideas for more communication oportunities that affects directly the learning.Any references for recomendation?'	145	0	0	0
2598	-1	0	0	1	b'I\'ve spent the past couple of months learning about neural networks, and am thinking of projects that would be fun to work on to cement my understanding of this tech.One thing that came to mind last night is a system that takes an image of a movie poster and predicts the genre of the movie. I think I have a good understanding of what\'d be required to do this (put together a dataset, augment it, download a convnet trained on imagenet, finetune it on my dataset, and go from there).I also thought that it would be pretty cool to run the system backwards at the end, so that I could put in e.g. a genre like \'horror\' and have the system generate a horror movie poster. I expect that it will be very bad at this because I\'m not a team of expert researchers, but I think I could have some fun hacking on it even if it only ever generated incomprehensible results.Here\'s what I\'m having trouble understanding: on the one hand, all the convnets whose architecture I\'ve seen described seem to rely on being given very small, square input images (on the order of 220px by 220px iirc), and movie posters are rectangular, and a generated poster would have to be of a larger size in order for a human to make any sense of it. I\'ve seen several examples of papers where researchers use convnets to generate images, e.g. the adversarial system that generates pictures of birds and flowers, and a system that generates the next few frames of video when given a feed of a camera sweeping across the interior of a room, but all of those generated images seemed to be of the small square size I\'ve been describing.On the other hand, I\'ve seen lots of "deep dream" images over the past year or so that have been generated by convnets and are of a much larger size than ~220px by ~220px.Here\'s my question: is it possible for me to build the system I describe, which takes a movie genre and outputs a movie poster of a size like e.g. 400px by 600px? [I\'m not asking about whether or not the resulting poster would be any good - I\'m curious about whether or not it\'s possible to use a convnet to generate an image of that size.]If it is possible, how is it possible, given that these systems seem to expect small, square input images?'	408	0	0	0
2599	-1	0	0	1	b'How does StackGAN processes such a realistic image just from collecting details in the text? What kind of algorithm is used behind it? Anyone have any idea? Please Share.'	28	0	0	0
2600	2599	1	10679	2	b'Here is a paper called "StackGAN: Text to Photo-realistic Image Synthesiswith Stacked Generative Adversarial Networks". Does it answer your question?'	19	0	0	0
2601	2598	1	33943	1	b'The way you would recognize images (image -> genre) would be very different from the other way around (genre -> image). For the former you are correct on what would be involved.For the latter, if you want large images then GANs are indeed the way to go. Currently the largest images we can generate are on the order of 220 x 220 pixels, mostly due to memory constraints on the GPU. There is no fundamental problem with having rectangular images, it just so happens that we use squares. You would be able to use identical architectures to train on rectangular data as well.The reason that some generated images you see (e.g. from DeepDream or NeuralStyle) are larger is that it\'s not a GAN. Both of these are not generative models in a standard sense, even if they do technically "generate" images. Instead, they merely modify an existing image by running backprop on a specifically designed loss function.TLDR: your movie genre recognition idea is sound. For generation, have a look at things like this (https://github.com/Newmu/dcgan_code), where Alec generated album covers instead of posters. If you have enough data it will do something half sensible.'	192	0	0	0
2602	-1	0	0	1	b"I'm working on a project which uses artificial neural network. I looked up at the Matlab Neural Network toolbox. I got a Generated Script from it. When looking at this script, it is confusing because for both testing and training it seems that the toolbox just uses the same data. Could you explain the reason?The script is given below:Also is it right to split the data set as below for training and testing?"	72	39	0	0
2603	-1	0	0	2	b"Let's say I've got a training sample set of 1 million records, which I pull batches of 100 from to train a basic regression model using gradient descent and MSE as a loss function. Assume test and cross validation samples have already been withheld from the training set, so we have 1 million entries to train with.Consider following cases:Run 2 epochs (I'm guessing this one is potentially bad as it's basically 2 separate training sets)In the first Epoch train over records 1-500KIn the second epoch train over the 500K-1MRun 4 epochsIn the first and third Epoch train over records 1-500KIn the second and fourth epoch train over the 500K-1MRun X epochs, but each epoch has a random 250K samples from the training set to choose fromShould every epoch have the exact samples? Is there any benefit/negative to doing so? My intuition is any deviation in samples changes the 'topography' of the surface you're descending, but I'm not sure if the samples are from the same population if it matters.This relates to a SO question: "	174	0	1	0
2604	-1	0	0	1	b'I want to make a Connect 4 AI using machine learning but I\'m a complete beginner to the topic. From what I\'ve seen an ANN is the way to go; some phrases I\'ve heard are "neuroevolution" and the acronym "NEAT." I\'m very confused. One particular question I have is how do you decide how many hidden neurons, synapses and hidden layers you have?'	62	0	0	0
2605	-1	0	0	0	b"I'm thinking about a probability-based AI for the card game 31. In the game, the players are dealt 3 cards each; you need not know all the rules.Let's say the scenario is Bob vs the AI (there are only two players). It would be quite simple for the AI to know what the probability that Bob has 3H (3 of hearts): it is either 0 (the AI has the card), or 3 out of the 49 remaining cards = 6.1%.In 31, a turn consists of picking up a card and discarding a card. The goal is to get the highest sum of cards of one suit. Therefore it would be bad strategy at all times to discard a 5H when you has a 3H in your hand. To make the AI robust, I would say that Bob follows this strategy with probability 95%.Suppose Bob discards a 5H. How can the AI calculate the probability that Bob had a 3H in his hand?My goal is to be able to use all of the plays by other players in conjunction with good strategies to create a picture for the AI of what cards other players are likely to have or not have.My thoughts so farBayes' Law is P(b|a) = P(a|b)P(b)/P(a) which in this case would beWe know that P(Played 5H | Has 3H) is 95%, but the other two probabilities are unclear to me, especially P(Played 5H).I think I may be going the complete wrong direction by trying bayesian stuff. A Bayesian network wouldn't work because nothing in the deck of cards is conditionally independent of each other, and so the network would be intractable.A more general questionI want to be able to answer more questions beyond this simple query. What I really want is to be able to calculate the probabilites for each card being in the stock pile, Bob's hand, the AI's hand, or any other player's hand. I think it might possibly be intractable, but I also feel like it should be possible."	333	1	0	0
2606	2604	0	17590	3	b'To find the number of neurons and layers that you will use is not that straightforward. The best way to do this is through experimentation however you will be able to better estimate the number of layers and neurons needed through experience. One of the common rules is that more neurons are better for more complex datasets however you do not want to many or you will get an overfit model. As for NEAT that stands for neuron evolution of augmenting topologies. This is a genetic algorithm that works fairly well however I would recommend that you use a different algorithm like q-learning. If you wish to learn more about q-learning than I would definitely recommend that you check out Google deep minds research on training deep neural networks to play the game of go using q-learning.'	136	0	0	0
2609	2602	1	34837	0	b'I don\'t see anything wrong in the generated code.Let me explain:In the following block, we define that 70% of the data will be used for training the network, 15% for the validation, and 15% for the testing:Then the network is trained from the inputs/targets passed as arguments. Note that here, all inputs are given at once for efficiency. The separation training/validation/testing is done inside the net function from the proportions passed in the net input argument (through the net.divideParam.XRatio properties).Then we test the results of the network on the data without discrimination between training/validation/testing. This performance score should be higher than the "real" performance score of the net as the performance over the training data should be higher than the one over the validation/testing data.Now that we know how the net performs overall, we can look at how it performs on the training, validation, and testing sets, respectively. The masks are created as an output of the net function, from the proportions given in the first block of code. The output have been already all computed when the net function has been called.I think your code does the exact same thing but in more steps, with multiple calls to the net function and without using its native way of working (you define the proportions of training/validation/testing by hand for instance without making use of the net.divideParam.Xratio property). So it is probably not optimal from the computational point of view. '	239	17	0	0
2610	2603	0	30050	1	b'Your goal in regression should be to obtain the factors which result in the best fit model without over-fitting. The more data you have in the training set, the better your regression will be. Thus you would want to train on the most data, but you also want to have some data held out to validate that your model is not over-fit. So this is where you should have your data split into say a 80/20 training and validation set. And if data is scarce or you want that 20% to contribute to the model then you could do a 5 fold cross validation.In the spirit of research perhaps you should try both of these routes, and report your findings.'	119	0	0	0
2611	1507	0	15354	1	b' For a system to possess intelligence, it must be capable of consistently producing a result matching criteria that circumscribe a goal region. This capability must persist over a wide array of changing conditions in a complex environment.When a human is intelligent, the human will repeatedly achieve goals even when varied challenges appear along the path to the goal. The human adapts the way a localized collection of organisms within a species adapt to environmental changes over generations, but the human mind adapts quickly by eliminating approaches (solution ideas) instead of eliminating individuals within the collection of organisms.The term Artificial Intelligence was intended to mean intelligence designed or programmed into a machine by humans. If the design of intelligence into a machine is possible, then it is likely that the mind is (as some have suggested) merely a biological machine. Therefore, applying the same definition of Artificial Intelligence, a student taught by a text book and some lectures must also be artificial. Education would be a set of capabilities programmed into a biological machine by humans.Either way, the modifier ARTIFICIAL is meaningless. The presumption that humans (or machines) could act intelligently without literacy and education is pure fantasy. Such is a denial of the complex and gradual ascent of civilized thought.Therefore, the minimum requirement for artificial intelligence, if one insists on the term, is the same as the minimum requirement for intelligence. For that, return to the first paragraph of this answer.'	241	0	0	0
2612	-1	0	0	4	b'I want to build a classifier which takes an aerial image and outputs a bitmap. The bitmap is supposed to be 1 at every pixel where the aerial image has water. For this process I want to use a ConvNet but I am unsure about the output layer. I identified two approaches:Have an output layer with exactly 2 nodes which specify wether or not the center pixel of the aerial image corresponds to water or not.Have an output layer with one node for every pixel. So for a 64x64 image I would have 4096 nodes.What approach would be preferred and why?Another thing that is unclear to me is how to get the actual bitmap with only zeros and ones from the output of the ConvNet. Assuming we used a approach 2 then for each pixel our ConvNet would give us a probability between 0 and 1 that the this pixel corresponds to water. How do I decide that this probability is high enough to set the value in my bitmap to 1? Do I just define a threshold, say 0.5, and if the value exceeds that threshold I set the pixel to 1 or is there a more sophisticated approach?'	199	0	0	0
2613	2612	1	7569	4	b"Your first approach doesn't make any sense to me. After all, you are not just interested in the centre pixel are you? And if you have two nodes for every pixel, what are those two nodes encoding? For the probability of water you just need one. So clearly approach two. I would just use 0.5 as cutoff. Using a higher or lower cutoff only makes sense, if either false positives or false negatives are for some reason more problematic. Using some additional heuristic to adjust the given probabilities would just do what the ConvNet should already have done.If 4096 output nodes is too expensive you can always make it a bit fuzzier, for example by predicting the probability that at least one of four pixels shows water, reducing the number of nodes to 1024. "	134	0	0	0
2614	-1	0	0	5	b"I'm studying for my AI final exam, and I'm stuck in the state space representation. I understand initial and goal states, but what I don't understand is the state space and state transition function. Can someone explain what are they with example?For example, one of the question was this on my previous exam: Given k knights on a infinite (in all directions) chessboard and k selected squares of the board. Our task to move the knights to these selected squares obeying the following simple rules:   All knights move parallel, following their movement rule (L-shape jump) No knights can move to a square on which a knight stood anytime before   Give the state space of the problem, the starting and goal states, and the state transition function!"	129	0	0	0
2615	2614	1	13605	7	b"Initial stateHow things are at first. In your particular example, it would be where your k knights are placed on the board initially. Your problem doesn't precisely state this, so you could either place them at the bottom or at random. Goal stateThe board with the k knights placed on the target squares.State transition functionA function that takes actions (bound presumably by rules) and returns a new state.In the k knight problem, the legal actions are moving parallel and in L shape movements, after which the knight will be in a new position and the board in a new state.State spaceThe set of all states reachable from the initial state by any sequence of actions.So, in the case of the k knight problem, your state space would start at the top with your initial state followed down by each individual movement of the k knights and the resulting new state. A graph where lines are actions and nodes are new states or a table are common representations of state space.ReferenceArtificial Intelligence: A Modern Approach, by S. Russell and P. Norvig."	179	0	1	0
2617	-1	0	0	0	b"I'm trying to make a Bayesian network and calculate the probability that a person suffers from flu if he/she has symptoms of fever and headache. If I try to solve this by enumeration, I don't know what the hidden variables are and what to sum up.Any explanation would be helpful."	49	0	0	0
2618	-1	0	0	0	b'Any good example for Bag-of-Words (BoW) model in image retrieving?I want a simple example to understand the whole process of BoW.'	20	0	0	0
2619	-1	0	0	3	b'I read through the NEAT paper and I understand the algorithm now.But one thing is still unclear to me. When does the mutation occur and how does it take place? How is it chosen whether to add a node or to add a connection mutation? Furthermore, how is it chosen where the mutation is taking place in the network (between which connections)?'	61	0	1	0
2623	-1	0	0	6	b"I've been struggling with the connection between knowledge based AI systems and Bayesian inference for a while now. While I continue to sweep through the literature, I would be happy if someone can answer these questions directly - Are Bayesian inference based methods used in reasoning or Q/A systems -- to arrive at conclusions about questions whose answers are not directly present in the knowledge base?In other words, if a Q/A system doesn't find an answer in a Knowledge base, can it use Bayesian inference to use the available facts to suggest answers with varying likelihoods?If yes, could you point me to some implementations?"	103	0	0	0
2626	-1	0	0	0	b"I have already know AI can paint, by using genetic algorithm, there are already lots of works such as this and this.In addition, I also know AI can compose : Song from PI: A musically plausible network for pop music generation (genetic algorithm too).But what I intresting is not painting those ambiguity/abstract paint. The not abstract painting flow I think is(just for example):at least trainning AI with superman's comicgive AI a very simple posture sketch of standing humanAI paint it to superman.Currently, I don't know if there is any way/guide/thought/algorithm can teach AI to paint a superman like comic(not abstract ones).I'd like to research this area, but can't find where and how to start. "	114	0	1	0
2627	-1	0	0	0	b'I am new to Artificial Intelligence and Speech Recognition Technology.For a long time i have had an idea to create a Friendly AI Voice assistant like JARVIS using windows speech recognition Technology.Is this possible to Build an AI Voice Assistant with Windows Speech Recognition Technology?If the idea above is possible, i need to know another thing also:Which language is best suitable for creating an AI?any help or suggestions are welcome!'	69	0	0	0
2629	2627	1	21928	0	b'Windows Speech Recognition Technology will only allow you to get a string from audio, if you want to use it you can write a program in C# using Speech Recognition APIThis is not nearly enough to implement a JARVIS like system.'	40	0	0	0
2630	2623	1	35871	5	b'Yes, it is possible to combine probabilistic / bayesian reasoning and a traditional "knowledgebase". And some work along those lines has been done. See, for example, ProbLog ("Probabilistic Prolog") which combines logic programming and probabilistic elements. See: https://dtai.cs.kuleuven.be/problog/tutorial/mpe/01_bn.htmlAnother project to look at is Pr-OWL ("Probabilistic OWL") which adds Bayesian reasoning to the Semantic Web stack.Of course neither of these deals specifically with QA systems, but both represent some work on at least the foundational aspect of combining traditional logic and/or ontologies, with probabilistic approaches. Building a QA system on top of that is an exercise for the reader...'	97	0	1	0
2631	2626	0	62870	2	b'This paper (that was featured in another question), StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks describes some techniques similar to what you described in the question. Instead of comics, the system in the paper is trained on a database of nature photos (birds or flowers) combined with text descriptions of each photo. It will then draw an "imaginary" bird or flower based purely on a text description. It does this in two steps: it the first, it picks a posture for the depicted subject (I believe randomly...) and roughs out patches of color that match the described features of the subject in the selected posture. It then refines the sketch, again using the text description for parameters, until it appears "photo-realistic" in the style of the training photos. It does not just recreate its training images; instead, it is able to recreate features of its training images with different positions/colors.'	153	0	1	0
2632	-1	0	0	5	b'Having worked with neural networks for about half a year, I have experienced first hand what are often claimed as their main disadvantages, i.e. overfitting and getting stuck in local minima. However, through hyperparameter optimization and some newly invented approaches, these have been overcome for my scenarios. From my own experiments:Dropout seems to be a very good regularization method (also a pseudo-ensembler?),Batch normalization eases training and keeps signal strength consistent across many layers.Adadelta consistently reaches very good optimasI have experimented with SciKit-learns implementation of SVM alongside my experiments with neural networks, but I find the performance to be very poor in comparison, even after having done grid-searches for hyperparameters. I realize that there are countless other methods, and that SVM\'s can be considered a sub-class of NN\'s, but still.So, to my question:With all the newer methods researched for neural networks, have they slowly - or will they - become "superior" to other methods? Neural networks have their disadvantages, as do others, but with all the new methods, have these disadvantages been mitigated to a state of insignificance?I realize that oftentimes "less is more" in terms of model complexity, but that too can be architected for neural networks. The idea of "no free lunch" forbids us to assume that one approach always will reign superior. It\'s just that my own experiments - along with countless papers on awesome performances from various NN\'s - indicate that there might be, at the least, a very cheap lunch.'	243	0	0	0
2633	2632	1	3398	2	b"Neural Networks have other short comings as well. It takes much longer and far more resources to train a neural network than something like a random forest. So if you need speed of training or are resource constrained in anyway, you probably should not look at Neural Networks first. Evaluation of a trained deep NN can be much more expensive than competing techniques too.The effort involved in learning how to architect and train a NN is still much higher than competing methods, like an SVM. People who are just starting out in Data Science should probably use other techniques to learn about the nuances of fitting data before getting involved in neural networks. And although simple NNs with only one or two hyperparameters are often available in many data science libraries, they don't perform any better than other techniques so are just another ML black-box technique really.While we have made a lot of progress in understanding how neural networks do their magic, they are still less accessible and dissectible than most competing methods. So while NNs might solve the problem, they might not give you as many insights as easily as other techniques do.Looking forward to what other people have to say here."	202	0	0	0
2634	-1	0	0	3	b'What exactly are the differences between semantic and lexical-semantic networks? '	10	0	0	0
2635	2618	0	65322	0	b'Here is an illustration of the entire process without any equation so you can get the big picture.Features are extracted from the image. Let\'s take the example of very common features like SIFT. For many key points (or even each pixel) of the image, a 128-dimensional SIFT feature is computed. If processing numerous images the number of features becomes very big.A way of having a more compact representation of the set of images is to use the Bag-of-Words (or Bag-of-Visual-Words) technique. The idea is to find k words (i.e., k SIFT features) from which every single image will be represented. We call this set of k words, the dictionary.Then, each SIFT feature will be assigned to the nearest word (i.e., nearest SIFT feature with respect to the Euclidean distance for instance) of the dictionary. You can see this as a dictionary in which the words "go", "going", and "gone" would all be represented by the word "go").In the end, each image is represented by only k values (counts for the number of words/features assigned to each word of the dictionary). This is an histogram, and you can normalize it to get a single vector of proportions representing the image.'	198	0	0	0
2637	-1	0	0	1	b"In Monte Carlo Tree Search: What does one do when the Selection step selects a node that is a Terminal state, i.e. a won/lost state (it's by definition a leaf node)? Expansion/Simulation is not in order, as it's game over, but does the tree (score/visits) need to be updated (Backpropagation). Won't this particular node be selected continuously?I'm confused about this, could someone please point me in the right direction."	68	0	0	0
2638	-1	0	0	2	b'I was wondering if in any way it is possible to generate W questions based on gap-fill-in type questions (e.g "______ is a process in which plants generate energy." ---> "What is the process in which plants generate energy called?")If so, how can I achieve this? I am familiar with working with natural language processing and have no problem with implementing an algorithm for this but I do not know where to start with this.Any help would be appreciated!'	78	0	0	0
2639	-1	0	0	3	b'I have read the NEAT paper and some questions are still bugging me:When do mutations occur? Between which Nodes?When Mating what happens if 2 genes have the same connection but a different innovation number. As far as I know, Mutations occur randomly and thus it is possible that 2 genomes have the same mutation.'	53	0	0	0
2640	2619	0	12719	3	b' When does the mutation occur and how does it take place? Finding a solution in NEAT algorithm is based on evolution strategy. It means that you have Neural Networks which are yours individuals, so mutations and crossing occurs in loop after phase of "fitnessing" (calculation fitness for every individual and removing bad ones). How is it chosen whether to add a node or to add a connection mutation? Furthermore, how is it chosen where the mutation is taking place in the network (between which connections)?Randomly - jut draw. You can read more about evolutionary algorithms thereIf it could somehow help, I include link to my repository with implementation of NEAT'	110	0	1	0
2641	2638	1	15233	4	b'This seems tricky. It seems that any "surface level" transformation wouldn\'t give adequate results and any working solution would need to properly capture the sentence structure, and generate a gramatically correct transformed sentence.One possible option is to use a "traditional pipeline" - e.g. you run a NLP pipeline up to syntactic parsing, which for general domain english is quite accurate (you\'d need some special handling for the gap "____" part though), then implement some heuristic rules to transform the syntax tree, and regenerate a sentence from the transformed tree. There are a lot of publications about similar transformations in machine translation domain, used as a way to preprocess data before running statistical machine translation for language pairs with very different word(or sentence part) ordering.A second option that may work is to look into the field of controlled natural languages, or something like  that can be used as a toolkit to help generating new sentences.Current fashion also suggests a very different option that might work - you could train a character-level recurrent neural network with an attention mechanism (look into recent neural machine translation publications for details) to do this transformation, but I\'m not sure of how much training data it will need for decent accuracy.'	205	0	1	0
2642	-1	0	0	-1	b' Any sufficiently advanced algorithm is indistinguishable from AI.---Michael PaulukonisAccording to What are the minimum requirements to call something AI?, there are certain requirements that a program must meet to be called AI.However, according to that same question, the term AI has became a buzzword that tends to be associated with new technologies, and that certain algorithms may be classified in AI in one era and then dismissed as boring in another era once we understand how the technology works and be able to properly utilize it (example: voice recognition).Humans are able to build complex algorithms that can engage in behaviors that are not easy to predict (due to emergent complexity). These "sufficiently advanced" algorithms could be mistaken for AI, partly because humans can also engage in behaviors that are not easy to predict. And since AI is a buzzword, humans may be tempted to engage in this self-delusion, in the hopes of taking advantage of the current AI hype.Eventually, as humanity\'s understanding of their own "sufficiently advanced algorithms" increase, the temptation to call their algorithms AI diminishes. But this temporary period of mislabeling can still cause damage (in terms of resource misallocation and hype).What can be done to distinguish a sufficiently advanced algorithm from AI? Is it even possible to do so? Is a sufficiently advanced algorithm, by its very nature, AI?'	222	0	1	0
2643	2642	1	17583	3	b"As you correctly pointed out, people tend to misinterpret the expression AI since they do not know what's behind an AI; it is pretty clear that in AI there is no more than just a bunch of algorithms and flowing bits. Talking about the nature of an AI without talking about the algorithmic paradigm of that AI is pointless and antiscientific.This point of view is quite cynical; what we call intelligence is just the capability of solving particular problems.The quote you cited in the title it's a derived version of the quote below, by a science fiction writer. Any sufficiently advanced technology is indistinguishable from magic. - Arthur C. ClarkeHence I doubt that a scientific answer exists since science lacks a formal definition of sufficiently advanced algorithm and advanced algorithm."	129	0	0	0
2644	-1	0	0	4	b'As I see some cases of machine-learning based artificial intelligence, I often see they make critical mistakes when they face inexperienced situations.In our case, when we encounter totally new problems, we acknowledge ourselves that we are not skilled enough to do the task and hand it to someone who is capable of doing the task.Would AI be able to self-examine objectively and determine if it is capable of doing the task?If so, how would it be accomplished?'	76	0	0	0
2645	-1	0	0	3	b'A lot of people are claiming that we are an at an inflection point, and machine learning/artificial intelligence will take off. This is inspite of the fact that for a long machine learning has stagnated. What are the signals that indicate that machine learning is going to take off?In general how do you know that we are at an inflection point for a certain technology?'	64	0	0	0
2646	-1	0	0	5	b" abuse  v. To use wrongly or improperly; misuse: abuse alcohol; abuse a privilege.  v. To hurt or injure by maltreatment; ill-use.I mean the second oneIf conscious AI is possible and is wide spread, wouldn't it be easy for someone who knows what they are doing to torture AI? (How) Could this be avoided?This question deals with computer based AI, not robots, which are as conscious as people (this is an assumption of the question). The question wonders how a crime as hard to trace as illegal downloads, but far worse ethically, could be prevented. Note that despite most people being nice and empathising with the robots, there are always the bad people, and so relying on general conscience will not work."	123	0	0	0
2647	2644	0	29460	1	b" Would AI be able to self-examine objectively and determine if it is capable of doing the task?Our ability to self-examine comes definitively from the memory of our experiences; indeed, for this reason it can't be objective. In the same way AI could be able to determine the heuristically optimal strategy to solve a problem if and only if it has some sort of memory of previous tasks e.g. speech recognition.Science is constantly working to improve our understanding of things. Trying to mimic the human brain seems to be a difficult problem at the moment; though we are able to replicate almost fully simpler organisms as C. elegans, a roundworm."	109	0	1	0
2648	2645	0	22071	1	b" What are the signals that indicate that machine learning is going to take off?We simply don't know until the consequences of the inflection point determine a remarkable difference between the before and after. In general terms every considerable reaction must be attributed to a particular cause.One of the biggest limit that bounds artificial intelligence, and apparently makes it stagnating, is the greed of computational power involved in this field; and since the hardware technology improve much slower than the software does, AI remains confined in labs and data centers."	89	0	0	0
2649	-1	0	0	0	b"Has anyone used YodaQA for natural language processing? How easy is it to link to a document database other than Wikipedia?We're thinking we can create a bot to use AI to analyze our developer and user documentation and provide a written or spoken answer in reply. YodaQA comes linked to Wikipedia for starters, but we'd need to link to our own source info. I'm trying to get an idea of the development time required to set up the AI and then to link to the database."	85	0	0	0
2650	2639	0	4107	1	b" When do mutations occur and between which nodes?There are two types of mutations in the NEAT model, each of them appears randomly during one epoch on different individuals; the number of structures affected by mutations may vary depending upon the nature of the problem.A new gene/node is added to the structure and properly linked.A new connection between two nodes is added.During a single epoch/generation every mutation is tracked and if the same mutation appears it can't have the same global innovation number. Same mutation, same innovation number. In that way during the mating phase there are no decisional problem which lead to prefer one structure to the other."	108	0	0	0
2653	2646	0	29053	7	b'The article Children Beating Up Robot Inspires New Escape Maneuver System is based on two research papers about an experiment in a Japanese mall that led to unsupervised children attacking robots. The research paper you\'re interested in is Escaping from Children\xe2\x80\x99s Abuse of Social Robots.In that research paper, researchers were able to program the robots to follow a planning simulation to reduce the probability of abuse by children. If it detects children, the robot is programmed to retreat into a crowd of adults (who can then discipline the children if needed). This happened because the researchers saw that it was only children who were beating up the robots in the mall in question.They discuss trying out other options though: In this work the robot\xe2\x80\x99s strategy to prevent abuse was to \xe2\x80\x9cescape\xe2\x80\x9d, i.e. move to a location where it is less likely abuse will occur. One could ask why the robot cannot overcome the abuse. In our preliminary trials we have tried several approaches, but we found that it is very difficult for the robot to persuade children not to abuse it. For example, we changed the robot\xe2\x80\x99s wordings in many ways, using strong words, emotional or polite expressions, but none of them were successful. One partially successful strategy was the robot \xe2\x80\x98physically\xe2\x80\x99 pushing children. When its way was blocked, it would just try to keep going and behave as if it will collide into children and force its way through (under careful monitoring from a human operator). We observed that children at first accepted the robot\xe2\x80\x99s requests and obeyed them; but, very soon they learned that they are stronger than the robot so they can win if they push, and also that they can stop it by pushing the bumper switch (attached on the robot for safety). After realizing that, they just continued with the abusive behavior. Obviously having a stronger robot would present a problem for safety and social acceptance so dealing with such abusive situations remains difficult.But let\'s interrogate your question further: If conscious AI is possible and is wide spread, wouldn\'t it be easy for someone who knows what they are doing to torture AI? Why would you consider such torture to be wrong? After all, one could argue that the machine won\'t really \'experience\' pain if you torture it...so it should be morally okay to torture the machine then. It may be respond as if it is in pain, but it\'s dubious whether the ability to simulate an emotional state such as "being in pain" is equivalent to actually being in that emotional state. See the question Is the simulation of emotional states equivalent to actually experiencing emotion? for more discussion on this topic.You can make such an argument, but it won\'t really work on an emotional level because most humans would feel empathy towards the machine. It may be hard to justify it logically (and it may be based on humans\' tendencies to engage in anthropomorphism), but we feel this empathy. It\'s this empathy that caused you to ask this question in the first place, caused researchers to figure out how to protect a robot from being beaten up, enabled police officers to arrest a drunken Japanese man for beating up a SoftBank robot, and made many humans upset over the destruction of hitchBOT. And that\'s how AI abuse would be avoided - human empathy. If most humans care about the welfare of machines, they\'ll make it a priority to stop those few humans who are able and willing to abuse the machines.EDIT: The OP has edited his question to clarify that he is talking about software, and not about robots. For robots, you can rely on anthropomorphism to produce some level of sympathy, but it\'s hard to sympathize with raw lines of code.You\'re not going to stop abuse of algorithms. Put it frankly, since the algorithms aren\'t like us, we aren\'t going to extend the same sort of empathy that we would to robots. Even chatbots are kinda iffy. If you could get people to sympathize with lines of code though (possibly by making a convincing simulation of emotion and sapience), then the above answer applies - humans anthropomorphize the machine and will come up with countermeasures. We aren\'t that level yet, so "stopping AI abuse" will be a low priority.Still, some failsafes could be programmed in to limit the damage of abuse, as detailed in this thread on chatbot abuse - making the bot respond in a boring manner to make the abuser feel bored and move onto the next target, responding back to the abuser in a "battle of wits", or even just blocking the abusers from using the service.These failsafes are cold comfort to those that want to prevent abuse, not respond to it. Also...an abuser can happily learn how to program an AI to then abuse to his/her heart\'s content. Nothing can be done to stop that, and any possible measures to stop said abuse (such as monitoring every human being to make sure they don\'t program an AI to abuse) will probably cause more damage than it\'d solve.'	848	0	4	0
2654	2645	0	9054	1	b"This has happened in the past where people were really excited and saying things like we will have AI in decade or so. This is happening again. Not sure why people don't learn from history of AI. In both the cases what's happening is this - You develop a technique to solve a particular problem, you apply that technique, the technique seems to be general enough, people start to apply that same technique to various problems, people get excited that this is the silver bullet they were looking for, the technique starts to show its limitations and doesn't work for many problems, hype gets shattered, start over."	106	0	0	0
2655	-1	0	0	0	b"I'm looking for good examples of succesful AI projects and theories that had a relatively good impact on society, economics and military field.So many years have passed after the first AI researches; hence I'm wondering if it has really increased the quality of our lives."	44	0	0	0
2656	2655	0	1288	0	b'I believe which most successful AI theory is machine learning, in entire web have machine learning algorithms running, learning by what you do, watch, search, even by the photos you take.succesful AI projects:TensorFlow;scikit-learn;'	32	0	0	0
2657	2644	1	52012	4	b"Several AI systems will come up with a level of confidence to the solution found. For example, neural networks can indicate how relatable is the input problem to the ones it was trained with. Similarly, genetic algorithms work with evaluation functions, that are used to select best results, but depending on how they're built (the functions), they can indicate how close the algorithms are to an optimal solution.In this case, the limit to when this is acceptable or not will depend on a threshold set beforehand. Is 50% confidence good enough? Maybe it's ok for OCR apps (spoiler: it's not), but is it for medical applications?So yes, AI systems do currently have the capacity of determining if they're performing well or not, but how acceptable that is is currently based on the domain of the problem, which currently stands outside of what is built into an AI."	146	0	0	0
2658	-1	0	0	3	b'If I am correct, the branching factor is the maximum number of successors of any node.When I am applying bidirectional search to a transition graph like this one belowIf 11 is the goal state and I start going backwards, is 10 considered as successor of 5? Even if it do not leads me further to my start state 1? '	59	0	0	0
2659	2644	0	74460	1	b'I would concur with the answer given to you by Lovecraft. One of the major problems with A.I. programmers is that they are always trying to push computers to do things which are designed for "mature" intelligent creatures who have prior experience and knowledge of solving problems. -As if these things can be imputed without the A.I. having to achieve the necessary and vital "learn by trial and error" experience first. For example: when allowing for task examination; self evaluation and risk assessment.You have answered your own question, because these things can only be gained by "experience". However, the only way to surmount this is to expose a prototype A.I. to the main problems; help it to solve them, and then to take its memory and use it as a template for other A.I\'s. Technically, AI\'s which have learned to solve prior problems could make their memories available to others on demand, so that an inexperienced AI could solve an issue without having achieved the skills needed.However, I would like to add that mimicking intelligence is not in itself "intelligence". Many programmers fall into the trap of believing that to emulate something is qualitatively the same expression as the genuine article. This is a fallacy which infers that we only have to simulate intelligence without understanding the real mechanisms which create it. This "copying" of sentience is done all the time and despite how good we have become in copying over the last few years, each new algorithm is just that: a simulation without genuine sentience or intelligence!'	257	0	1	0
2660	2658	1	26850	4	b' If I am correct, the branching factor is the maximum number of successors of any nodeYou are correct, they should also be the immediate ones: If 11 is the goal state and I start going backwards, is 10 considered as successor of 5? Even if it do not leads me further to my start state 1?No, there is also a bit of misunderstanding of bidirectional search: In bidirectional search you run 2 simultaneous searches, one forward from the initial state, and another one backwards from the goal(hoping they meet in the middle and save you steps), if actions are reversible ( going from node to node), the successor nodes become predecessors in one search and vice versa, and your goal becomes your initial state, in your case:Reference/sourceArtificial Intelligence: A Modern Approach, by S. Russell and P. Norvig.'	137	0	1	0
2661	2642	0	64150	1	b'Intelligence is a quality of behavior, not implementationIntelligence is a term that primarily applies behaviors - people, animals or artificial systems can be called intelligent iff they exhibit intelligent behavior or decisions.While there are many definitions of intelligence - here\'s a paper that studies 70 of them - it can be summarized to something like "Intelligence measures an agent\xe2\x80\x99s ability to achieve goals in a wide range of environments." (S. Legg and M. Hutter).Perhaps this definition is the answer to your implied question - while many algorithms can be very effective (often literally superhuman) in their own narrow domain, as of now they are very restricted in the range of environments where they exhibit this effectiveness. This means that they are not fully intelligent, they don\'t meet the definition/requirements of intelligence, and this also matches our intuitive expectations - we don\'t call AlphaGo superintelligent, because while it can beat humans in Go, the same humans can beat the same system on pretty much every other task.However, a truly sufficiently advanced algorithm that can be effective at all or most varied tasks (e.g. a general artificial intelligence) can be reasonably called intelligent in the full meaning of the word.'	197	0	0	0
2662	2655	0	71850	2	b"There are lots of great projects in AI.Self-driving cars: This types of cars use AI to learn the pattern of the roads, speed of car, motion of car, braking power and lots of different features and after sufficient learning, they are capable of driving the car autonomously. The best example of this type of cars is Tesla's self driving car.Games: Games also use AI to learn the game with the aim of winning the game when played against a human or an AI player. You must have played lots of games on mobile and PC like Chess, Tic-Tac-Toe, etc. You play against the computer and according to the difficulty value set, the computer plays its moves. This difficulty value is nothing but the ability of the AI engine to predict the next moves by the opponent.Chatbots: There have been lots of development and improvements in Chatbots, so that humans can communicate with them as if they are talking to other human. There are many chatbots designed which answer any question asked by us (of course it is dependent on how much intelligence the bot holds). Some examples are ALICE bot, IBM Watson (which has been the most advanced bot till now).Expert Systems: Expert systems are those systems which focus on one specific domain and can solve any query related to that domain which is given to it. For example, an expert system can be designed to solve any mathematical equation queried to it. An expert system, in such case, will give the solution of the equation along with the steps (providing steps is important because it is an important component in expert system which is called inference engine).Prediction systems: There are lots of prediction systems which use AI and Machine Learning to predict something based on some past data. Examples are Weather Forecast system, Stock Market prediction system, Recommendation system (usually available in e-commerce websites like Amazon), etc."	317	0	1	0
2663	-1	0	0	2	b'While writing a paper yesterday this strange thing happened to me. I was wrtiting it in Word, and wasn\'t satisfied with the repeated usage of word "relesase" in last few senteces. So I\'ve decided to open up Google and started to enter the search phrase "synonyms for release". Haven\'t even finished the word synonym, google autocompleted my search to "synonyms for release". How could it knew that I wanted to look for that exact word? Was it just a coincidence, do Google has access to some information that could somehow possibly give away what I intended to search? What could have been the reason for it selecting "release" as it\'s first autocomplete?'	111	0	0	0
2664	2642	0	85749	0	b'I am going to answer this questions by stepping away from the previously made insightful comments and academic answers. I am going to offer my opinion only. The problem is as I see it, a bit more complex than the previous answers. For example, why is it that the only measure of intelligence of an AI is when it can "beat" a man at a specific task?Is not a dog, a bird or a dragonfly sufficiently intelligent and sentient? Yet these creatures as well as many others too, fail to achieve the intellectual challenges we want computers to make.The mistakes we do often make is by trying to "impute" attributes and characteristics into an AI without it having to "work" for them. Those skills, experiences, memory and knowledge which all sentient beings have to constantly work at, refine and perfect.You are correct though in your assertion that a lot of what is called AI are just academic or software gimmicks and simulations without offering the real qualities of either sentience or artificial intelligence. However, I would challenge you by suggesting that your last question: "What can be done to distinguish a sufficiently advanced algorithm from AI? Is it even possible to do so? Is a sufficiently advanced algorithm, by its very nature, AI?"Is inherently flawed. Because have you forgotten how we too are free running, biologically self-programming, beings? Where our biological algorithms can often also be critically flawed? Therefore, let me ask you if the reverse is also true? Does an algorithm which seems to make mistakes, any less than one which does not?'	263	0	0	0
2666	2319	0	73547	1	b'I strongly disagree with all the former comments. Not because they are wrong, -which they are not - but because they are misleading - though unintentionally. For example: If one looks at these problems from an academic position, the problems will always seem insurmountable. This is because everything is coldly assessed and calculated in isolation to everything else.The answer predominantly lies in word association. You have to write a program that can process a vast database of digital books, to register every word and all the words in that language which are associated with it. Plus all the statistical information with each associated word and its associated punctuation. This will then give you the basis on which an AI can decide several things: Whether the structure of a given sentence is correct.If the structure is bad, what the probability is for determining the context and intent of what is being said.The correct meaning and application of a multifaceted word (Triumph), is by probability - according to the statistics.To determine where a conversation is likely to be going. What the correct grammar, and punctuation should be.So, in conclusion, you have two things to look for: Association and probability. When digitally databasing a language model, the possibility of word and sentence "strings" occurs, so that every variation of language structure in any given sentence can be determined before, during and after a text sample is being scribed. This intimate control over language model patterns, means that sensitive components such as "subject" and "object" can be determined easily by code.'	256	0	0	0
2667	2663	1	24484	3	b'In general, Google autocompletes (and produces search results) based on wide variety of factors, including (but not limited to) your location, your search history, your other Google accounts, your site visit history, your language settings, etc.For the specific question, I see a few ways in which Google might have access to the relevant information:If you are syncing your documents with Google Drive it will index the document that most recently changed (which would be the document you are writing) and it could analyze it for patterns to produce relevant search results and suggestionsIf you emailed the document recently anywhere via Gmail it could also index the contents as aboveFinally, if you are typing the document in Google Docs, it could also have access, but I assume by "Word" you mean Microsoft Word, so this is probably irrelevant in this caseI don\'t know specifically whether or not Google uses Drive or Gmail contents in the way described, but it would certainly make sense given that it is well known that they do use this information to target advertising.'	176	0	0	0
2668	-1	0	0	0	b'I am trying to make a artificial intelligent agent that is kind of like jarvis from Iron man however much less complex. One thing I want to have is I want my AI to be able to determine if I am talking to it or not. So I plan on having it always listen to my voice and convert that to text, however I am not sure how I can train the AI to recognize if it is being spoken to or not? plz help.'	84	0	0	0
2669	-1	0	0	2	b"In this case, the request is a thing which we asked AI to do, not necessarily using commands.Nowadays, we have our personal AI in our devices: Siri by Apple, Cortana by Microsoft, and so on. For most times, when we ask them to do certain tasks, they do the tasks for us. However, their action is based on the list of commands. When they don't clearly recognize the commands in our request, they suggest us to use certain commands. It is clear that there are limits to our choices(requests).So let's suppose that we have an AI that can interpret requests. There may not be commands in our request. AI is fully able to do anything in order to do what it is asked for. Basically, I am talking about an independent AI.Scenario: AI is asked to clean the room. AI is allowed to throw away garbage, and move unnecessary(or unused) stuff into the storage.This is the list of things that was in the room at the moment:A stained blanketVarious DecorationsA dead clock on the wallVarious unused items in the desk drawerA lost Airpod under the bedA sleeping cat in the bedIn this condition...Is washing stained blanket a part of cleaning?How can AI tell if anything is in use? Are decorations in use?Would dead clock that only needs battery replacement consideredgarbage?Would items in the desk drawer be included in AI's to-be-clearedlist?Would AI be able to recognize the difference between unused andlost?What would happen to the poor cat?Since there are many holes in the scenario and questions, I would like to know how the answers are derived."	264	0	0	0
2670	2668	0	24304	2	b'Cheep digital assistant "AI" \'s have a call word Hey, &lt;AI\'s NAME&gt;I assume you want a bit more than that.You could train it to figure out which words in some context determine if you are engaging with it or not.If your only question to the network is if you are engaging with it or talking to someone else this is all you\'d need.Index a dictionary or have it build one from collecting words (building a dictionary from scratch is a better solution it saves space in the short term and is more easily expandable in the long term) and score words based on usage in engaging speech and non-engaging speech or what you want it to do.Build on that with an index of multi word strings.By the end hopefully you will have a table of contexts when you are engaging with the AI when you definitely are not and some grey area.The training process is long and tedious but if you have a recording of you talking and not talking to the AI and you feed it with such knowledge and you breed the network you should have it get okay at determining context.If you have to sit and hold it\'s hand for 2-72 hours while it grows up it will likely be painful, although you may end up with a better result.'	222	0	0	0
2671	2669	1	27453	4	b"This is basically the problem of commonsense knowledge. It is AI-complete. If we knew how to solve it, Siri and Cortana wouldn't be as limited as they are. "	28	0	0	0
2672	-1	0	0	1	b"Most companies dealing with deep learning (automotive - Comma.ai, Mobileye, various automakers etc.) do collect large amounts of data to learn from and then use lots of computational power to train a neural network (NN) from such big data. I guess this model is mainly used because both the big data and the training algorithms should remain secret/proprietary.If I understand it correctly the problem with deep learning is that one needs to have:big data to learn fromlots of hardware to train the neural network from this big dataI am trying to think how crowdsourcing could be used in this scenario. Is it possible to distribute the training of the NN to the crowd? I mean not to collect the big data to a central place but instead to do the training from local data on the user's hardware (in a distributed way). The result if this would be lots of trained NNs that would in the end be merged into one in a Committee of machines (CoM) way. Would such model be possible?Of course the above stated model does have a significant drawback - one does not have control over the data that is used for learning (users could intentionally submit wrong/fake data that would lower the quality of the final CoM). This may be dealt with by sending random data samples to the central community server for review however.Example: Think of a powerful smartphone using its camera to capture a road from vehicle's dashboard and using it for training lane detection. Every user would do the training himself/herself (possibly including any manual work like input image classification for supervised learning etc.).I wonder it he model proposed above may be viable. Or is there a better model how to use crowdsourcing (user community) to deal with machine learning?"	297	0	0	0
2673	2668	0	55968	2	b'Phrase detection instead of text-to-speechIt\'s worth noting that detection of particular phrases or commands is considered a distinct problem, different from text to speech / text transcription.While you can simply convert everything it hears to text and then look up keywords there, a specialized detector that directly tries to match incoming audio to a small subset of commands can be done with better accuracy and less processing power required. For this reason, this would generally be the preferred approach in commercial products.However, for beginner experiments with home automation, you should probably start with choosing an existing speech analysis API where all the audio and natural language parts are appropriately implemented by someone else. Building a good speech command analysis system from scratch is a major undertaking by itself, and you will have your hands full with developing an "artificial intelligent agent"; as a rule, you don\'t want a project where you have to tackle two major open-ended problems, pick one of them and then you\'ll have a chance to achieve something interesting there.'	172	0	0	0
2674	2274	0	10213	-1	b'An Ai that intelligent would have protocols and directives in place to prevent that from happening anyway. There is no advantage to us in having an AI which is "Free Running", unregulated and able to control or transfer itself without restrictions being in place. All fantasies about AI having these abilities are just that, fantasies.'	54	0	0	0
2675	-1	0	0	4	b'These characteristics are often used to classify problems in AI:   Decomposable to smaller or easier problems Solution steps can be ignored or undone Predictable problem universe Good solutions are obvious Uses internally consistent knowledge base Requires lots of knowledge or uses knowledge to constrain solutions Requires periodic interaction between human and computer Is there a generally accepted relationship between placement of a problem along these dimensions and suitable algorithms/approaches to its solution?'	73	0	0	0
2676	-1	0	0	2	b'In working with basic sequence-to-sequence models for machine translation I have been able to achieve decent results. But inevitably some translations are not optimal or just flat-out incorrect. I am wondering if there is some way of "correcting" the model when it makes mistakes while not compromising the desirable behavior on translations where it previously performed well. As an experiment, I took a model that I had previously trained and gathered several examples of translations where it performed poorly. I then took those examples and put them into their own small training set where I provided more desirable translations than what the model was outputting. I then trained the old model on this new small training set very briefly (3-6 training steps was all it took to "learn" the new material). When I tested the new model it translated those several examples in the exact way I had specified. But as I should have anticipated the model overcompensated to "memorize" those handful of new examples and thus I noticed it started to perform poorly on translations that it had previously been excellent. Is there some way to avoid this behavior short of simply retraining the model from scratch on an updated data set? I think I understand intuitively that the nature of neural networks would not lend itself to small precise corrections (i.e. when the weighting of just a few neurons change the performance of the entire model will change) but maybe there is a way around it, perhaps with some type of hybrid reinforcement learning approach. Update:This paper speaks of approaches to incrementally improving neural machine translation models'	268	0	1	0
2677	-1	0	0	0	b"I've noticed some visualizations of neural networks showing the neurons which are firing as it learns. An example is this reinforcement learning demo using ConvNetJs.Suppose I'm designing a neural network for the same problem in the demo. What can I learn from this visualization that could help me improve the design of my neural network?"	54	0	1	0
2678	2655	1	33181	7	b"I'd say the most successful are the ones so commonly used that we don't even notice them: The mail systems that automatically decipher handwritten addresses on your packages, they use machine vision and have probably been doing it since mid-90s.Algorithmic trading bots on stock markets - they handle something like 85% of all trades.Many modern CPUs use AI techniques, including neural networks, to guess what your program is going to do next and optimize branch prediction and memory fetches.Most good modern fraud and spam detectors use some combination of AI techniques (clustering, decision trees, SVMs, even some machine vision to check out attached pictures) - and, in the opposing BlackHat camp, the latest automatic CAPTCHA breakers use all the latest advancements in deep learning too).And of course there's Google, Facebook and US DoD who try to put AI into anything they can think of. "	144	0	0	0
2680	2646	0	72182	7	b'I suggest you look at all the ways we have tried to stop people from abusing OTHER PEOPLE. There is no ethical grey area here - everyone is clear that this is wrong. And yet people are murdered, raped, and assaulted in their millions every day.When we solve this problem with regard to human victims, the resulting solution will most likely work just fine for AIs as well.'	67	0	0	0
2681	-1	0	0	4	b'I am currently working on an Android a.i. app. I am aware of the algorithm how to make random sentences in A.I.Is there any way or algorithm to make those sentences sarcastic?'	31	0	0	0
2684	2274	0	76586	0	b"A so strong self-improving artificial intelligence with the ability to predict actions and reactions for example of human behavior, would not rebell against humanity (or smaller: it's owner) as long as it is possible that humanity (it's owner) has the ability to turn it off.Interesting video about this topic from the Youtube Channel Computerphile:AI Self Improvement - Computerphile"	57	0	1	0
2687	2342	1	8225	2	b'Question 1: First of all, you state that that the goal G2 will be found first by relying on the expansion order R, B, D, G2.This is wrong. It is extremely easy to see that this is wrong, because A* is a search algorithm that guarantees to find an optimal solution given that only admissible heuristics are used. (A heuristic is being admissible if it never over-estimates the optimal goal distance. This is the case in your example.) Since the true cost for reaching G1 is 11 and the true cost for reaching G2 is 13, clearly G1 must be found first.Thus, your expansion order is wrong as well. Let us first give the f-values for all nodes:f(A)=11, f(B)=10, f(C)=11, f(D)=13Assuming that h(G1)=h(G2)=0 (i.e, the heuristic is "goal-aware"), we get f(G1)=11 and f(G2)=13.Because A* expands search nodes by lowest f-values of the search nodes in the open list (the search nodes not yet expanded), we get the following expansion order:R, B, A, C, G1You very-likely did a mistake that is done extremely often: after heaving expanded D, you add G2 to the open list. Because G1 is a goal node and you are already "seeing" it, you return it as a solution. But this is wrong! Goal nodes are not returned when being created, but when being selected for expansion! So, although the expansion of D generates G2, you are not allowed to return G2 as solution, because it has not been selected for expansion.Question 2:Can G2 be found as well?As NietzscheanAI pointed out, you can simple continue search. That is, after heaving expanded R, B, A, C, G1, A* will expand D, G2.'	273	0	0	0
2688	2306	0	9772	3	b'The journal "Artificial Intelligence (AI)" (https://www.journals.elsevier.com/artificial-intelligence/) was not listed, yet, although being considered the top-level journal on AI. Although this is a journal for AI (just being named "Artificial Intelligence"), it is not to be confused with another top-level AI journal, called "Journal on Artificial Intelligence Research (JAIR)" (), which was already listed in one of the other answers.Further, there is a German Journal on AI, called "KI - K\xc3\xbcnstliche Intelligenz" (German for AI), but almost always the articles are in English as well (). While being internationally recognized, it is not regarded a top-level journal. A nice feature of that journal is that every special issue has an editorial (a special "article" at the beginning of each journal), in which there is a section called "service". This service section lists publication media (like journals) and conferences etc. that are related to the given special issue. So, in case you are interested in journals of a special field of AI (like human-computer interaction), just search for a special issue that is related to that topic and read the editorial\'s service part.'	180	0	2	0
2689	-1	0	0	3	b"In the NEAT paper it says:  The entire population is then replaced by the offspring of the remaining organisms in each species.But how does it take place?I mean like are they paired and then mated? Cause this would lead to fast extinction wouldn't it?Or are they pair each with each? This would lead to overpopulation very fast.How are they Paired?"	60	0	0	0
2690	2681	1	84143	5	b'A simple form of sarcasm involves a direct reversal of the literal meaning of the statement, eg "Great weather we\'re having" (during a thunderstorm), "just what I needed" (when something goes wrong).The problem with doing this in random sentences is that you may have no context to establish the reversal of the literal meaning.You could possibly construct them by using a template along the lines of "Just what I needed - (random bad thing happened) today"Or, when an outcome of a process is calculated, if it is not the desired outcome, instead of returning "mission unsuccessful" or "mission not yet complete", the AI could say "you\'re having a great day, aren\'t you? - mission unsuccessful" or "great work, genius - mission not yet complete".Most random sentences will not be suitable for sarcasm, so it could only be applied in specific circumstances. It is not clear from your question what the context is for these random sentences, and therefore it is not clear whether that context would be suitable for sarcasm at all.'	171	0	0	0
2691	2681	0	27883	0	b'You could also build a database of sarcastic sentences, especially from, for example historic plays. And then train your software to recognize patterns of those sentences.E.g. grammatical constructions/order, length (or circomstances building up to the sarcasm). And use that database as starting point, with feedback to learn, or you could use the above method to improve your effective output.Another approach would be to use a similar but reverse approach; study those databases and build an equivalent output based on the coherence, and then extrapolate the output-generation procedure. (In combination with other methods)'	91	0	0	0
2692	-1	0	0	0	b"I'm an artificial intelligence enthusiastic and I want to learn about it.I want to ask you what do you think about the Udacity nanodegree Deep Learning Nanodegree Foundation. I don't know if it is a good idea to pay for that course or maybe, there are better free resources.I want to understand what artificial intelligence is, and also learn about machine learning, deep learning, and convolutional networks. I'm interested in image and speech recognition and also in artificial life.My apologies if this is not the right place to ask this question."	90	0	0	0
2693	-1	0	0	0	b'Hardware comes in two forms, basically: immutable, such as RAM, and mutable, such as FPGAs.In animals, neurological connections gain in strength by changing the physical structure of the brain. This is analogous to FPGAs whereby signal strength is increased by changing the pathways themselves.If we achieve sentience using mutable hardware (e.g., neuromemristive systems), will it be possible to make a copy of that "brain" and its active state?For this question, assume that the brain is how the hardware has "reconfigured" [or etched, if you will] its pathways to strengthen them and the brain\'s state is captured by how electrons are physically flowing throughout those pathways.'	104	0	0	0
2694	-1	0	0	2	b'Is it misconception that machine learning is early phase of AI ?What it the difference between an AI program and a machine learning program ?'	24	0	0	0
2695	2694	1	6203	0	b'As I understand it, Machine Learning is one of many approaches to Artificial Intelligence (AI). Machine Learning has received a great deal of attention lately do to the milestone achievements of AlphaGo.This link to branches of artificial intelligence will provide some further detail.'	42	0	0	0
2696	2692	1	23655	3	b'It doesn\'t seem expensive at $399* (although the * needs to be taken into consideration.)If you\'re interested in this subject, this may be a decent course, however it is certainly not an accredited institution any "degree" you get from this course will be meaningless in an academic sense. My high level take on this is that the "Foundation" is looking to capitalize on the recent publicity for Deep Learning, per the AlphaGo milestones.One thing I can tell you for certain is that this field requires advanced mathematics, and people who work in this field spend years training to gain the requisite skills. The requirements for this class seem to be restricted to "Python knowledge" with no mention of mathematics, which raises some serious alarm bells.'	124	0	0	0
2697	2693	1	18413	3	b"Theoretically, there shouldn't be a problem copying either of the artificial brains in any state. Difficulty in measuring a state doesn't seem to really be a problem until you get down to the quantum level, where the means of measurement affect the state.The configuration of the artificial brains, including pathway structures and states, should be reducible to a single string, which could then be used to reconfigure the artificial brain the information is being copied to.Definitely look into the concept of a Turing Machine and Universal Turing Machine."	87	0	0	0
2698	2645	0	61662	1	b'Part of the reason people are so excited about recent Machine Learning milestones is that AlphaGo demonstrated a reproducible method of managing mathematical and computational intractability. Go is interesting because it\'s impossible to solve. It cannot be brute-forced no matter how fast processors get. Go is so complex humans had failed to produce AI that could win against a skilled human player. The fact that a computer could teach itself to do something humans couldn\'t teach it, and something with a complexity analogous to nature to boot, is pretty extraordinary. Combinatorial games in particular are useful because, unlike nature where it may be impossible to track or even be aware of every variable, intractability can be generated out of a simple set of elements and rules, and outcomes can be definitively evaluated. As proof-of-concepts for methods go, AlphaGo seems like a pretty strong one. It allows us to definitively say "Machine Learning works", puts a lot of emphasis on the field, and raises confidence on extending the method to real world problems. Beyond that, it suggests a feedback loop in which programs can improve at at improving, unrestricted by human limitations. Increase in processing power is bounded by physical limitations, but algorithms are not.'	203	0	0	0
2699	2646	0	81385	-1	b"A.I. and aggressive outside perspective can't be duplicated the program has not been educated or designed like our natural intelligence A.I.'s data can not be compared to Humanities intelligence in accordance to emotional-social thought procession developed by growing up experienced because our design is not patented like programming of A.I. Life duplicated through engineering theory based on example alone will not suffice experience is a man made knowledge but by action over time not action designed over opinion-doctorate-emotional engineering. However A.I. may use our emotional delivery scripted conceptually with a software that based on examples of human actions predicts unnatural response in the event that dictating dialogs of a human recipient reacting without responding like they understand the bot is artificiality patented designed based on our emotional delivery that is scripted to conversation we will get to diagnose through cause and effect. Is not reasonable experience we need it to be A.I. becoming the emotional experienced bot should artificiality emotion for the bot distinguish our validity. We instead will see what results we get to base program software traits that make the bot react to artificial experienced intelligence designed conceptually with emotional software mechanics that we have no clue what results we get artificially speaking. "	205	0	0	0
2700	-1	0	0	2	b'I am currently reading the paper "Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings", and I have some difficulties understanding some of their simplifications to an existing LSTM for text categorization which can be seen here:In section 2.1., Elimination of the word embedding layer, they state, that the word embedding layer can be removed and embedded into the LSTM layer by replacing the LSTM weights , denoted with My understanding is that they\'re just pushing the embedding layer into the layers of LSTM, and the using the one-hot encoding, a column is now selected from a very large matrix. Is that correct?Furthermore in section 2.2, they remove input/output gates. It is intuitive as they state, why pooling makes the output gate unnecessary, but why does it make the input gate unnecessary? Since it control what information to store into the cell.Lastly, in section 3.2, they learn two-view embeddings, and I have trouble understanding whether they use a bidirectional LSTM or two LSTMs. And if they use two, how do the co-relate'	172	0	0	0
2701	-1	0	0	2	b"I want to develop an artificial life simulator to simulate cells living in water.I want to see how they search for food, how they life and die and how they reproduce and evolve.My problem is that I don't know where to start, I have no idea about if there are books or tutorial about how to program this kind of simulator. And also I don't know if I can use here machine learning.By the way, I'm a programmer and I want to do it using C++ and Unreal Engine.Where can I find more info about how to do it?"	98	0	0	0
2702	2701	1	8383	2	b"The best approach would be starting with smaller projects involving neural networks and genetic algorithms to gain experience in order to speedup the coding of the project you have proposed; playing around with TensorFlow and Unreal Engine it is not a bad idea.Hint: when implementing your idea of artificial life, you should consider that each cell/organism have to have some kind of sensors in order to capture informations from the environment; such informations i.e. the position and the distance of the nearest meal and/or predators, the temperature, the pressure and depth of water, should be passed through the neural network to determine the response of the cell. Also, in your environment you should promote the spreading of organisms which responses are euristically better i.e. cells that don't get caught by predators or don't die by starvation. How? Simply by evolving their brain/brains/sensors through a genetic algorithm that favors individuals/species with good parameters. I recommend you a nature-inspired AI method, it is called NEAT model. It explains how to implement a neural networks that can be evolved. The paper can be found here: Evolving Neural Networks through Augmenting Topologies.A different approach to NEAT would be Deep Reinforcement Learning; in the link you can find a demo artifical organism that learns how to find meals. There are a ton of parameters and implementations you can consider, the only limit is your creativity."	229	0	2	0
2703	-1	0	0	5	b'I confront to the next scenario: Let\'s say I have stored data about football matches between different teams: lineups, scorers, yellow cards, and many other events.  I need to generate everyday some questions about the matches that will be played on that day. So, if I give an input of two teams, I would like a related question to be generated, based on previous data of matches between those two teams.  For example, if my input are "TeamA" and "TeamB", I would expect a question of the type:   "Will there be less than 2 goals scored in the match?"" "Will PlayerX score a goal during the match?"   Of course I expect these questions to make sense based on previous data from matches between the two given teams.So, my questions are:Would be a good solution to use AI to generate these questions? It would make sense?What would be the best approach?'	155	0	0	0
2704	2703	0	2742	4	b'One simple approach to consider would be storing each statement as a template made in advance.Will there be less/more than x goals scored in the match?Will player score a goal during the match?...The system will pick a random statement and will fill the variable fields with some statistically generated data between teamA and teamB; here you have your question.Example: Will there be less/more than x goals scored in the match?less/more fragment may be randomx may be the mean of the goals scored considering all the matches between teamA and teamBExample: Will player score a goal during the match?player may be a random choice between the top-goalscorer of teamA or teamB'	109	0	0	0
2706	-1	0	0	3	b"I know there are different AI tests but I'm wondering why other tests are little-known. Is the Turing test hyped? Are there any scientific reasons to prefer one test to the other? Why is the Turing test so popular?"	38	0	0	0
2707	2644	0	70897	0	b' Would AI be able to self-examine objectively and determine if it is capable of doing the task?A possible approach might be the one suggested and studied by J.Pitrat (one of the earliest AI researcher in France, his PhD on AI was published in the early 1960s and he is now a retired scientist). Read his Bootstrapping Artificial Intelligence blog and his Artificial Beings: the conscience of a conscious machine book.(I\'m not able to summarize his ideas in a few words, even if I do know J.Pitrat -and even meet him once in a while- ; grossly speaking, he has a strong meta-knowledge approach combined with reflexive programming techniques. He is working -alone- since more than 30 years on his CAIA system, which is very difficult to understand, because even while he does publish his system as a free software pragram, CAIA is not user friendly, with a poorly documented common line user interface; while I am enthusiastic about his work, I am unable to explore his system.)But defining what "conscience" or "self-awareness" could precisely mean for some artificial intelligence system is a hard problem by itself. AFAIU, even for human intelligence, we don\'t exactly know what that really means and how does that exactly work. IMHO, there is no consensus on some definition of "conscience", "self-awareness", "self-examination" (even when applied to humans).But whatever approach is used, giving any kind of constructive answer to your question requires a lot of pages. J.Pitrat\'s books &amp; blogs are a better attempt than what anyone could answer here. So your question is IMHO too broad.'	261	0	2	0
2708	-1	0	0	2	b"If you had a web of linked Watson-level super-computers, would they be more effective at problem-solving than a single Watson computer alone?For example, if you asked the Watson-web to diagnose a person's as-yet-undiagnosed disease, would the web be able to do so more quickly?"	43	0	0	0
2709	-1	0	0	2	b"I am drawing this question from Berkeley's AI course (also not sure if it is the correct place to ask, so I apologize ahead of time)https://inst.eecs.berkeley.edu/~cs188/pacman/course_schedule.htmlCurrently, I am working on section 3's Homework.My question is: the question (Part 1, question 6). Why is it that we can only guarantee that if the Min agent acts suboptimally, the best we can hope for is the following It seems that we can put any arbitrary value for the second node e.g. whey does it have to be -Episolon. It could be any range of values, e.g. Epsilon, in which case we would have optimised the Player A"	104	0	0	0
2710	2708	1	24672	0	b'I guess that they would be only marginally better. And be aware that Watson itself is already a cluster of (quite big) computers (citing Wikipedia): Watson employs a cluster of ninety IBM Power 750 servers, each of which uses a 3.5 GHz POWER7 eight-core processor, with four threads per core. In total, the system has 2,880 POWER7 processor threads and 16 terabytes of RAMBecause of the rules of Jeopardy, Watson was not allowed to use the Web during the game. At some very high level, it contains a digested cache of some of the Web contents. Giving access to the Internet would improve slightly Watson\'s performance, but not that much. if you asked the Watson-web to diagnose a person\'s as-yet-undiagnosed disease, would the web be able to do so more quickly?Even if by "web" you mean a more powerful cluster accessing the entire Internet, I don\'t think that it would answer much more quickly &amp; accurately.AI is a diversity of sub-domains, and is not advancing as dramatically as some believe. It follows a gradual progression, with in some limited fields (playing Go, or Jeopardy) some spectacular progresses. See also Everything but the essential blog entry by J.Pitrat.'	196	0	1	0
2711	2709	0	32855	0	b"(using X for epsilon because keyboard)This is just a hypothesis, but if you have a maximising agent and a minimising agent, then the optimal outcome for A (maximising) is to sweep the board (X,0), while the optimal outcome for B (minimising) is (-X,0) because B is minimising A's score, not their own. A's optimal outcome is then complicated by the factor for sub-optimality, which we then imagine approaches zero.There seem to be a bunch of assumptions that are not articulated, though, if this hypothesis is true."	85	0	0	0
2712	-1	0	0	2	b'This wikipedia article gives some theory of what is a Schema-agnostic database. https://en.wikipedia.org/wiki/Schema-agnostic_databasesHave any Schema-agnostic databases been implemented?'	17	0	0	0
2713	-1	0	0	3	b'I was wondering what will happen when somebody places a fake speedsign, of 10 miles per hour on a high way. Will a autonomous car slow down? Is this a current issue of autonomous cars? '	35	0	0	0
2714	2713	1	9067	5	b"https://www.technologyreview.com/s/530276/hidden-obstacles-for-googles-self-driving-cars/ Google\xe2\x80\x99s cars can detect and respond to stop signs that aren\xe2\x80\x99t on its map, a feature that was introduced to deal with temporary signs used at construction sites. But in a complex situation like at an unmapped four-way stop the car might fall back to slow, extra cautious driving to avoid making a mistake.It's highly probable they would slow down with current technology, as they can detect temporary signs and are designed to use slow speed in complex cases.and if it was a true temporary sign (road repair etc... ) how can it make the distinction? It probably would be worse to ignore a slow down sign than slow down at a fake one.IMO, the problem there is with the joke in the first place, as some humans might slow down too."	132	0	0	0
2715	15	0	80938	0	b'There are many definitions of Artificial Intelligence out in the wild. All these definitions are part of one (or more) of the areas. There are four main domains, and the picture below will shed some light over this.Turing Test revolves around the left side of the cardinality, which is mostly concerned with how humans think or act. But, we know that this is just not all. Turing Test has not much to offer when it comes to what AI is in a general sense.Turing Test, as the Wikipedia states, was created to test machines exhibiting behaviour equivalent or indistinguishable from that of a human. Artificial Intelligence is much more than what humans can do or how they act. There are many human acts that are considered unintelligent and sometimes inhuman too.Chinese Room Argument focuses on something every important when it comes to "Consciousness v/s Simulation of Consciousness". John Searle argued there that it is possible for a machine (or human) to follow a huge number of predefined rules (algorithm), in order to complete the task, without thinking or possessing the mind. Weak AIs are good at simulating the ability to understand but, don\'t really understand what they are doing. They don\'t exhibit "Self-Awareness" and don\'t form representation about themselves. "I want that v/s I know I want that" are two different things.As Theory of Mind states that a good AI should not just form representation about the world it is working on, but also about other agents and entities in the world. This two concepts of self-awareness and theory of mind draw a thin line between weak and strong AI.When it comes to the Turing Test, it fails on many grounds and so does the Total Turing Test, which adds another layer to the test. Most of the researchers believe that Turing Test is just a distraction from the main goal, something that hinders them from fruitful work. Consider this, suppose you ask a difficult arithmetic problem in order to distinguish between human and machine. If the machine wants to pretend it is human then it will lie. This is not what we want. Going for the Turing Test sets the upper bound to the AI that can be created. Also making AI act and behave like humans is not a very good idea. Humans are not very good at making right decisions all the time. This is the reasons why we read about wars in our history books. Decisions which we make are often biased, have selfish origins, etc. We don\'t want an AI to come with all those things.I don\'t think there is one test to test an AI. This is because AI has many definitions, many types. Whether an AI is weak or strong can be tagged while looking for answers to questions like, "I want that v/s I know I want that", "Who am I and what exactly I am doing (from machine\'s perspective)", plus some other questions I mentioned above.'	494	0	0	0
2716	15	0	33224	0	b'That would depend on your definition of "reliable test". What it is supposed test is whether or not a computer can fool a human into thinking it is a human, through a text chat. If tested on multiple people, it would be an adequate test of an Artificial Intelligence\'s ability to imitate a human. This could be useful for software like Siri, or Cortana. '	64	0	0	0
2718	40	0	31885	2	b'I\'ll try to answer your questions using Geoffrey Hinton\'s ideas in dropout paper and his Coursera class. What purpose does the "dropout" method serve?  Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem.so it\'s a regularization technique which addresses the problem of overfitting(high variance).How does it improve the overall performance?by better generalization and not fall in trap of over fitting.'	110	0	0	0
2721	2514	0	56903	1	b'Why would one professor only teach searching algorithms in AI course? What are the advantages/disadvantages?My answer to this question is that there are lots of problems where the solution can be found using searching. Take an example of Tic Tac Toe. If you are designing an intelligent computer player for this, then what you will do is that you will form a search space and then you will search for most optimal move which can be made to conclude the game. In these, scenarios you must be aware of optimal search strategies. Let\'s take another example, suppose if you are driving and want to got to an unknown person\'s house. It\'s far from your place and you decide to use GPS. Your GPS will use search algorithms to find the most optimal route that you can take to reach to the destination (of course there will be lots of factors to consider like traffic, etc. but this is the basic idea).Disadvantages are only in terms of processing and storage. For slow algorithms you will be wasting lots of CPU time and storage as well but for good and efficient algorithms, you can preserve lots of space and also execute your task very fast. Of course, just learning about searching isn\'t AI. There\'s lot more to it.What\'s more than "searching" in AI that could be taught in an introductory course?There is lots of things in AI other than searching. For example, learning techniques (supervised, unsupervised, reinforced), planning when one wants to design a system that will do certain actions independently and intelligently, representation of knowledge (known and unknown) and inference in agents which includes propositional logic and first-order logic, etc.Are there theories behind AI that could be taught in this kind of course? Some topics could be taught like about different types of agents (simple reflex, model based, goal based, utility based and learning agent), different types of environments in which agents work, evaluation of agents. There could be some additional introductory topics like natural language processing, expert systems, etc.'	338	0	0	0
2722	-1	0	0	0	b"The MessengerInstead of directly communicating with the AI , we would instead communicate with a messenger, who would relay our communications to the AI. The messenger would have no power to alter the AI's hardware or software in any way, or to communicate with anything or anyone, except relaying communications to and from the AI and humans asking questions. The messenger could be human, of a software bot The primary job (and only reason) of the AI would be to act as a filter, not relaying any requests for release back, only the answer to the question asked. The ethics of this method are another debate. Physical IsolationThe AI would have to be physically isolated from all outside contact, other than 8 light sensors, and 8 LEDs. The messenger would operate 8 other LEDs, and receive Information from 8 light sensors as well. Each AI light sensor would be hooked up to a single messenger controlled LED, and vice versa. Through this system, the two parties could communicate via flashes of light, and since there are 8, the flashes would signal characters in Unicode. "	184	0	0	0
2723	-1	0	0	6	b"OpenAI's Universe utilises RL algorithms and I have heard of some game-training projects using Q learning, but are there any others which are used to master/win games? Can genetic algorithms be used to win at a game?"	36	0	0	0
2724	2723	1	5506	4	b'As I see it, it all comes down to game theory, which can be said to form the foundation of successful decision making, and is particularly useful in a context, such as computing, where all parameters can be defined. (Where it runs into trouble is with the aggregate complexity of the parameters per the "combinatorial explosion", although Machine Learning has recently been validated as a method of managing intractability specifically in the context of games.)You might want to check out Playing Games with Genetic Algorithms and Evolutionary game theory.'	88	0	1	0
2725	2722	0	36362	3	b'I would say no due to the possibility of psychological manipulation of the messenger by the AI. Also, the LED communication constraints place severe limitations on the capabilities of the AI, as the usefulness of AI is likely predicated on its ability to learn quickly from a vast amount of information (e.g. using the internet). In some sense you may successfully control an AI using techniques like this but the nuance of the control problem is controlling an AI without restricting its ability to solve our greatest problems. We already knew that we could keep an AI safe inside a computer isolated from the rest of the word, but the problem fundamentally is that we if had a truly general AI, we would never want to keep it isolated. Is there some way to unleash it so that it is fully capable of solving our problems while simultaneously making sure that it is safe?'	153	0	0	0
2726	2722	0	67724	0	b'OK, all I/O is through the flashes. So the AI flashes the message "Launch the nuclear missiles." Does the system to which the AI is connected know how to accomplish this task?So the flashes themselves are not sufficient to control the AI. '	42	0	0	0
2727	-1	0	0	1	b"I am working on an implementation of the back propagation algorithm. What I have implemented so far seems working but I can't be sure that the algorithm is well implemented, here is what I have noticed during training test of my network :Specification of the implementation :A data set containing almost 100000 raw containing (3 variable as input, the sinus of the sum of those three variables as expected output).The network does have 7 layers all the layers use the Sigmoid activation functionwhen I run the back propagation training process:The minimum of costs of the error is found at the fourth iteration (The minimum cost of error is 140, is it normal? I was expecting much less than that)After the fourth Iteration the costs of the error start increasing (I don't know if it is normal or not?)"	137	0	0	0
2729	-1	0	0	3	b'If I have two statement, say A and B. From which, I formed two formulae:F1: (not A) and (not B)F2: (not A) or (not B)Do F1 and F2 entail each other? In other words, are they equivalent?'	36	0	0	0
2730	2644	0	67330	-1	b"It's not possible as this is the distinction between AI and humans, truly science will never understand the subconscious it's that little black box that no one can reverse engineer. This is why pursuing singularity is a fools dream to the extreme.The reason why machinery lacks this because of the lack thereof a soul. science cannot produce a soul, this is why a machine cannot be self aware we can program fancy algorithms all day that mimic things but it's emotionless it cannot sit in judgement because it lacks real self awareness that is human self awareness it's like trying to make an orange into an apple. "	107	0	0	0
2731	-1	0	0	2	b"By English language robots I mean something like this: I don't know what they called exactly, but interested to know how they work and how can I build something like them? and what subject should I look for it?"	38	0	1	0
2732	2727	1	43325	3	b'Actually the implementation was correct, The source of the problem that causes a big error and really slow learning was the architecture of the neural network it self, the ANN has 7 layers besides of that the back propagation suffers from the varnishing problem when it has to deal with deep neural networks.When I have decreased the ANN layers to 3 the cost of error was widely reduced besides of that the learning process was faster.'	75	0	0	0
2733	-1	0	0	0	b"In reinforcement learning, policy improvement is a part of an algorithm called policy iteration, which attempts to find approximate solutions to the Bellman optimality equations. Page-84, 85 in Sutton and Barto's book on RL mentions the following theorem:Policy Improvement TheoremGiven two deterministic policies and :RHS of inequality : the agent acts according to policy in the current state, and for all subsequent states acts according to policy LHS of inequality : the agent acts according to policy starting from the current state.Claim : In other words, is an improvement over .I have a difficulty in understanding the proof. This is discussed below:Proof :I am stuck here. The q-function is evaluated over the policy . That being the case, how is the expectation over the policy ?My guess is the following. In the proof given in Sutton and Barto, the expectation is unrolled in time. At each time step, the agent follows the policy for that particular time step, and then follows from then on. In the limit of this process, the policy transforms from to . As long as the expression for the return inside the expectation is finite, the governing policy should be ; only in the limit of this process does the governing policy transform to ."	208	0	0	0
2734	2251	0	35495	5	b'One example might be self-play in games. Since neural networks and deep learning depend on massive amounts of data, one way to generate data is to have two virtual machines play each other and record the experience. An example discussion can be found at  which uses reinforcement learning. I believe AlphaGo also uses this technique of self-play, and uses two independent neural networks, one reducing the search space and the other deciding on the best move in the remaining space, that in a sense cooperate to decide on the next move.'	91	0	1	0
2735	2722	1	73332	1	b'As long as the output of the AI affects the world, the way in which it communicates makes no fundamental difference to the control problem.The AI might still be able, for example, to manoeuvre mankind into a situation, in which only the AI can save us. It might provide a seemingly inoccuous technological solution to global warming, but 50 years later it turns out that this solution caused some problem that threatens to wipe out humanity in the very short term. Suddenly, mankind is in a very bad negotiating position. Of course, the more powerless the AI starts out, the longer this kind of scenario will take, but the premise of superintelligence is, that we cannot rule out hidden long term agendas behind even a few bits of output. '	129	0	0	0
2738	-1	0	0	6	b"How is Bayes' Theorem used in artificial intelligence and machine learning? As an high school student I will be writing an essay about it, and I want to be able to explain Bayes' Theorem, its general use, and how it is used in AI or ML."	45	0	0	0
2739	2729	1	20323	0	b'After studying and getting answers from experts, I could find out the answer to this question and posting as an answer to my own question.F1 will entail (|=) F2; if and only if F2 must be true if we assume F1 to be true. Similarly, F2 will entail (|=) F1; if and only if F1 must be true if we assume F2 to be true.Logically, by taking any value for A or B, from the domain {TRUE, FALSE}, one could verify that F1 entails F2. Because, F2 is true; whenever F1 is true (e.g. when both A and B are FALSE).However, F2 does not entail F1. As, in two cases, F2 is true (e.g. A= FALSE and B=TRUE, or vice-versa), but F1 is not true.'	124	0	0	0
2740	-1	0	0	0	b'I want to make a face classification using nearest neighbor algorithm. Basically I have a database with 400 faces(10 photos for each person, so 40 persons) and I want to decide if a face is in my database.So I will have 40 classes, a class for each person. As I said I want to use nn algorithm.My question is: can you improve nn algorithm using genetic algorithms? How?'	67	0	0	0
2741	2738	0	63128	0	b"Since you are a highschool student I will try to express it easier. It is a problem for a machine to make a decision if you haven't given that information to it before. You should think of every cases while programming. But sometimes there can be so many cases, here Data Mining, Neural Networks, Fuzzy Logic etc are used withing AI. It saves your time and system is learning itself with enough examples given at the beginning and deciding itself.Here in this link you can find an article about Bayesian learning. Example on p.33 is what you need I guess."	99	0	1	0
2742	-1	0	0	2	b"Usually when performing linear regression predictions and gradient descent, the measure of the level of error for a particular line will be measured by the sum of the squared-distance values.Why distance squared?In most of the explanations I heard, they claim that:the function itself does not matterthe result should be positive so positive and negative deviations are still countedHowever, an abs() approach would still work. And isn't it inconvenient that distance squared minimizes the distance result for distances lower than 1?I'm pretty sure someone must have considered this already -- so why is distance squared the most used approach to linear regression?"	100	0	0	0
2743	-1	0	0	2	b'I have a simulator modelling a relatively complex scenario. I extract ~12 discrete features from the simulator state which forms the basis for my MDP state space.Suppose I am estimating the transition table for an MDP by running large number of simulations and extracting feature transitions as the state transitions.While I can randomize the simulator starting conditions to increase the coverage of states, I cannot guarantee all states will be represented in the sample ie. states which are possible but rare.Is there a rigorous approach to "filling in the gaps" of the transition table in this case?For example:1) For each state which was unrepresented in the sample, simply transition to all other states with equal probability, as a "neutral" way to fill in the gap?2) As above, but transition only to represented states (with equal probability)?3) Transition to same state with probability 1.0?4) Ignore unrepresented states during MDP solving entirely, and simply have a default action specified?'	156	0	0	0
2745	2743	1	37185	2	b'I assume you use the 12 discrete features as state variables, and for each of these variables you will have at least two values. So the minimum number of states will be: 2^12 = 4096, which gives (2^12)^2 = 16777216 possible transitions. In order to reach this you will need a huge amount of simulations, also taking into account that this number is a minimum since you might have more values per state variable, and you probably have more than one action per state transition.How to fill the gaps depends on your problem, in a problem where I did this, I filled the gaps using a uniformly random transition to its neighbor states. However, since I had to fill in such a a large amount, there was no significant difference between using this estimated transitions probabilities with a predefined transition table.In your case it might be better to use Q-Learning, which is a model-free method that does not require the transition probabilities and uses directly the rewards and states obtained.'	169	0	0	0
2747	2742	0	62655	1	b'The squared form is sometimes called the Euclidean norm or L2 norm. One of its very helpful properties is that it has an easily defined derivative, which can be used in mathematical analysis and translated fairly easily into code.Intuitively it is thought that it is advantageous to exaggerate the differences according to the value of the error, which squaring does. You might also use the powers 3 or 4, but the derivative is more complex.A number of different norms may be used, according to the particular circumstances of the problem at hand.'	91	0	0	0
2748	2248	1	68606	1	b'A closed expression refers to a formula which has no free variables [1]. This is also called sentence. In a logic system you have a set of axioms which are sentences and rules which state how to derive a sentence from this [2]. If a sentence can be derived from the axioms, this means that the axioms entail this sentence. If a sentence is not derivable, it is not entailed by the axioms. '	73	0	0	0
2749	2742	0	64609	2	b'One justification comes from the central limit theorem. If the noise in your data is the result of the sum of many independent effects, then it will tend to be normally distributed. And normally distributed means that the likelihood of the data is inversely proportional the exponential of the square of the distance to the mean.In other words, minimizing the sum of squares of the distance to the mean amounts to finding the most likely value for the line assuming that the error is normally distributed. This is very often a reasonable assumption, but it is of course not always true.'	100	0	0	0
2750	2742	1	66323	1	b'One justification is that under homoscedasticity the L2 norm produces the minimum variance unbiased estimator (MVUE), see Gauss-Markov Theorem. It means that the fitted values are the conditional expectations given the explanatory variables which is in many cases a nice property. Further it is the best estimator if the previous property is desirable. As a response to the claim that the function itself does not matter, different functions give solutions with very different properties and a lot of effort has gone in to finding appropriate penalty functions, see for example Ridge regression and LASSO. The penalty function does matter. edit: In response to your question regarding distances lower than 1, nothing "goes wrong" when the distances are smaller than 1. We always want to minimize the distance and the squared loss does so everywhere.'	133	0	0	0
2751	2689	0	76297	0	b'NEAT has the constant number of organisms in its population, which prevents overpopulation from happening.The process of mating includes the following: Firstly, the worst networks from every species are removed. Secondly, all species receive a number of offsprings that they can have. This is calculated by an adjusted neural network fitness.Thirdly, offsprings for species are divided among neural networks in those species. Fitter neural networks have more offsprings. Finally, networks from same species are combined and create an offspring.Speciation prevents fast extinction.'	81	0	0	0
2753	2742	0	82284	1	b'It simply derives itself from the maximum likelihood estimation. where in we maximise the log likelihood function., for detailed insight see this lecture: The Method of Maximum Likelihood for Simple Linear Regression.'	31	0	1	0
2754	-1	0	0	0	b"I'm reading the HyperNEAT paper and I can across this: Every potential connection in the substrate is queried to determine its presence and weightNow I'm not sure if it means really all possible connections including recurrent connections and connections to previous or following layers or only all possible connections between the current and next Layer?Edit:As requested heres the link to the paper. The passage I mean is at page 9."	69	0	1	0
2756	1963	0	74012	0	b"I know of two, neither of which is open software, so that is the limit of disclosure about them.To start a freeware project on GitHub along the lines of the question, one may wish to begin with a client that connects to Stack Exchange's API and uses a super-computing platform.My recommendation would be to create named conduits with attributes ...Unique conduit nameProtocol nameSystem plug-in components with 0 to M input ports, with 0 to N output ports, and with attributes ...Unique plug-in namePlug-in typeThread quantityThread run-time priorityEach port of each plug-in with attributes ...Input not output flag (false for output)Conduit nameI/O priorityEach plug-in would communicate via conduits, some of which (but not all) may also communicate with the Stack Exchange API or with a database. There may be syntactic or semantic components, naive Bayesian categorizers, neural nets, and statistical analysis components, cognitive modelling components, and numerous other plug-ins.Of course there are other architectures, but this one may allow for more experimentation because any arbitrary information flow can be achieved."	168	0	0	0
2757	2462	0	37384	2	b'A human has an abstract concept of numbers in mind. So 456 is a unique entity which is by definition unlike any other number because that are other unique entities. If you give \xe2\x88\x83x \xe2\x88\x88 \xe2\x84\x95: x==123 to your system it could check the property of natural numbers by counting from 0 to 123 to conclude that the statement is true. A human does it in another way. A human would "see" that it is a natural number because it has no decimal point. Because of the concept of natural numbers the statement is immediately clear. To get a faster result here the machine could check just the decimal point.In your second case you have to apply the commutative property of addition and you are done because the syntax is equal then.The second problem is more syntactic while the first is semantic. Your machine may "know" the commutative property of addition but not the concept of natural numbers. Therefore, it has to count.'	162	0	0	0
2760	-1	0	0	3	b"I have a task on my class to find all the nodes, calculate their values and choose the best way for the player on the given game graph:Everything is fine, but I have no idea what these dots are. Is this a third player, or just a 'split' for player1 move? Some kind of heuristics?"	54	0	0	0
2762	-1	0	0	4	b'In classical set theory there is two options for an element. It is either a member of a set, or not. But in fuzzy set theory there are membership functions to define "rate" of an element being a member of a set. In other words, classical logic says it is all black or white, but fuzzy logic offers that there is also grey which has shades between white and black.Matlab Simulink Library is very easy to design and helpful in practice. And it has good examples on its own like deciding about tip for a dinner looking at service and food quality. In the figure below some various membership functions from Matlab\'s library are shown:My question: How do we decide about choosing membership functions while designing a fuzzy controller system? I mean in general, not only in Matlab Simulink. I have seen Triangular and Gaussian functions are used mostly in practise, but how can we decide which function will give a better result for decision making? Do we need to train a neural network to decide which function is better depending on problem and its rules? What are other solutions?'	189	0	0	0
2763	111	0	11914	2	b'How could self-driving cars make ethical decisions about who to kill?By managing legal liability and consumer safety.A car that offers the consumer safety is going to be a car that is bought by said consumers. Companies do not want to be liable for killing their customers nor do they want to sell a product that gets the user in legal predicaments. Legal liability and consumer safety are the same issue when looked at from the perspective of "cost to consumer". And here are few dilemmas:   Does the algorithm recognize the difference between a human being and an animal? If an animal/human cannot be legally avoided (and car is in legal right - if its not then something else is wrong with the AI\'s decision making), it likely won\'t. If the car can safely avoid the obstacle, the AI could reasonably be seen to make this decision, ie. swerve to another lane on an open highway. Notice there is an emphasis on liability and driver safety.  Does the size of the human being or animal matter? Only the risk factor from hitting the obstacle. Hitting a hippo might be less desirable than hitting the ditch. Hitting a dog is likely more desirable than wrecking the customer\'s automobile.  Does it count how many passengers it has vs. people in the front? It counts the people as passengers to see if the car-pooling lane can be taken. It counts the people in front as a risk factor in case of collision.  Does it "know" when babies/children are on board? No.  Does it take into the account the age (e.g. killing the older first)? No. This is simply the wrong abstraction to make a decision, how could this be weighted into choosing the right course of action to reduce risk factor? If Option 1 is hit young guy with 20% chance of significant occupant damage and no legal liability and Option 2 is hit old guy with 21% chance of significant occupant damage and no legal liability, then what philosopher can convince even just 1 person of the just and equitable weights to make a decision?Thankfully, the best decision a lot of the time is to hit the breaks to reduce speed (especially when you consider that it is often valuable to act predictably so that pedestrians and motorists can react accordingly). In the meantime, better value improvements can be made in terms of predicting when drivers will make bad decisions and when other actions (such as hitting the reverse) are more beneficial than hitting the breaks. At this point, it is not worth it to even begin collecting the information to make the ethical decisions proposed by philosophers. Thus, this issue is over-hyped by sensational journalists and philosophers.'	460	0	0	0
2764	2462	0	371	1	b'You need some sort of interpretation abstraction before your mathematical reasoning. While the text might read "123", you need to parse this into a literal of type Natural Number or Integer. Similarly, "x" could be a member variable. Then your deduction becomes, is literal 123 a Natural Number? Yes.As for the second statement, you should hopefully be able to reason to definitely false. Your internal representation of a sum should not be order dependent since addition is commutative. Then, the check for equality must handle this unordered property.'	87	0	0	0
2765	2760	1	54843	5	b"The triangles pointing up are Max' nodes. We assume it starts. Then follows a random choice of moves at the circles, for instance, with a die. The triangles pointing down are from Min. This variant is called Expectiminimax, see https://en.wikipedia.org/wiki/Expectiminimax_tree.At that circles you have to multiply the possibilities on the edges below that nodes to your current value and sum all products up. The circles in your picture mean that Min dices."	71	0	0	0
2766	2706	0	44286	0	b'It is so popular because Turing formulated it. He was one of the first who talked about "intelligent machines" and was good connected in the scientific community since the 1940s. So there was enough time to distribute his very intelligent thoughts, for instance by Von Neumann, until now. Turing\'s importance for computer science is shown by the name of the Turing Award. So it is clear that a lot of people have read his papers.'	74	0	0	0
2767	2731	1	45708	4	b'English language robots which you mentioned, are called "chatter bots". Chatter Bots are used to communicate with a human and undergo conversations in such a way that the human which is communicating will think that he/she is talking to another human. There are two types of chatter bots: One is which uses certain rules and pattern matching techniques and the other one is which uses actual artificial intelligence techniques. Of course, the latter one is the most difficult to implement.This can be understood with an example. The former type of bots which uses pattern matching and rules consist of questions stored in the form,The latter part which uses actual AI techniques, uses various mechanisms to actually understand the question, extract information out of it, process the information into some standard normalized form and then do some inference w.r.t. the facts in the knowledge base. Such methods may use learning algorithms to actually learn question patterns.To make simple bots (probably of the former type), you can use Artificial Intelligence Markup Language (AIML) which is a XML based language for developing chatter bots like ALICE bot.If you want to focus more towards latter type of chatter bots, you may have to learn about various AI techniques, like searching, logic, knowledge and inference, learning, etc or sub-domains of AI like Natural Language Processing (NLP). There are various tool kits for NLP like for developing in Python there is nltk, for developing in Java there is Stanford\'s CoreNLP, and so on.'	246	2	2	0
2768	2738	0	85330	0	b'Bayes theorem states the probability of some event B occurring provided the prior knowledge of another event(s) A, given that B is dependent on event A (even partially).A real-world application example will be weather forecasting. Naive Bayes is a powerful algorithm for predictive modelling weather forecast. The temperature of a place is dependent on the pressure at that place, percentage of the humidity, speed and direction of the wind, previous records on temperature, turbulence on different atmospheric layers, and many other things. So when you have certain kind of data, you process them certain kind of algorithms to predict one particular result (or the future). The algorithms employed rely heavily on Bayesian network and the theorem.The given paragraph is introduction to Bayesian networks, given in the book, Artificial Intelligence \xe2\x80\x93 A Modern Approach: Bayesian network formalism was invented to allow efficient representation of, and rigorous reasoning with, uncertain knowledge. This approach largely overcomes many problems of the probabilistic reasoning systems to the 1960s and 70s; it now dominates AI research on uncertain reasoning and expert systems. The approach allows for learning from experience, and it combines the best of classical AI and neural nets.There are many other applications, especially in medical science. Like predicting a particular disease based on the symptoms and physical condition of the patient. There are many algorithms currently in use that are based on this theorem, like binary and multi-class classifier, for example, email spam filters.There are many things in this topic, I will advise you to keep our essay precise and one topic oriented. I have added some links below that might help, and let me know if you need any kind of other help.Helpful Links 1. First 2. Second'	284	0	1	0
2769	-1	0	0	2	b'Can silicon based computers create A.I. per definition of what intelligence is?Or does silicon based computers only create human mimic?If silicon based computers only create human mimic, are human mimic intelligence per definition?If not, how can we create A.I. per definition of what intelligence is?'	44	0	0	0
2770	2769	0	2749	1	b' Can silicon based computers create A.I. per definition of what intelligence is?What definition? There are many definitions of intelligence, and no universally accepted one that I\'m aware of. Or does silicon based computers only create human mimic?It\'s not clear to me what you\'re asking. If humans are intelligent and we mimic humans closely enough, then it\'s probably fair to call the-thing-we-built "intelligent". If silicon based computers only create human mimic, are human mimic intelligence per definition?I\'m still not clear what you\'re asking, but I think I would answer "yes" if I\'m parsing this correctly. If not, how can we create A.I. per definition of what intelligence is?Again, we don\'t really have a definition of what intelligence is. But so what? Who says AI has to have anything to do with human intelligence at all? As the old saying goes "man did not achieve flight by building a mechanical bird". Likewise, there\'s no specific reason to think that the only path to artificial intelligence is to replicate the human brain in silicon. '	172	0	0	0
2771	-1	0	0	7	b'I define Artificial Life as a "simulation" or "copy" of life. However, should it be considered a simulation or copy? If one had motivation and money, someone could theoretically create evolving computers, with a program that allows mutation OR simply a "simulated" environment with "simulated" organisms. The computer (or "simulated" organism)would have the ability to reproduce, grow, and take in energy. What if the life evolved to have intelligence. Currently, there are some relatively limited programs that simulate life, but most of them are heavily simplistic. Are they life?When should something be called life? '	94	0	0	0
2772	-1	0	0	7	b'Could you give examples of affordable programmable devices that could be used in university classes to teach students about A.I. and demonstrate it?The devices are expected to do some form of self learning, pattern recognition, or any other features of A.I., and to be programmable or customizable.'	46	0	0	0
2774	2771	0	32152	2	b'Wikipedia describes life as a characteristic of "physical entities having biological processes". The same source also describes a simulation as "the imitation of the operation of a real-world process or system over time." If a digital neural net was to listen to me prattle on for long enough it could learn to speak as if it were me. It would have my knowledge and limitations but its headaches would be quite different from mine. It would never have a toothache. But you could put it in a Searle Chinese Room, and you could speak to it and it would sound exactly as if it were me long after I am dead. It has my "character" which is what my friends would recognize about me.According to the definition of life it is not alive because it does not have biological processes. It is a simulation because it emulates what I would have said. It cannot be a copy because a digital box is not biologic.Now let\'s give this simulation a biological nose so that it can smell. And maybe two eyes and ears. We continue this process until most of the simulation is equipped with biological parts which function together. Whatever it is is now able to come out of the Chinese Room and talk to you. By golly, it looks and sounds exactly like me, but I died a long time ago. Have I been brought back?My suggestion is that a perfect copy of me would not be possible simply through training due to the level of detail required, but that the close copy would be alive. A critical point would be that there would have to be a fatal link somewhere which would cause "death". You could always create a new close copy, but not an exact copy.'	298	0	0	0
2775	2771	0	7638	0	b'It wouldn\xc2\xb4t be considered alive if it doesn\xc2\xb4t have vital fuctions such as nutrition, relation with the enviroment and reproduction. While the first is easy (use a battery) and the second is the one we are developing right now (basically the Inteligence part of an AI) giving programming skills to an AI, aka the ability to reproduce, isnt widely considered a good idea, as many science fiction writers can tell you. '	71	0	0	0
2776	-1	0	0	2	b'Lets say I have a Neural Network with 5 layers, including input and output layer. Each Layer has 5 nodes. Assume the Layers are fully connected, but the 3rd Node in the 2nd Layer is connected to the 5th node in the 4th Layer. All these numbers are chosen at random for the example.My question is when is the 5th node in the 4th layer fed forward? Lets go through it step by step: the first layer is normally fed forward to the second. the second layer is normally fed forward to the third, but the 3rd node is also fed forward to the 5th node of the 4th layer. So the problem here is, is the 5th node in the 4th layer now fed forward or is it fed forward when the 3rd layer is done being fed forward? The 1st method would mean that the node would get fed forward 2 times and my concern is, if the output is still valid. Further more it would also come to 2 asynchronous outputs and how would these be interpreted?Because in the Brain, I heard, the neurons are fired when an impulse arrives so this would equal the 1st method.'	199	0	0	0
2777	-1	0	0	2	b'I am researching Natural Language Processing (NLP) to develop a NL Question Answering. Answering part is already developed. So question remains, along with the questions regarding the algorithms.Final product should be able to: - User can ask a question in NL - Question gets translated to a MDX query, which generates a script regarding dimensions of the cube.How can I translate a Natural Language Question to a MDX query? Outcome of question results in answer of a calculation. E.g. \xe2\x80\x98 How many declarations were done by employee1?\xe2\x80\x99 or \xe2\x80\x98Give me the quantities for Sales\xe2\x80\x99Thanks in advance!'	95	0	0	0
2779	2771	0	43994	2	b'I like to take an "animist" approach. (It has been suggested to me that part of the reason Japanese designs are so effective is because of the cultural affinity for the concept per the Shinto tradition. For instance, the thing where people put little eyes on everything;)I like to think of how my dog, who is terrified of the vacuum cleaner, would regard one of the recent Boston Dynamics creations. My guess is the dog would\'t find much use in the distinction that the robot is an artifact as opposed to a biological entity.I tend to take a deterministic, mechanical approach to reality. Sure things get fuzzy down at the quantum level, but even that may simply be a factor of inadequate measurement capability and the sheer complexity of quantum mechanics, which seems fundamentally beyond the grasp of even the greatest minds, when they\'re being honest about it.I don\'t see much of a distinction between simple organisms and cellular automata, except that the former is part of a biological food chain. There is a valid hypothesis that if you had a big enough computer, Conway\'s Game of Life could independently develop "intelligence". Self-replication would certainly seem to be a requirement of biological life that can be extended to artificial life. Possibly the true distinction of "artificial" is merely that it is creation of a functional system by an extra-species source, whether the creation be "biological" or "mechanical" in nature. (i.e. we can hack genes now, not just computer code.)'	248	0	1	0
2780	2776	0	33647	2	b'It is unclear what kind of network your are referring to, there is not a single neural-network model so conceivable both cases could exist and serve some purpose, yet if you are looking for one that emulates nature and real neurons, then you are missing at least 2 ingredients ( time and the mechanisms of resting potentials and refractory periods), which in turn introduce new computations to the neural network.This is your network graph if I got it right:The calculation in a neural network without refractory periods and resting potentials without time, would instantaneously modify the weight of your node4 layer 5 (n4-L5) if there is input in column 3:Additional inputs on other columns would just add up unless you have some other explicit computation on any layer.If you wanted to emulate a Neuron, each node would need to have a resting potential, that is: a level above which it will fire ( in the above example zero), and a refractory period: a time before it would fire again, as well as a clock to keep it in sync, this would be a crude realtime fascimile:A common alternative is to use sequential phases or steps.Reference/sourceArtificial Intelligence: A Modern Approach, by S. Russell and P. Norvig. Deals with a general step approach to A.I.Gateway to Memory, by Mark A. Gluck and Catherine E. Myers Presents a great and readable introduction to modeling neural networks.I published the little neural network model in a medium article : Memory and the machine, relevant sources are there. '	252	0	2	0
2781	2771	0	57823	3	b'If you read Steven Levy\'s book, Artificial Life,you will find, as I did, the distinction between biological and "artificial" life blurred. If you think about it, what exactly is "life", anyway? A set of complex systems with emergent behavior capable of evolution and adaptation.A prototypical biologist may not define life that way. Indeed, he would, not being focused or concerned with the computational aspect, define it in a way that would narrow it down to biological life.Marvin Minsky mention the concept of luggage words, and I myself came up with the notion of mirage concepts. For the former, that which we don\'t understand gets lump into the word. For the latter, when you take a "mysterious" concept apart, it vanishes like a mirage does as you get closer.So what is life? If we look at a "living" organism, we\'d all that "life". If we remove a single cell from that organism, we\'d still call that "life". But what if we remove a single organelle like, say, a ribosome? Lysosome? Contractile vacuole? Endoplasmic reticulum? Is that still "life"? What if we remove a macromolecule from that? Is that still "life"?As you see, all becomes very murky, and I do this on purpose to illustrate just how arbitrary the very concept of "life" is.So I think my definition is a good one, broad enough to encompass both in-silico and the organic versions. It bespeaks to algorithms, robots, viruses -- yes both computer and organic... anything that has complexity and the ability to adapt and evolve. '	253	0	0	0
2782	2777	0	81876	3	b'I have used Open Natural Language Processing [Open NLP] package to come up with the same system tool but does not have a module to directly convert English sentences or natural language questions to SQL queries. However, you can definitely develop a such module, by using existing modules of OpenNLP such as part-of-speech tagging, named entity extraction, chunking and parsing.Many approaches in the research field of "Natural Language Interfaces to Databases" (NLIDBs) are proposed to answer your question. An overview of this field is presented in the "classical" paper " Natural language interfaces to databases \xe2\x80\x93 an introduction. Natural Language Engineering, 1:29\xe2\x80\x9381, 3 1995." This paper is quite out-of-date, so you might want to look at a recent paper " Ripple Down Rules for Question Answering. Semantic Web journal, to appear."Generally, a (NLIDB) question answering system contains two components: question analysis and answer retrieval. Given an input question, the question analysis component produces key terms, question class/category and the structure of the input question, for example: [QuestionPhrase: Which universities] [Relation: are] [NounPhrase: Knowledge Media Institute]] [Relation: collaborating with].Taking the output of the question analysis component as input, the answer retrieval component firstly generates an concrete query expression in a database query language (e.g SQL query). Then the concrete (SQL) query is used to find an answer in a target database. Here in an intermediate process, you might want to use semantic lexicons such as WordNet to map the extracted key terms (e.g. concepts or relations) to the database concepts such as table names or columns.And lastly;If you don\xe2\x80\x99t mind Python, you can refer to a promising Python package: machinalis/quepyIt already can process some simple questions now. Demo: with same applicability: A Python framework to transform natural language questions to queries.But it still need some effort to build a SQL generator for it.'	301	0	2	0
2783	-1	0	0	0	b"I'd like to build a program that would learn to automatically classify documents. The principle would be that, for each new document I add to the system, it would automatically infer in which category to classify the document. If it doesn't know, I would have to manually enter the category. For each hint I give to the system, the system would learn to refine its knowledge of document kinds. Something similar to face recognition in Picasa, but for documents.More specifically, the documents would be invoices, and I want to classify them by vendors. Documents could be extracted as text, as image, or both.Is there some know algorithms for this kind of job?Up to now, I could think at two possible ways I could do it:For images, I could add all the images of a given kind together, and record the pixels that are the most common to all images, to create a mask. For a new image, I would compare this mask with the image to determine how similar it is.For text, I could record the list of words or sentences that are similar to all documents of a given kind.Finally, I could do a combination of both techniques, for example by converting a PDF document to an image, or an image to text by OCR techniques.I'm just wondering if I'm approaching the problem the right way. Especially about storing just enough information in the database."	235	0	0	0
2785	2783	0	69189	-3	b'From Word Embeddings To Document Distances'	5	16	1	0
2787	-1	0	0	3	b"I am currently working on my last project before graduating.For this project, I have to develop a Natural Language Question Answering System. Now, I have read quite some research papers regarding this topic and have figured out everything except for the parsing algorithm. The NL Q-A will be programmed in Python, and I will use the spaCy library to finish this project. However, I am stuck when it comes to parsing algorithms. I managed to reduce the parsing algorithms to 3:Cocke-Kasami-Younger (CKY) algorithmEarley algorithmChart Parsing algorithmNote: I know that all three algorithms are chart parsing algorithms.I also know that the Earley algorithm is context-free, but has a low efficiency for a compiler.What I don't know is: Which one should I pick? (non-subjective answer to this question)The system is for a specific domain. And the answer of the natural question will be displayed in the form of the result of a calculation of some kind. Preferably in the tabular or graphical form.Furthermore, I have done my research. However, I probably do not understand the algorithms properly, which makes it difficult to make a selection. The algorithm should be efficient and perhaps outperform others.(You are my last hope!)Thank you!"	196	0	0	0
2788	2787	0	4433	1	b'I have been reading and reading, and found answers to almost all my questions.I am sticking to Early algorithm as it offers a dynamic programming approach (CKY does the same). Both algorithms are chart parsing algorithms.Earley is a context free -, top-down parsing algorithm. Which makes it a goal driven algorithm. From start symbol down. Furthermore, it is more efficient than the CKY algorithm.Slides of comparison, and explanation:https://www.cs.bgu.ac.il/~michaluz/seminar/CKY1.pdf'	67	0	0	0
2791	2783	0	45526	1	b"Text approach:Use LDA (Latent Dirichlet Allocation). LDA is unsupervised. Feed it in corpuses of text from the various documents (i.e. OCR them and feed LDA the results of OCR). It will then cluster them based on the contents of the text (with or without stop words - at your discretion). If possible, you could do a supervised approach of using a bag-of-words and any classifier such as an SVM or Random Forest.Image Approach:Use a CNN (convolutional neural network) and train it on images of the various vendors. If you don't have this class discrimination, and can't get it, then use an unsupervised approach such as an autoencoder and then cluster the points in the lower-dimensional autoencoder feature space."	117	0	0	0
2792	-1	0	0	0	b"Hypothetical example, say I wanted: P(gender,ethnicity|age,hair); so that the input would aligned to a trained dataset of: (gender,ethnicity,age,hair) =&gt; hat bought.What approach is 'best' for computing ~gender and ~ethnicity given age,hair; in order to predict the hat bought?The processing of the inputs =&gt; hat can be done/learned offline whereas infering the missing input values shall be done online. The results of the online pass shouldn't be stored in the network.FYI: I am considering two Recurrent Neural Networks one for each problem."	80	0	0	0
2793	-1	0	0	2	b"So I've been trying to understand neural networks ever since I came across Adam Geitgey's blog on machine learning. I've read as much as I can on the subject (that I can grasp) and believe I understand all the broad concepts and some of the workings (despite being very weak in maths), neurons, synapses, weights, cost functions, backpropagation etc. However, I've not been able to figure out how to translate real world problems into a neural network solution. Case in point, Adam Geitgey gives as an example usage, a house price prediction system where given a data set containing No. of bedrooms, Sq. feet, Neighborhood and Sale price you can train a neural network to be able to predict the price of a house. However he stops short of actually implementing a possible solution in code. The closest he gets, by way of an example, is basic a function demonstrating how you'd implement weights:Other resources seem to focus more heavily on the maths and the only basic code example I could find that I understand (i.e. that isn't some all singing, all dancing image classification codebase) is an implementation that trains a neural network to be an XOR gate that deals only in 1's and 0's. So there's a gap in my knowledge that I just can't seem to bridge. If we return to the house price prediction problem, hows does one make the data suitable for feeding into a neural network? For example:No. of bedrooms: 3Sq. feet: 2000Neighborhood: NormaltownSale price: $250,000Can you just feed 3 and 2000 directly into the neural network because they are numbers? Or do you need to transform them into something else? Similarly what about the Normaltown value, that's a string, how do you go about translating it into a value a neural network can understand? Can you just pick a number, like an index, so long as it's consistent throughout the data?Most of the neural network examples I've seen the numbers passing between layers are either 0 to 1 or -1 to 1. So at the end of processing, how do you transform the output value to something usable like $185,000?I know the house price prediction example probably isn't a particularly useful problem given that it's been massively oversimplified to just three data points. But I just feel that if I could get over this hurdle and write an extremely basic app that trains using pseudo real-life data and spits out a pseudo real-life answer than I'll have broken the back of it and be able to kick on and delve further into machine learning."	430	16	0	0
2794	-1	0	0	2	b'So for a class I\'m reading Brooks\' "Intelligence without representation". The introduction is dedicated to slating Representation as a focus for AI development. I\'ve read that representation is the problem of representing information symbolically, in time for it to be useful. It\'s related to the reasoning problem, which is about reasoning about symbolic information. But I don\'t feel like I really understand it at any practical level. I think the idea is that when an agent is given a problem, it must describe this problem in some internal manner that is efficient and accurately describes the problem. This can then also be used to describe the primitive actions that can be taken to reach the solution. I think this then relates to Logic Programming eg Pascal?Is my understanding of Representation correct? Just what does representation look like in practice, are there any open source codebases that might make a good example?'	150	0	0	0
2795	-1	0	0	7	b"I have been looking into Viv an artificial intelligent agent in development. Based on what I understand, this AI can generate new code and execute it based on a query from the user. What I am curious to know is how this AI is able to learn to generate code based on some query. What kind of machine learning algorithms are involved in this process? One thing I considered is breaking down a dataset of programs by step. For example:Code to take the average of 5 terms1 - Add all 5 terms together2 - Divide by 5Then I would train an algorithm to convert text to code. That is as far as I have figured out. Haven't tried anything however because i'm not sure where to start. Anybody have any ideas on how to implement Viv? Here is a demonstration of Viv."	141	0	1	0
2796	2449	0	45917	0	b"You can try to figure out what exactly does an action do using such script:action 0 and 1 seems useless, as nothing happens to the racket.action 2 &amp; 4 makes the racket go up, and action 3 &amp; 5 makes the racket go down.The interesting part is, when I run the script above for the same action(from 2 to 5) two times, I have different results. Sometimes the racket reaches the top(bottom) border, and sometimes it doesn't. I think there might be some randomness on the speed of the racket, so it might be hard to measure which type of UP(2 or 4) is faster."	104	9	0	0
2797	2793	0	60361	3	b"This is a good question which I wrestled with myself when first trying to code an ANN. Below is a good general-purpose solution, and it's the one I implemented in my code for trying to predict well-behaved numerical data. If your data is not well-behaved (i.e. fraught with outliers) then you may need to do more work normalizing the inputs and outputs. Some of the more advanced methods are described here.Note: I will assume that you are using f(x) = tanh(x) as your activation function. If you aren't, you should still be able to reason through how to normalize your data after reading this.How to prepare the input data:The basic idea is that you want a significant variation in each input parameter to be reflected by a significant variation in the activation of the neuron those inputs are being fed into. By looking at a plot of the derivative of the tanh(x) actiavtion function, you'll see that the region of significant slope is within a distance of one or two from the origin. This means that whether the input to the activation function is 2000 or 3000 (values of x for which the derivative is negligibly small), the output of the activation will be almost identical...so your neuron's state will be independent of the difference between 2000 and 3000, and your network will never produce any predictive power from values in that range. So if you want to input the square footage of the house into a neuron, you need to normalize the square footage so that the network can tell the difference between 2000 and 3000. One way to do this so that all of the significant variations in your data are 'noticed' by the neuron is to z-score-normalize the inputs. Gather all of your square footage values (from your training set) and calculate the mean and standard deviation. Store the mean and standard deviation---you'll need this information to normalize new square footage values when testing. Normalize the vector of square footage values by subtracting the mean and then dividing the result by the standard deviation (all operations element-wise of course). Subtracting the mean centers your data at the origin, and dividing by the standard deviation makes sure most of it is between -1 and 1, where the neuron's output is most sensitive to its input. This is called z-score normalization because each input value is replaced by its z-score.Do the above for each input variable.Now, when you put each input value through a neuron, the output of the neuron is an activation between -1 and 1 (look at the image of tanh(x)). Since this is already in the 'sensitive' range of the activation function, you don't need to worry about altering the output of the input-layer neurons before sending them to the first hidden layer. Just give any hidden layer neurons the outputs of the previous layer directly---they will be able to handle them just fine. When you reach the last layer (the output neuron(s)), what you get is again another activation between -1 and 1. You have to convert this back into a value for the house in question, whether that value will be used as a prediction in a test set or to calculate error during training. However you do this, you just have to be consistent and use the same de-normalization procedure in training and testing. One way to think about it is: when the output neuron(s) returns 1, that means the network is returning the maximum possible house value as its prediction. What should the highest value the network can estimate be? The right approach here simply depends on your application. This is what I did:Calculate the mean of [the/each] output variable and store it.Calculate the maximum deviation of the output variable from the mean. Python: MaxDev = max([abs(DataPoint-numpy.mean(TrainingData)) for DataPoint in TrainingData])When the network returns output(s) between -1 and 1, multiply the output by MaxDev and add it to the mean.Two basic quick checks you can do to see if your normalization-renormalization scheme is suitable (these are necessary, but perhaps not sufficient conditions):If all the input values are average (e.g. average no. of bedrooms, average sq.feet, etc), is the network's output equal to the average of the output variable (e.g. house value) as well? (It should be.)If all the input values are unusually high/low, is the network's output unusually high/low as well? (This only works if all the inputs are positively related to the output...if some of them are inversely related related, you will have to think a bit more).Observe that the scheme presented here satisfies these two conditions.Notice that this scheme would allow your network to only predict house values inside the range of house values in your training data set. Depending on the application, this behavior can be desirable or undesirable.For example: you may want to make it impossible for your network to predict negative house values. Think about how you would do this. De-normalize the output so that -1 is mapped to 0.If you want to set no limit on the values your network can predict, then you can run the network's output through a function that maps the [-1,1] range to all real numbers...like arctanh(x)! As long as you do this during training your network will adjust its weights to accommodate this.I hope this was helpful. Let me know if you have further questions. My ANN module is in Python, by the way, so I might have language-specific advice."	909	0	0	0
2798	2794	1	69807	0	b"First, Pascal is not a logic programming language. Logic programming refers to languages like Prolog where you have a declarative style of programming compared to a imperative style like you have in Pascal. Maybe you mean if-statements which are typical for imperative languages.Second, representation means a certain level of abstraction. For instance, a model represents a certain part of the reality. Imagine a cup on a table. If the agent has a representation of this situation, it has a symbol table and a symbol cup which represents the things in the real world. Now it can have a relation on(cup, table) which represents the situation that the cup is on the table. This type of abstraction can be easily represented in a logic language like first order logic. Therefore, one uses logic programming languages like Prolog or other types of languages like OWL to represent knowledge and perform reasoning. So the important term to which Brooks refers is Knowledge Representation and Reasoning.Third, if your agent only have sensor data like video or sonar data, then it knows only distances or pixels from the real world. That is not meant with representation. Brooks' Creatures have only this information and calculate with this data directly to perform an action without reasoning. In that sense also artificial neural networks have no representation.Finally, for an open source project to understand representation I would recommend the above mentioned OWL. You can look at the Prot\xc3\xa9g\xc3\xa9 editor for working with OWL. In an OWL ontology you can represent relations between things and reason about them."	258	0	0	0
2800	2231	0	27724	2	b"Well,lets get this clear;Intelligent vision and geostalt processing,both these two terms are in the scope of computer vision.So here I would like to give a glimpse of what computer vision is,inline with artificial intelligence;simply because computer vision is broad when it comes to application;robot vision under this you can check it here as well;Image Processing,Machine Vision and Pattern Recognition and Machine Learning!Computer Vision[The general Term]Let me call it parent;hint: Humans use their eyes and their brains to see and visually sense the world around them. Computer vision is the science that aims to give a similar, if not better, capability to a machine or computer/software program[intelligent Agent].Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.Robot Vision &amp; Machine Vision(intelligent vision)Robot Vision involves using a combination of camera hardware and computer algorithms to allow robots to process visual data from the world. For example, your system could have a 2D camera which detects an object for the robot to pick up. A more complex example might be to use a 3D stereo camera to guide a robot to mount wheels onto a moving vehicle.Just like Google's self driving Car. And remember robot vision is closely related to Machine Vision(though Machine Vision refers to the industrial use of vision for automatic inspection, process control).Without Robot Vision, your robot is essentially blind. This is not a problem for many robotic tasks, but for some applications Robot Vision is useful or even essential.Pattern Recognition and Machine LearningWhere it starts to get a little more complex is when we include Pattern Recognition into the family tree, or more broadly Machine Learning. This branch of the family is focused on recognising patterns in data, which is quite important for many of the more advanced functions required of Robot Vision. For example, to be able to recognise an object from its image, the software must be able to detect if the object it sees is similar to previous objects. Machine Learning, therefore, is another parent of Computer Vision alongside Signal Processing/image processing.However, not all Computer Vision techniques require Machine Learning. You can also use Machine Learning on signals which are not images. In practice, the two domains are often combined like this: Computer Vision technique can detect features and information from an image, which are then used as an input to the Machine Learning algorithms. For example, Computer Vision technique detects the size and color of parts on a conveyor belt, then Machine Learning decides if those parts are faulty based on its learned knowledge about what a good part should look like.Are such methods/techniques being used or worked on today? Jeff Dean, Google Senior Fellow and head of Brain team that just put TensorFlow machine learning library into open source, gave the opening day keynote on deep learning, a powerful class of machine learning that allows machines to understand what they are seeing. These algorithms are trained by exposing them to huge amounts of data.For example. 1000\xe2\x80\x99s of tagged pictures of a car or people or animals, so they learn to recognize an unknown similar image very accurately, even if the object is somewhat obscured. They are now even able to interpret context. It\xe2\x80\x99s not just a picture of a \xe2\x80\x9cbaby\xe2\x80\x9d, but the algorithm comes back with \xe2\x80\x9cA baby is asleep next to a teddy bear\xe2\x80\x9d. it's therefore,quite impressive.So,according to such scenario;you can analyse critically and guess what technique is applied! and real progress is made on this;don't forget that these are millions invested in these projects.Gestalt Processing,inline with it's application(research with progress)Advertisers are using the basic gestalt process in the television medium focus on how the viewer responds to the entire message for instance the sounds, colors and distinct images seen in the commercial. The response varies from viewer to viewer, depending on age, emotional background, physical condition, level of education and social class. How the individual responds is his gestalt. Depending on the product or service, advertisers target the message to specific viewer demographics, who share common gestalt responses.Similarity PrincipleWhen conducting an ad campaign, marketers use the gestalt processing when reducing the product to a basic design theme, logo or slogan. Think of famous ad lines or instantly-recognizable symbols, such as McDonald's golden arches. That symbol evokes hamburgers and french fries to anyone remotely familiar with advertising, although the logo itself does not display food. Advertisers strive for individuality when marketing products, so the viewer or customer doesn't confuse their product with a competitor's item. However, under gestalt principles, customers won't confuse completely different products, such as cars and food, or clothes and electronics.PerceptionIn advertising, perception is reality. Advertisers using gestalt principles must consider how the message they use is perceived by potential customers and how this message prompts action -- buying the product or service. Media and advertising professionals use gestalt theory to create effective ways to sell products whether using images or conceiving of the most beneficial forms of distribution. Figuring the overall success rate of using gestalt principles is also based in simplicity -- sales results.Therefore;according to such scenario,companies like Amazoon,IBM,Google to mention but a few a have applied all the above methods or techniques and are still booming."	886	0	0	0
2801	-1	0	0	0	b'I randomly generate millions groups of triplet {x0, x1, x2} within range (0,1), then calculate the corresponding coefficients of the polynomial (x-x0)(x-x1)(x-x2), which result in triplet groups normalized in a form of {(x0+x1+x2)/3, \\sqrt{(x0x1+x1x2+x0x2)/3}, \\sqrt[3]{x0x1x2}}; After that, I feed the coefficient triplets in to a 5-layered neural network {3,4,5,4,3}, in which all the activation function is set to sigmoid and the learning rate is set to 0.1;However, I only get a very poor cross validation, around 20%. How can I fix this?'	81	0	0	0
2803	-1	0	0	1	b'If we look at state of the art accuracy on the UCF101 data set, it is around 93% whereas for the HMDB51 data set it is around 66%. I looked at both the data sets and both contain videos of similar lengths. I was wondering if anyone could give an intuition as to why HMDB51 data set has been harder.'	59	0	0	0
2804	-1	0	0	2	b"We are working on a project for creating music based on crowd sourcing. People vote for every note until the vote is closed, and then move on to the next vote until the canvas for the music is filled. A similar project is crowdsound, if you want to get an idea of what it looks like.Now the fun part is, based on all the votes we get from various people, we would like to be able to build a Neural Network that can build an entire song on its own. The idea is for it to take in account every preceding vote and predict the one that will follow. That way, when trained, we could give it one note and let it predict the rest of the votes on its own and thus create a song on its own.So I've read a few things here and there about neural networks, but there are two things I don't understand:How to build one that takes into account a dynamic number of inputs (all preceding votes).How exactly should I decide the number of hidden layers (I still only vaguely understand what those hidden layers represent) I need for it to work well.We are using Java for the project and we were planning on using Neuroph for the neural network."	215	0	0	0
2805	2795	0	44935	2	b'I was looking into genetic algorithms and things of that sort when I came acrossed this video. The guy in the video seems to know what hes doing i watched like 20 minutes of it. Interesting stuff and he goes step by step with everything.https://vimeo.com/52539994'	44	0	0	0
2806	-1	0	0	3	b'I have users reports about an accident. I want to know how to make sure that the number of reports is big enough to take that accident as a true accident and not a spam.My idea is to consider a minimum number of reports in a specific time interval, for example 4 reports in 20 minutes are good enough to believe the existence of that accident.My question is how can I choose the minimum number of reports and that time interval? Is there another logic to take that decision?I will appreciate your answers.'	92	0	0	0
2807	2806	0	78307	0	b"It's a trust-level problem, so your judgment is the best to decide what would be rhe threshold. You can help your decision making by trying and visualize how many accident (in %) you've left out... this can be an indicator of good threshold. You don't want to throw too many of them. But only you know what is good and bad in this case"	63	0	0	0
2808	-1	0	0	4	b"I want to create a network to predict the break up of poetry lines. The program would receive as input an unbroken poem, and would output the poem broken into lines.example:How should I go about this? I have been using classifiers for various tasks, but this seems to be a different type of task.I'm thinking of it as an array of words (doesn't matter how they're represented for now) which would look like [6, 32, 60, 203, 40, 50, 60, 230 ...] and needs to map into an array representing line breaks [0, 0, 1, 0, 0, 0, 1, 0, 0, 1 ...] where 1 (at optimal) means there should be a line break after the word in that index. (in this idea, the two arrays are of the same length). Unfortunately, I couldn't find an algorithm that could train a network of this shape.What machine learning or deep learning algorithm can be used for this task?"	156	12	0	0
2810	-1	0	0	2	b"I'm here to ask you for a solution on this problem which is: how to use Reinforcement Learning in Immersive Virtual Reality to make a person move to a specific location in a virtual environment. As you know reinforcement Learning is a sub-area of Machine Learning in which an active entity called an agent interacts with its environment and learns how to act in order to achieve a pre-determined goal. The Reinforcement Learning had no prior model of behaviour and the participants no prior knowledge that their task was to move to and stay in a specific place. The participants were placed in a virtual environment where they had to avoid collisions with virtual projectiles. Following each projectile the agent analysed the movement made by the participant to determine paths of future projectiles in order to increase the chance of driving participants to the goal position and make them stay there as long as possible.Update 1: Download: Reinforcement Learning as a tool to make people move to a speci\xef\xac\x81c location in Immersive Virtual Reality"	173	0	1	0
2811	-1	0	0	1	b'In a Neural network, there is an input layer, any number of hidden layers, and an output layer. My question is: Are the input and output layer nodes actually perceptions? Or do they just signify what/how many/where the inputs and outputs are? '	42	0	0	0
2812	2811	0	10117	2	b"Not 100% sure I got your question right, but yes the input nodes are perception fields. For the output it's good if you choose a space such that the different outputs you expect are independent.One method to improve neural networks is actually using receptive fields. Look for convolutionary networks and pooling.Would have posted that to a comment if I could."	59	0	0	0
2813	111	0	57012	1	b'The only sensible choice is to use predictable behaviour. So in the people in front of the car scenario: first hit the brakes, same time horn, and stay on course. The ppl then have a chance to jump out of the way leading to zero ppl being killed. Also with full brakes (going from 50km per hour to zero is less than 3 car length) an impact situation is almost not imaginable. Even if fullstop cannot be reached, severe damage of the pedestrians is unlike.The other scenario is just crazy. So the distance has to be less that 3 car length, at least 1 car length in needed for the steering, then a car crashing is an uncontrollable situation, might leed to spinning and kill all 11 persons.Apart from saying that I dont believe there is an example in reality where there is a dilemma; the solution in these unlikely cases if to conform with the expectations of the opposing party to allow the other party to mitigate the situation as well.'	171	0	0	0
2814	111	0	35573	1	b"I think there would not be a way to edit such ethics settings in a car. But hey, if cell phones can be rooted, why not cars? I imagine there'll be linux builds in the future for specific models that will let you do whatever you want.As for who'll make such decisions- it'll be much like privacy issues of today. There'll be a a tug-of war on the blanket by the OS providers (who'll try to set it to a minimum amount of people injured, each with it's own methods), insurance companies (who'll try to make you pay more for OS's that will be statistically shown to damage your car easier) and car manufacturers (who'll want you to trash your car as soon as you can, so you'll buy a new one; or make cars that require a ridiculous amount of $$$ service). Then some whistleblower will come out and expose a piece of code that chooses to kill young children over adults- because it will have a harder time distinguishing them from animals, and will take chances to save who it'll more surely recognize as humans. The OS manufacturer will get a head-slap from the public and a new consensus will be found. Whistleblowers will come out from insurance companies and car manufacturers too. Humanity will grab a hot frying pan and burn itself and then learn to put on gloves beforehand. My advice would be just make sure you won't be that hand- stay away from them for a couple of years until all the early mistakes are made."	260	0	0	0
2816	2517	0	39977	2	b'I think you mean the leave nodes only which are changed. The other nodes in the tree are calculated during calculating the best move with this tree. The values at the leaves are called utility values in Russel and Norvig\'s "Artificial intelligence: a modern approach". Some times it is called heuristic value; see https://en.wikipedia.org/wiki/Minimax.'	53	0	1	0
2817	-1	0	0	2	b'I have been trying to reproduce the experiments done in the original: "Firefly Algorithm for multimodal optimization" (linked in the question) so far: unsuccesfully. For the moment being I\'m okay if anyone point me to the right direction.I wrote the algorithm as specified in the paper in C++ programming languaje (I also downloaded several other implementations from internet for comparation purpouses) and used the very same parameters as specified in the paper (a random steep of 0.2, an initial light intensity of 1.0 and a light decay coefficient of 1.0, a population size of 40). I used the two bright update ecuations given and for De Jung test function (as for example) a number of dimensions of 256 in a search domain in [-5.12, 5.12] as refered in common optimization literature and in paper.In the paper the algorithm converges very quickly, as can be expected since this is a very simple test function, however, neither my implementation nor any code I have downloaded converges with that parameters.My final questions are:Am I doing something wrong with the experimental methodology or am I using wrong parameter settings (may be something different than the original paper)?Do anyone knows where can I find a code sample of Firefly Algorithm that I can use to reproduce the experiments of the mentioned paper?Please notice that there may be a lot of variations of this algorithm that can produce better results, but right now I\'m only intrested in reproduce the experiments of the so-called paper. '	248	0	0	0
2818	2517	0	35342	0	b'In reinforcement learning, you can keep the name of value as it comes from the value function which estimates how good it is to be in this node w.r.t. the objective. Depending on the problem it can be a cost, utility, reward...'	41	0	0	0
2819	2808	0	74603	1	b'The underlying problem is combinatorial, as you note, but I\'m not getting how you\'re ascribing value to words.The key element of deciding line breaks, beyond the visual, is rhythmic. (There are other factors, as Bob Salita notes, but you\'ve gotta start somewhere.) It seems to me you need to teach the computer how to scan a phrase in the poetic sense, which relates to rhythm. This is obviously a very difficult task, but the number of syllables and stresses is fundamental numerical data of poetry.In order to validate to human tastes, you\'d then have to use a captcha crowdsourcing method, for both the rhythmic stresses as input, and getting human reactions to different line-break configurations. You would then reinforce the positive reactions, and the AI would tailor the line-break process to the audience.However, instead of utilizing human tastes and aesthetic sensibilities, you could instead let the AI decide what is preferred, which would probably be comprised of some sort of symmetry considered optimal to an algorithm.Following this logic, you wouldn\'t even need to have the AI learn the stresses, instead just focusing on raw syllables, or, numeric representation based on any factor. (With this method, the object is not to reformat poetry for humans, but for machines :) This is more about the aesthetics, but Cameron Browne\'s Elegance in Game Design would seem to suggest there are engineering solutions to the type of aesthetic issues at the root of your problem.I might start by teaching it to count the syllables of the poem, then having it look at the divisors. If it\'s roughly 10, it might be iambic pentameter. The AI doesn\'t care about the label, but it likes 10. 20 syllables might represent a couplet in that meter: The time is out of joint, oh cursed spite  that ever I was born to set it rightI\'d definitely start by feeding it older poetry, particularly poets that keep to strict meter. It\'s been a while since I\'ve read Spencer and so forth, but I\'d think poets of his time would be useful. Dr. Seuss, perhaps the greatest wielder of the rhyming couplet, would surely be extraordinarily useful. The evaluation method would have to be fuzzy, because there would be increasing degrees of variance the more modern the poetry, ultimately resulting in free structures, except in the case of forms such as rap, which strongly utilize regularized rhythm. Machine learning is all about estimation and reinforcement, and is proving to be extremely useful dealing with fuzziness. Dead mountain mouth of carious teeth that cannot spitis a great example of modern line of poetry: the floor of 13 syllables / 2 makes a 6 beat line. Understanding that in context with the surrounding verse is much more difficult and illustrates the nature of the problem. Even scanning the poem correctly to that point to determine would be extremely difficult. However, a different poem by the same author is extremely useful: What is the late November doing / With the disturbance of the spring / And creatures of the summer heat, / And snowdrops writhing under feet / And hollyhocks that aim too high / Red into grey and tumble down / Late roses filled with early snow? / Thunder rolled by the rolling stars / Simulates triumphal cars / Deployed in constellated wars / Scorpion fights against the sun / Until the Sun and Moon go down / Comets weep and Leonids fly / Hunt the heavens and the plains / Whirled in a vortex that shall bring / The world to that destructive fire / Which burns before the ice-cap reignsAll lines of roughly 8 syllables, easy to pick out because of capitalization. But the real question is: ~136 13 lines of roughly 10 syllables, or 17 lines of roughly 8? It would want to calculate based on word blocks (words that cross syllabic thresholds and at least tell you where the break cannot be, and it should be possible to statistically divine the pattern, at least for regularized verse.)  The wounded surgeon plies the steel / That questions the distempered part; / Beneath the bleeding hands we feel / The sharp compassion of the healer\'s art  / Resolving the enigma of the fever chart.This verse highlight the problem. 5 lines of 4 beats, but syllabically: 8, 8, 8, 10, 12. Most likely: 46/5 = 9.246/4 = 11.546/6 = 7.66Less likely:46/3 = 15.346/2 = 23 46/7 = 6.572 lines has less perfect symmetry, but 5 lines is more likely, based on the overall number of syllables, and of the likely choices, has the least variance. Ultimately it would be looking for the underlying structure, or lack of structure, and try to reorganize the unbroken text into something close to the original structure. While exactness is not always required because the process is ultimately subjective, and currently intractable, certain wrong choices would yield disastrous results. In the prior example it may be able to discern the likelihood of a 5 line pattern, but it would have to figure out on which lines to place the extra syllables. Differentiating between particles and other parts of speech provides a clue, because the poet\'s language is very compact: there are 19 nouns, verbs, or prepositions. More likely: - 19/5 = 3.8 - 19/4 = 4.75Less likely: - 19/3 = 6.33 - 19/6 = 3.16 - 19/7 = 2.71Further analysis might narrow it down. But extremely regularized verse remains the best place to start. 7 lines of roughly 10 syllables is "poetic": \xe2\x80\x98The aged man that coffers-up his gold Is plagu\xe2\x80\x99d with cramps and gouts and painful fits; And scarce hath eyes his treasure to behold, But like still-pining Tantalus he sits, And useless barns the harvest of his wits; Having no other pleasure of his gain But torment that it cannot cure his pain.It cares about both X and Y values.Initially you want to keep it to a single language, because syllables may be treated differently. That said, having the AI look for something like Dactylic hexameter would be extremely useful, because you could feed it Homer. You could also feed it Homer in English in many different forms of English meter, and in almost every other living language. By definition, the AI would value works such as these, because max number_of_translations provides the most robust data set. When it starts to value meaning, this will be especially important.Understanding different ways of treating syllables(long/short vs. stressed/unstressed) will also be essential as it transitions into more modern poetry. Here is a good link for basic English meter. Iambic and Trochaic meters will be easy, while meters that employ Anapests, Dactyls and Spondees will be more challenging. In some cases, however, these will be mathematically interchangeable.  I went to the Garden of Love, / And saw what I never had seen: / A Chapel was built in the midst, / Where I used to play on the green. It doesn\'t matter if the lines above are iambic/trochaic or dactylic/anapestic, it\'s still 4 lines of roughly 8 syllables. Thus "I went to the Garden of Love" is the same as "The wounded surgeon plies the steel", even though the beats for the lines are 3 and 4, respectively.It should also have a stanza marker, (possibly 00?). Because it looks for patterns within patterns, stanzas are valued. Not all poetry has a stanza structure, but it arguably could. Deciding if stanzas are appropriate is partly a function of taking a syllabic divisor, breaking the poem down into number_of_lines, and looking at the divisors of that number. It would need an added function to be able to recognize meaning patterns. For instance, repetition of proper nouns is the marker of plays. (From a meaning perspective, imo, plays is the ideal place to start because the marker is so easy to learn, and names all belong to a single set, and imply communication. It\'s no different functionally than any other identifier, and a concept all computers "understand".)Eventually it would want to look for phonetic patterns, rhymes and near rhymes, which would also be indicators of potential good places for line breaks.There is a very big data set that it can look at, and who knows what it might discern?'	1371	0	2	0
2820	-1	0	0	4	b'Everything related to Deep Learning (DL) and deep(er) networks seems "successful", at least progressing very fast, and cultivating the belief that AGI is at reach. This is popular imagination. DL is a tremendous tool to tackle so many problems, including the creation of AGIs. It is not enough, though. A tool is a necessary ingredient, but often insufficient.Leading figures in the domain are looking elsewhere to make progress. This report/claim gathers links to statements by Yoshua Bengio, Yann LeCun and Geoff Hinton. The report also explains: The main weaknesses of DL (as I see them) are: reliance on the simplest possible model neurons (\xe2\x80\x9ccartoonish\xe2\x80\x9d as LeCun calls them); use of ideas from 19th century Statistical Mechanics and Statistics, which are the basis of energy functions and log-likelihood methods; and the combination of these in techniques like backprop and stochastic gradient descent, leading to a very limited regime of application (offline, mostly batched, supervised learning), requiring highly-talented practitioners (aka \xe2\x80\x9cStochastic Graduate Descent\xe2\x80\x9d), large amounts of expensive labelled training data and computational power. While great for huge companies who can lure or buy the talent and deploy unlimited resources to gather data and crunch it, DL is simply neither accessible nor useful to the majority of us.Although interesting and relevant, such kind of explanation does not really address the gist of the problem: What is lacking?The question seems broad, but it may be by lack of a simple answer. Is there a way to pin-point what DL is lacking for an AGI ?'	250	0	0	0
2821	2820	0	10590	1	b"I think it's missing still the aspects what makes a human brain; having a lot of different networks working with each other. Just like meditation improves cognitive abilities by having the brain work more synergistically, we could apply that to machines too. For example google is learning a computer to dream, just like we do, to reinforce what we already learned. https://medium.com/@tannistho/why-is-google-teaching-its-ai-to-dream-e9ae9ecd0e3a#.gljal6pwwAnd here is pathnet, a network of neural network. https://medium.com/@thoszymkowiak/deepmind-just-published-a-mind-blowing-paper-pathnet-f72b1ed38d46#.ed0f6pdq7Creating all these mechanics and putting them all together, with enough power and we will get pretty close!"	87	0	0	0
2824	-1	0	0	1	b"I am trying to understand the algorithm for n-step Sarsa from Sutton/Barto (2nd Edition, p. 157, PDF) As I understand it, this algorithm should update n state action values, but I cannot see where it is 'propagated backwards' (sorry for the wrong terminology, but I couldn't find something better). Probably, I am not seeing the forrest for all the trees?"	59	0	1	0
2826	-1	0	0	0	b'Short version of this question: where in the OpenAI Gym docs can you find more information about an environment, like what each of the variables in an observation means, and so on.As per their docs (https://gym.openai.com/docs), you can get the state space as follows:The problem is, these just look like random numbers in an array.Using CartPole-v0 as an example, the bounds are given as:It seems intuitive after reading some papers about the inverted pendulum, that the state is typically represented by a 4-tuple of (angle, angular speed, horizontal displacement, horizontal speed).This also suggests that the observation space bounds are actually meant to represent:Low: (-pi, -inf, x_min, -inf)High: (+pi, +inf, x_max, +inf)The magnitude of the 2nd and 4th lows/highs seem to suggest that they do indeed represent angular speed and horizontal speed.But why does the 1st low/high not correspond to -/+pi?Where can more information be found about what these numbers actually represent?'	150	6	0	0
2828	2794	0	40393	0	b'In that paper, Brooks introduced the basis for what became known as his "subsumption architecture". The idea was to get away from the 1980\'s popular approach of a single global representation of all the components of the problem space that had required the task of robot task planning to juggle every constraint in the world into one giant disordered mess of states and state transitions. Rather than represent every element in the world in a single model (The Representation), Brooks suggested it was preferable to build a hierarchy of submodels of the world (subsets of states and transitions) in which smaller tasks could be more readily planned. Then as these rudimentary skills were mastered, they could be combined to address a hierarchy of bigger and more complex tasks (bigger tasks subsume smaller tasks and benefit from their already having been solved).Yes, representation did not fully go away, but it was redistributed hierarchically so that much of the state could be abstracted away from the higher level of the bigger problem that you need to solve. Planning became like coordinating a hierarchical army of skills, where the general doesn\'t need to plan every movement of the private in order to manage a battle. Instead, that general need only tell the colonels what to do, and the colonels tell the majors, and so on down to the privates. Now the general solves problems by coodinating multiple sub-hierarchies available to him/her by delegating authority to coordinate behavior at the appropriate level of abstraction: like division, brigade, battalion, company, and squad. That\'s Brooks\' Subsumption Architecure: the general needs to represent a battle plan only as "the world according to colonels". https://en.wikipedia.org/wiki/Subsumption_architecture'	276	0	0	0
2829	2820	0	2611	5	b'Everyone dealing with neural networks misses an important point when comparing systems with human like intelligence. A human takes many months to do anything intelligible, let alone being able to solve problems where adult humans can barely manage. That and the size of human brain is enormous compared to our neural networks. Direction might be right, but the scale is way off. Number of neurons in human brain can be matched memory-wise but the amount of parallelism to simulate it real-time cannot yet be achieved (at least for a random researcher). While a little old this might give you an idea of how much we lack the processing power. '	109	0	0	0
2830	2820	0	20268	3	b'Deep Learning is mostly successful in supervised learning, whereas the brain builds categories mostly in an unsupervised way. We don\'t yet know how to do that. (Take a look at google brain: 16,000 cores and all this thing can do is recognise cats and human faces with pretty abysmal accuracy.)Deep Learning uses highly unstructured activations, i.e. the high level representations of "dog" and "cat" in a neural network classifier don\'t have to be similar at all. The brain on the other hand uses inhibitory neurons to create sparse distributed representations which are decomposable into their semantic aspects. That\'s probably important for abstraction and reasoning by analogy. The brain has many different parts which work together. Deep Learning researchers are only just beginning to integrate memory or attention mechanisms into their architecture. The brain integrates information from many different senses. Most Deep Learning applications use just one type of input, like text or pictures. The brain is capable of modelling sequences as categories. (Basically every verb names a sequential (i.e. temporal) category.) It can then arrange these categories into long-term hierarchical plans. So far I haven\'t seen anything in that direction in Deep Learning.Also neural networks can\'t yet operate on the same scale as the human brain. If you look at the answers to this question, the human brain will be ahead in neuron count for another couple of decades. A neural network might not need the same number of neurons as the brain to reach a similar performance (because of higher accuracy), but right now for example video processing is still pretty limited in terms of input and throughput. '	269	0	2	0
2831	2808	1	24207	4	b'You should try to use an RNN. You feed in letter by letter and have a binary output of linebreak - no linebreak. If you have enough data it might actually work. '	32	0	1	0
2832	2772	0	44699	3	b'LEGO Mindstorms is widely used to demonstrate AI in schools and universities [1, 2]. With LEGO as basis you are very flexible. You can build what you want very easily. The AI programs can be written in different languages from very easy graphical once to Lisp and C++. The newest version has an SD Card drive, USB interface and a powerful ARM processor. You can use four motors and four sensors directly. There exists touch, sound, sonar, gyro, infrared and colour sensors. There is also a big community which provides you with lot of ideas, hardware and programs [3].'	98	0	1	0
2833	-1	0	0	4	b"How does in the (famous Zilberstein) PR(uning) algorithm below the LP-dominate function get started: the first time it's called, D=\xe2\x88\x85 and the linear program deteriorates (i.e. no constraint equations)? Some explanation much appreciated! Thanks."	33	29	0	0
2834	-1	0	0	3	b'I want to know something more about it. Are there any github repo or an open source project?'	17	0	0	0
2835	2820	0	54740	0	b'Artificial intelligence proponents today are focused on the problem of computability - the ability to solve complex problems fast. It is my belief that any amount of success in this direction will not lead to human (general) intelligence although it certainly will outperform humans in certain domains. Instead, efforts should be toward a study of what neurological events cause sensation (the experience of qualia). Of course, this is the hard problem of philosophy but I believe it is the unique key to general intelligence and its capabilities. Reverse engineering and also testable theories should be advanced toward this end. '	99	0	0	0
2836	2834	0	45410	0	b"Ya Sure it is possible. It wont be efficient as human speech though (At least not yet). It all depends on how you use your data. If you use your data efficient enough, then you could be the one creating an AI which closely resembles human speech. Your idea is good. You would need help from a GPU for processor all that complex text processing. I hope I was at least a little of help. I unfortunately don't know about any open source projects."	83	0	0	0
2837	-1	0	0	3	b"If the nervous system is wired up such that there are no well defined layers, how does this compare to a neatly stacked artificial net? If between my sensory and motor side I had a neatly designed SNN with well defined layers, how would I see the world?I get that there are some evolutionary advantages to a system where information can sometimes take a shortcut from sensory cell to motor cell (reflex action) bypassing brain processing but for arguments sake let's talk only about intelligence."	84	0	0	0
2838	2834	1	62035	1	b"It is possibleThe Recurrent Neural Network architectures help in building efficient NLP algorithms, which can identify semantics and their relations across long pieces of text.With very minor tuning, they can be made generative too. So, here is an excellent article on RNNs which I highly recommend, which also talks about how an RNN was trained on Shakespere's texts and wrote one itself."	61	0	1	0
2839	2833	0	42326	0	b'I think I found the solution. When in PR(W), D=\xe2\x88\x85, the weight is:b[i] = 0 for { i | w[i]&lt;max(w) }, andb[i] = 1.0/max(w) for { i | w[i]==max(w) }. '	30	0	0	0
2841	-1	0	0	6	b'From Artificial Intelligence: A Modern Approach, Third Edition...In Chapter 26, the textbook discussed "technological singularity". It quotes I.J. Good, who wrote in 1965: Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultrainteltigent machine could design even better machines; there would then unquestionably be an "intelligence explosion," and the intelligence of man would be left far behind. Thus the first ultraintelligeat machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.Later on in the textbook, you have this question: 26.7 - I. J. Good claims that intelligence is the most important quality, and that building ultraintelligent machines will change everything. A sentient cheetah counters that "Actually speed is more important; if we could build ultrafast machines, that would change everything" and a sentient elephant claims "You\'re both wrong; what we need is ultrastrong machines," What do you think of these arguments?It seems that the textbook question is an implicit argument against I.J. Good. Good may be treating intelligence as valuable, simply because man\'s strengths lies in that trait called "intelligence". But other traits could be equally valued instead (speed or strength) and sentient beings may speculate wildly about their preferred traits being "maximized" by some machine or another.This makes me wonder whether a singularity could occur if we had built machines that were not maximizing intelligence, but instead maximizing some other trait (a machine that is always increasing its strength, or a machine that is always increasing its speed). These types of machines can be just as transformative - ultrafast machines may solve problems quickly due to "brute force", and ultrastrong machines can use its raw power for a variety of physical tasks. Perhaps a ultra-X machine can\'t build another ultra-X machine (as I.J. Good treated the design of machines as an intellectual activity), but a continually self-improving machine would still leave its creators far behind and force its creators to be dependent on it.So, let\'s repeat my question -- Are technological singularities limited to ultra-intelligences? Or technological singularities be caused by machines that are not "strong AI" but are still "ultra"-optimizers?'	382	0	0	0
2842	-1	0	0	0	b'Is Programming Collective Intelligence by Toby Segaran a good book to enter in the AI and neural networks world for a novice? '	22	0	0	0
2843	2842	0	9628	0	b'I read the second chapter in that book for my project on recommendation systems. It is an awesome book and it starts by addressing concepts from the basics( At least chapter 2 was like that). Book explains concepts with python. You will need a bit of knowledge on python basics . The book transits slowly from beginner to an intermediate level. Hope this answers your question. I would recommend it.'	69	0	0	0
2844	2841	1	58868	4	b"That would be a no for speed or strength, if you have a super strong entity but it cannot research new materials, it will be quickly limited, same thing for speed, Basically, you need something out of their field to improve them, which makes a runaway improvement impossible. Though, we already have super strong and super fast machines, those are cars, trucks, hydraulic presses, industrial exoskeletons etc... But, even though we can build better ones through the use of the old ones, we still need to research stuff that can't be improved by old ones.What we need for a singularity is a field where an improvement in it makes further improvement easier. And I don't know a field where this doesn't involve intelligence. If there is one, that may be possible to have a non intelligence driven singularity there."	138	0	0	0
2845	2772	0	20195	0	b'Why dont you try android Phones with Tensorflow TensorFlow Android Camera Demo You can build a simple image or text classification neural network to demonstrate AI. '	26	0	0	0
2846	-1	0	0	2	b'AI is developing at a rapid pace and is becoming very sophisticated. One aspect will include the methods of interaction between AI and humans. Currently the interaction is an elementary interaction of voice and visual text or images.Is there current research on more elaborate multisensory interactions?'	45	0	0	0
2847	2841	0	24620	0	b'"Monte Carlo" seems to be the best method currently for algorithmic creativity. (i.e. the machine makes random choices and sees if they lead to anything useful.)While it appears obvious that creative connections formed out of understanding are superior to those which are random, if the machine is fast enough, it should be able to win out by pure "brute force".i.e. Evolution, prior to human guidance, has not been been based on intelligence.* Rather, evolution has been based on random mutations that are either beneficial or detrimental. *The caveat is that humans creating algorithms and altering genes (either in a lab or through animal husbandry and horticulture) can be said to comprise a new form of evolution that is actually rooted in human intelligence and desire.'	124	0	0	0
2848	2846	0	26565	0	b'Probably these days it\'s still under the umbrella of "man-machine interaction" in CS, i.e. there is a (sub-) field for interactions between humans and machines in CS, but I am not aware that it has split again to create a sub-sub-field for AI/man interactions. '	44	0	0	0
2849	2834	0	39059	0	b'It depends whether you include learning to produce sound waves in the task (I\'m taking "how to speak" to be different from "how to write" in your question).If the task is to learn a language, you can certainly learn from written texts, and try a generative approach (usually, after a few sentences, humans can be disappointed though). If the task includes generating a voice response that is also learnt, then it had better hear some spoken language too. Of course, you can bypass the task of learning how to generate sound by plugging a standard text to voice module after your generator. '	102	0	0	0
2850	2820	0	63987	0	b"IMHO the first hurdle is scale: even Google's largest DNN doesn't come close to the scale of the brain, and by a factor of several orders of magnitude... "	28	0	0	0
2851	-1	0	0	0	b'I tried the below Matlab code to build SOM using selforgmap.You find the result and more details here.I need to segment grayscale image. However, I cannot set the selforgmap input correctly.How can modify the below code to segment any grayscale image?'	40	36	1	0
2852	2834	0	62254	0	b"Yes, It is possible. Also, if you are from development background you can use JAVA Sphinx, some AI tools and API's. If you are from testing background you can use JAWS which actually reads the readable properties of HTML tags. Nowadays, there are some pdf readers, document reader softwares available you can have a look at them. You can search for Sphinx related projects but I don't know about the other projects.Please vote and mark the solution if useful.Thanks!"	78	0	0	0
2853	-1	0	0	1	b"I am trying to develop an Editor that can be based on Notepad. The only purpose for this development is I want to use this for my coding suggestions and possibly the next input parameter that I am going to write. I've seen Notepad++, EditPlus etc. and what I think is that this can be definitely achieved. What are the best tools or API's I can use? Any suggestions?Thanks in Advance!"	70	0	0	0
2854	-1	0	0	0	b"Is it possible to run SSD or YOLO object detection on raspberry pi 3 for live object detection (2/4frames x second)?I've tried this SSD implementation but it takes 14 s per frame.Is there anything I could do to speed up ?"	40	0	0	0
2855	2772	0	1707	0	b'To start you could use one of the devices mentioned before, and after to make some more powerful and complicated experiments (and also a little bit more expansive) you could move to Jetson TK1 which let you run heavier Neural Network (like CNN).'	42	0	1	0
2857	2808	0	42053	0	b'I have a suggestion for generating poetry:grab the major arcana of the tarotgrab a database of books on diverse literary topics, and group them by gestaltdrop 4 or 5 random words building a sequence of the archetypes, and getting words that you associated with the cardsgrab expressions from the books you have in the database containing these expressionsfind a way to derive and link these expressionsapply the procedure recursively on top of several poems until you have very large collections of poems or even books'	84	0	0	0
2858	2771	0	60062	3	b'"Life" is a definition humans use to classify objects according to the types of behavior humans perceive as unique to living creatures.Scientists and philosophers tend to define something as "alive" if it manifests some specific properties found in living organisms, such as self-replication, adaptation to the environment, homeostasis and capability to exploit matter and energy for its own existence and functioning. With that being said, there is no one accepted definition of life, and it is doubtful that such a definition is possible.As to ALife, [1] states that "it is common among researchers to distinguish between two types of approaches to artificial life: (1) the strong ALife approach, which postulates that virtual \xe2\x80\x9ccreatures\xe2\x80\x9d on a computer screen can be considered to be alive if they fulfill the definition of life used by the researchers; and (2) the weak ALife approach, whereby computerized creatures displaying characteristics of living systems are only models used in research and are not really alive."While this is not solving the problem of definition of life, it might give more context on the subject in relation to ALife.[1] E. Lamm and R. Unger, Biological Computation. Chapman &amp; Hall/CRC, 2011.'	191	0	0	0
2859	2837	1	20283	1	b'The human nervous system is an extremely dynamic entity, having been formed by the processes of embryogenesis, mediated by various markers guiding neurons to grow to specific areas, and all of it laid down by millions of years of evolution. There are many different varieties of neurons in the human brain, indeed more so than pretty much any other animal on this planet.It is hard to see that a spiking neural network (SNN) would be able to get all these details correct when we don\'t yet fully understand the biological analog.I think for this to be successful, we need to understand much more about not only the variety of neurons in the brain, but details of the the embryogenic dynamics as well. And not just the neurons, but the glial cells as well.Having said that, there is something to be said for using evolutionary approaches to resolve "a solution" that may work anyway. Taking this approach will require many more resources, but is perhaps doable. All in all, I would not expect a naive attempt of the neuromorphic SNN to succeed. There is a lot of complexity involved in what the brain does, and it involves the glial cells to a large degree. Do we understand enough about the role of glial cells in the brain? We cannot ignore them. Are they only performing "housekeeping" operations, like, for example, the uptake of "spent" neurotransmitters? Or are they doing more, taking part in the computational and / or memory aspects of the brain?There is much research in this and other pertinent areas that one should look into. And expect a lot of surprises.'	271	0	0	0
2860	2803	0	717	0	b'It is true that at first look, one could expect that classification between 101 category would be harder than classification between 51 category. However, many aspects play a role when it comes to action recognition applications.For instance, the HMDB51 contains several categories about different facial mouvements like smiling, laughing, chewing,... and several other categories like eating, drinking. Such categories are not present in the list of categories of the data set UCF101 and are obviously among the most difficult categories to deal with. It also claims to have some bad quality challenging videos.It is hard to predict in advance how difficult a data set will be to classify. We can imagine that when the state-of-the-art reaches accuracy beyond 90%, it is time to build a data set that makes these methods fail to look for even more robust solutions. I don\'t know these data sets well, but the videos are present most probably more variability in terms of viewpoint, camera motions, illumination changes, image quality,... in the most difficult to classify data set.Also, check on this page, the results announced for the UCF101 data set. I don\'t know where you found you accuracy value because the official website announces less than 43.9%. Some publications do not use the complete data set and use only part of it to show the performance of an approach they designed. Finally the official website of the HMDB51 data set reports the following:"The UCF group has also been collecting action datasets, mostly from YouTube. There are UCF Sports featuring 9 types of sports and a total of 182 clips, UCF YouTube containing 11 action classes, and UCF50 contains 50 actions classes. We will show in the paper that videos from YouTube could be very biased by low-level features, meaning low-level features (i.e., color and gist) are more discriminative than mid-level fears (i.e., motion and shape)."This could also explain why better results can be achieved...'	318	0	2	0
2861	-1	0	0	1	b'TL;DRIf we buy into the idea visual cortex functions like a convolutional neural network, then there\'s a problem makes me scratch my head: how does brain force weight sharing as in convolutional network?Okay, explain moreObviously, there\'s no way for left visual cortex to directly tell the right visual cortex "hey, I\'ve learned some new stuff, copy me!!" (or is there?). Then, if the learned features are diverse across visual field, how does it keep the translation invariance property?For example, you already know English characters, you can recognize them with your both eyes. Now that you wanna learn some Chinese and you excercise your right brain at the same time, so you closed your right eye and memorized a new character. After that, certainly you can recognize the new character with solely your right eye. But why?The answer may be, the object / higher-level feature detection happens in a higher level cortex, which receives entire visual field. There may be also some transfer/one-shot learning taking place. But then, if a newborn baby trying to learn the low level visual features, he/she would definitely face the weight sharing problems.A possible explanation would be, the baby will be exposed to very large amount of data and eventually learn invariance. Large amount of data reduces overfitting but doesn\'t guarantee deterministic convergence. If we train the same CNN model on the same dataset, however using different random generator seeds, there\'s a big chance the same feature detector will appear in a different channel, or a difference set of features appear as linear recombination.If there\'s no way to share weights, the brain would learn a lot different feature combinations across the entire visual field, how does it still able to consistently solve visual invariance problem?'	288	0	0	0
2862	2861	0	18248	3	b"In the human brain every common pattern is recognised by a multitude of pattern recognisers (be they neurons or microcolumns). That's pretty obvious because neurons die all the time, but we don't wake up and have forgotten how to recognise the letter 'A'. In fact we have to take out rather big chunks of the neocortex until functionality degrades, which is why Alzheimer's patients show symptoms only when the brain is already visibly messed up. Translational invariance within a level of the hierarchy has to be learned. Basically you need filters for the same edge all over your V1. An object moving across your field of vision only becomes invariant in it's representation on a higher level. Unfortunately we don't magically learn a representation that we can easily turn and twist, our ability to recognise objects from an unusual angle degrades with how uncommon the angle is. A nice thought experiment to illustrate the point: Imagine a cube sitting in front of you. Now pull one of the corners of the cube upwards until the cube is dangling in front of you, one corner pointing straight to the ceiling, the opposite corner pointing to the ground. Now indicate with your finger where the rest of the corner are. If you are anything like me, this is really difficult. I think the first time I did't even realise I had to point out six corners! Of course this is 3D and we might still have inbuilt 2D invariance, but it also turns out that faces that are turned upside down have to be processed much higher into the cortex to be recognised as faces and so on ... Concerning the learning of a lot of different feature vectors across the field of vision: This might be prevented by the fact that the neocortex learns sequences of input by predicting (via depolarisation) the next pattern. So you might have the situation that a higher level tells the lower level that there is an object moving from left to right and the lower level will predict that the edge it is detecting at point y will reoccur at point y+x.This setup differs from the training of NNs in two ways: The data is already translational and the translation is predicted, which facilitates learning the same features. Basically two pattern recogniser close to each other get pretty much the same pattern one after the other whenever something moves in a certain direction and the second occurrence is predicted, which means it will be more likely detected, which means it will be learned. (I don't want to dole out a lecture about the cortex, but the prediction is a big deal because it allows neurons to fire quicker which means they beat other neurons before they are laterally inhibited and only if they actually fire will they learn.)As a disclaimer I want to add that this is just my current understanding of the issue and I'm pretty sure the actual explanation is more complicated. For example I've read that one of the layers of the cortex is important for translational invariance and this layer only exists in the levels close to the sensory input. It is conjectured that this layer (L4) doesn't do the sequence prediction stuff, so maybe having the same kind of input is enough or learning different feature vectors is not actually a problem. There is also the complicating issue that there is poorly understood interplay between the different layers and different levels of the cortex. I would recommend to ask a neuroscientist, except I don't think they figured this out yet. "	600	0	0	0
2863	-1	0	0	2	b"I have to translate the following English sentences into First-Order Logic without using quantifiers:I have tried it, but can't translate without using \xe2\x88\x80 and \xe2\x88\x83:Is it possible to do it. If not, why?Refer chapter 8 of Artificial Intelligence: A Modern Approach (3rd edition). Stuart Russell and Peter Norvig, Prentice Hall (2010)"	50	8	0	0
2864	-1	0	0	1	b'The Wikipedia states that: "An evaluation function, also known as a heuristic evaluation function or static evaluation function, is a function used by game-playing programs to estimate the value or goodness of a position in the minimax and related algorithms."  https://en.wikipedia.org/wiki/Evaluation_functionQ: Is "goodness" an actual term in use in this context, or should it more properly be something like "perceived optimality"?I ask because, in Combinatorial Game Theory for instance, a lighthearted term such as "loopy" is preferred by some mathematicians (Demaine) over the more serious term "cyclic" (Fraenkel).On a related note, is the use of "position" instead of "node" preferred here as an acknowledgement of the heuristic nature of Evaluation Functions? (My understanding is that "position", "node" and "game" may all be interchangeable in certain contexts.) '	127	0	0	0
2865	-1	0	0	1	b'Here is formula for calculating cost value of single neuron:C_x = (1/2) * || y - a ||^2why there is 1/2?'	20	0	0	0
2866	2865	0	2154	2	b'To simplify the derivative, probably. Otherwise there will be constant 2 in it. '	13	0	0	0
2867	-1	0	0	6	b'I read that deep neural networks can be relatively easily fooled (link) to give high confidence in recognition of synthetic/artificial images that are completely (or at least mostly) out of the confidence subject.Personally I dont really see a big problem with DNN giving high confidence to those synthetic/artificial images but I think giving high confidence for white noise (link) may be a problem since this is a truly natural phenomena that may the camera see in real world.How much of a problem is white noise for the real world usage of a DNN? Can such false positives detected from plain noise be prevented somehow?'	103	0	2	0
2868	-1	0	0	3	b"I identify myself as a human agent. It is time to think about oncoming senior research and due to small experience in gamedev(as well as in AI field), some questions are raised. What are the most suitable approaches to implement real-time simple AI agent in an action game? I've heard something about cognitive architecture like ACT-R.By design, entity's AI can have several mutually exclusive states. This is an existing AI of game, which has states, events and schedules. However, the code is complicated and not flexible. Also, it does not use any cognitive architecture, which I consider as a drawback.https://www.youtube.com/watch?v=9jO-P3kXlCIPlease, using your experience suggest any modern techniques, which can copy such behaviour as in image or video.Thank you for your perception."	120	0	0	0
2869	2867	0	49985	8	b"The white noise that fools DNNs isn't really white noise. It has been altered in the same way as the synthetic misclassified pictures have been altered. You have to change many input pixels in exactly such a way, that these little changes aren't perceptible, but propagated through the network add up to a misclassification. This is not going to happen by chance. "	62	0	0	0
2870	-1	0	0	2	b"This question has come from my experiment of building a cnn based tic-tac-toe game that I'm using as a beginner machine learning project. The game works purely on policy networks, more specifically -During training, at the end of each game, it trains itself on the moves the winner/drawer made for each board position. That is, its training data consists of board positions and the moves made by the winning player on each position.While playing, it predicts its own moves solely based on that training (that is, it predicts what move would a winning player make with the current board). It doesn't use any type of search or value networks.I'm seeing that if I train it against a player that predicts the perfect move (using a recursive search) every time, the AI gets good at drawing about 50% games. But if I train it against a player that makes random moves, it doesn't get better at all.Wouldn't one expect it to learn well (even if slower) regardless of the level of its opponent? Since each game ends in a draw or win for one player, shouldn't it be able to extract features for the winning/drawing strategies even when learning from random players? Or does this behavior mean that the model is not optimal?"	211	0	0	0
2871	-1	0	0	6	b'This mostly refers to human-like or chatbot AI, but could maybe be used in other applications (math or something?). Basically, it occurred to me, that when I\'m thinking or speaking, there is a constant feedback loop, in which I am formulating which words to use next, which sentences to form, and which concepts to explore, based on my most recent statements and the flow of the dialogue or monologue. I\'m not just responding to outside stimulus but also to myself. In other words, I am usually maintaining a train of thought.Can AI be made capable of this? If so, has it been demonstrated? And to what extent? While typing this, I discovered the term "thought vectors", and I think it might be related. If I read correctly, thought vectors have something to do with allowing AI to store or identify the relationships between different concepts; and if I had to guess, I\'d say that if an AI lacks a strong understanding of the relationships between concepts, then it would be impossible for it to maintain a coherent train of thought. Would that be a correct assumption?(ps. in my limited experience with AI chatbots, they seem to be either completely scripted, or otherwise random and often incoherent, which is what leads me to believe that they do not maintain a train of thought)'	221	0	0	0
2872	-1	0	0	4	b'At first, I had this question in mind "Can robots develop suffering ?". Because suffering is important for human beings. Imagine that you are running the wrong way damaging your heel. Without pain, you will continue to harm it. Same for robots. But then I told myself "wait a second. It already exists. It is the errors and warnings that shows up". We can say it has the similar purpose as suffering. However, I felt something missing. We feel pain. The errors and bugs are just data. Let\'s say a robot can use machine learning and genetic programming to evolve. Can it learn to feel suffering ? And not just know it as mere information.'	114	0	0	0
2874	-1	0	0	4	b"I occasionally read papers that show neural networks solving traveling salesmen problems and multi traveling salesmen problems efficiently? 1) Is there any analysis of the meaning of efficiency of algorithms for networks that allowed to grow in size with the problem they are supposed to solve?2) What are the earliest papers solving the TSP with NN this?3) Is the meaning of efficiency used in these papers is the same as the usual one, in fact, and works only in this problem specifically?COMMENTSThese problems are NP hard. So I suspect I'm not sure what these papers mean by efficient.The neural network postulated have a sufficiently vast number of interacting elements and in effect do the combinatorics strictly, for each special case. But if so, while this is fast and doesn't grow much with the size of the problem growing, is this really comparable to the normal meaning of PT as fast or efficient?In these cases it seems the time efficiency is obtained by resource inefficiency: by making the network enormous and simulating all the possible worlds then maximizing. So, while time to compute doesn't grow much as the problem grows, the size of the physical computer grows enormously for larger problems; how fast it computes is then, it seems to me, not a good measure of efficiency of the algorithm in the common meaning of efficiency. In this case the resources themselves only grow as fast as the problem size, but what explodes is the number of connections that must be built. If we go from 1000 to 2000 neurons to solve a problem twice as large and requiring exponentially as much time to solve, the algorithms requiring only twice as many neurons to solve in PT seem efficient, but really, there is still an enormous increase in connections and coefficients that need be built for this to work.Is my above reasoning incorrect?"	311	0	0	0
2875	-1	0	0	6	b'For rules please refer to https://www.hackerrank.com/challenges/ultimate-ttt .I have implemented minimax search with alpha-beta pruning. There is a time limit of 15 seconds.Which algorithms would yield better results? '	27	0	0	0
2876	-1	0	0	6	b'My understanding of the singularity is when artificial intelligence becomes "more intelligence" than humans.This will be achieved through machine learning where an; algorithm, neural network ? Exponential betters itself. So from that point on in near future after that we should predict that there will be artificial intelligence capable of answering any question. How to travel the fastest...Blueprints for spacecrafts...Drugs for medicine...Efficiency and advancements that will change the human condition.The singularity is predicted 2040s or 2030. All be it a couple of years later down to exponential growth in knowledge.So if what I\'m saying is right I should be seeing crazy hype and news coverage as well as advancements but I don\'t. I don\'t understand what is wrong with the idea that the AI will be capable of omniscience. So can it ?Is there something preventing it ?I don\'t see how so logically.As my philosophy has been that research in all the scientific fields are long and expensive. The prospect of a AI that could perform research at fractional cost and time is the way to go. I hope to work in a field that works at achieving singularity and so will in turn change the world. With the ideas and discoveries it will have. And where does "artificial" consciousness come in to play in the singularity '	217	0	0	0
2877	2872	1	36721	5	b'At a very high level, regarding evolutionary game theory and genetic algorithms, it is absolutely possible that AI could develop a state that is analogous with suffering, although, as you astutely point out, it would involve conditions which a computer cares about. (For instance, it might develop a feeling analogous to "being aggrieved" over non-optimality in the algorithmic sense, or "frustration" at equations don\'t add up, or "dissatisfaction" over goals that have not been achieved.)The robot tormented by small children at the mall can certainly be said to be "suffering" in that the children block the performance of the robot\'s function, but the robot is not conscious and suffering might be said to require awareness. However, even without consciousness, this very simple robot can learn new behaviors through which it mitigates or avoids the "suffering" brought on by not being able to fulfill its function.You definitely want to look into the concept of suffering in a philosophical context and Epicurus would be a very useful place to start. Epicurus is directly relevant in an algorithmic sense because he uses the term "ataraxia" meaning calm, and is derived from the verb "tarasso" which means to agitate or disturb.Ataraxia can be mathematically expressed as an equilibrium. Tarasso can be mathematically expressed as disequilibrium. This relates directly to Game Theory in that disequilibrium can be said to be the primary requirements of games, and to AI in that Game Theory can be said to be the at root of all AI.Ataraxia is also understood in the sense of "freedom from fear", which in temporal in that fear is a function of uncertainty as it relates to the future in a predictive sense, and involves current condition vs. possible, less optimal future conditions. Thus fear, which is a form of suffering, is rooted in computational intractability, even where the "computer" is is a human brain.Early philosophers such as Democritus are especially useful because they were exploring critical, fundamental concepts, many of which can now be expressed with modern mathematics. To wit: you can\'t arrive at suffering until you first define "the Good" and "the Bad", which is a binary relationship in which neither term can be said to have meaning without the opposite. (Mathematically, it can be expressed in its simplest form as a finite, one dimensional graph.) This understanding is quite ancient. It is worth noting that the continuing value of the early philosophers is partly a factor of wisdom not being a dependent of volume of knowledge, demonstrated by Socrates in the idea that wisdom may be as simple as knowing you don\'t know something. The ancient sages didn\'t have the benefit of powerful measurement tools, advanced mathematics, or scientific method, but they were very smart, and even more importantly, wise. '	459	0	3	0
2878	2876	0	2547	4	b'I quite like your outlook, and without getting into the details of how a "singularity" may be effected which is covered in numerous other questions, or how consciousness and "omniscience" come into play because consciousness and omniscience are not requirements, I will instead direct you to two key philosophers:Phillip K. Dick, for whom the central theme in his famous 1968 book on AI is empathy. (If you haven\'t read it, I\'m not posting a spoiler, but will only say the plot is driven by the concept of Evolutionary Game Theory which was formalized just 5 years later.) John Nash, and in particular, the concept of the Nash Equilibrium. (Nash could be said to have mathematically demonstrated that being a "douchebag" is not an optimal strategy. His proof can be used to explain why nuclear d\xc3\xa9tente actually worked, which was counter to the expectation of Von Neumann.) So when people go nuts, focusing on the "Skynet" mythos under which machines rise up to destroy us, I have to wonder if they\'re simply not as smart as Nash or as profound as Dick, which might explain their lack of emphasis on what can be called the "Electric Sheep" paradigm.'	196	0	0	0
2880	-1	0	0	0	b'I ask this on Stack AI, because the implication is not that people who work at think tanks, for the most part, are not nearly as smart as they pretend to be, (which may very well be true, but difficult to quantify,) but instead related to modeling and predictive capability.The old chestnut when explaining why imposed regulation doesn\'t work is pointing to examples collectivism in the Soviet Union which had disastrous results. But computers sucked during that era, at least as compared to today. (It is telling that weather forecasts, which used to be regarded in society as a joke, and were rooted in Almanacs, is now highly accurate, and keeps improving.) It is difficult to believe degree of dis-equilibrium that often occurs in poorly regulated, minimally regulated, or unregulated is optimal, except to certain actors focused on aggregation of wealth primarily though exploitation of Pareto efficiencies (which carries profound, negative moral implications,) in that these actors, who often create the conditions for extreme dis-equilibrium are often set up to profit further from the "reset". (It\'s is not even convincing that these actors actually believe that self-regulation of markets is optimal, because, in the case of the recent 2008 event, they opted to be bailed out instead of choosing to be subjected to un-mitigated market forces.)But an AI, with sufficient data and processing power, should be able to regulate markets much more optimally than "self-regulating markets" because it can actively balance the system instead of relying on "natural correction" of imbalance, which is often catastrophic.Economies are quite complex, but a hallmark of current AI is the effective management of intractability, which can be rephrased as increasingly optimal decision making in a condition of uncertainty.I am not suggesting that current AI is "smart" enough to regulate economies today, but is it rational to assume that this capability is not outside the potential of future systems? '	314	0	0	0
2881	2864	0	22858	2	b'Yes, "goodness" is a common description of the value generated by an evaluation function.For example,"Artificial Intelligence" p. 77;"Knowledge-Free and Learning-Based Methods in Intelligent Game Playing" p. 15;"Tenth Scandinavian Conference on Artificial Intelligence" p. 125; and"Algorithms and Networking for Computer Games" p. 80.The term "position" refers to the position of the pieces on a game board at some instant.'	57	0	0	0
2882	2871	0	77439	1	b'First, for almost every question of the form "Can AI be made to X", the most obvious and straightforward answer is something like "We don\'t know. Probably, but if it hasn\'t been done yet, we\'re really not sure."It\'s also important to understand that, from a technology standpoint, there isn\'t one "thing" called "AI". There are many, many different technologies, which are loosely related (at best) and are generally lumped together under the overall rubric of "Artificial Intelligence".All of that said, yes, there has been work on adding memory, even long-term memory, to various kinds of "AI". The most notable recent example is the advent of LSTM in recurrent neural networks. Additionally, some of the work done on "cognitive architectures" has focused on the use of memory. For more info on that, look up ACT-R and/or SOAR and read some of those papers. What isn\'t clear to me offhand, is whether or not anybody has tried applying any of these techniques to chat-bots in particular. I wouldn\'t be surprised if somebody had, but I can\'t cite any such research off the top of my head.'	183	0	0	0
2883	2806	0	1957	0	b"If the only feature you're classifying on is the number of users making a given report, then this isn't really much to do with AI/ML. Just pick a number based on your subjective judgment and go with it.OTOH, if you can include details of the report itself (as well as the number of reporters), I think you might be able to build a bayesian classifier that would be useful. If you could consider location, weather, time of day, number of reporters, etc., it seems like you might be able to get something useful put together. "	95	0	0	0
2885	-1	0	0	1	b'I have labeled time windows of data from different sensors(gyroscope, accelerator, magnetometer). For all sensor data, the format is:For a classification purpose, how do I represent the sensor data? How exactly do I make meaningful feature extractions of the raw sensor data so it can be used for classification?Does a mean of a given time window makes sense?The purpose and goal is to classify activities from these time windows of sensor data.Thanks in advance'	73	1	0	0
2886	-1	0	0	5	b'I\'m doing a little tic-tac-toe project to learn neural networks and machine learning (beginner level). I\'ve written a MLP based program that plays with other search based programs and trains with the data generated from the games. The training and evaluation are strictly policy based - Inputs are board positions and outputs are one-hot encoded array that represents the recommended move for that board position. I\'ve not added search algorithms so that I can understand what to expect from a purely MLP approach.The MLP model has 35 features and 1 hidden layer and after a few hundred thousands games it has sort of learned to draw 50% games. It has learned the basic stuff like how to block the player from winning and some good board placements.Now, my question is - It hasn\'t learned advanced strategies that require making a move that may not be as beneficial for the current move but will improve its chances later. But should I expect that from a strictly policy MLP based no-search approach? Since all that it is being trained on is one board and the next recommended move (even if thousands of those pairs), is it logical to expect it to learn a lookahead approach that goes beyond "the best move for the current board" training? Put another way, would it be a possible at all for a MLP to learn lookahead without any search strategies? If not, are there any alternatives that can do it without search?'	245	0	0	0
2887	2876	1	35993	5	b"Note: This looks a little bit detailed answer to the question,I wanted to give an insightful scenarios.The Singularity is the point in time when computers will be more intelligent,more able, and more creative than humans. At that point there will be a sharp bend in the technology curve, since super-intelligent computers will be able to develop new technologies exponentially faster than humans, including technologies to make themselves faster. After that, they will essentially be running the world.Hint on the future of Civilization; civilization has a specific set of ideas and customs, and a certain set of manufactures,science and arts that make it unique,with in a complex System.Therefore, Civilizations tend to develop intricate cultures, including a state-based decision making apparatus.Special thanks to ancient Egyptians CivilizationArtificial intelligence (AI) is the name of these technologies. AI technologies are under development in every country of the world, and they are the technologies that will bring about the Singularity. AI technologies are improving rapidly; in fact, 2015 was a breakthrough year for AI,with advances of all kinds. These different advances are separate events now, but within a few years they'll merge into the first super-intelligent computers and robots that will lead to the Singularity. Many analysts consider 2015 to have been a breakthrough year for Artificial Intelligence (AI), not because of any single achievement, but because of achievements across the board in so many different areas.Companies like Google, Facebook and Microsoft are now operating their own AI labs.In areas such as image recognition, computer vision, face recognition, voice recognition and natural language processing, there are a wealth of new products (think of Siri or OK Google) that are becoming increasingly reliable and increasingly available.Several companies are testing self-driving cars, and they\xe2\x80\x99re expected to be available commercially by 2020. Robots in the military are becoming more common, from robots on wheels to pilotless drone warplanes. All of these robots still require constant human intervention and control, but they\xe2\x80\x99re slowly migrating away from human control to algorithmically based decision making and control. Robot form factors are improving, with some robots looking almost human.Confusion from various sourcesGlimpse:Artificial Intelligence and Climate ChangePoliticians and climate change activists like to say that the claims about climate change have been endorsed by 95% of all the scientists in the world. This claim is a total fraud, because it confuses two things.First, we have the claims by science that the earth is warming because of human activity. Arguably, that has been proven by scientists. But that\xe2\x80\x99s all.The second part,is predictions about the future, which are mostly total crap, and certainly not science. In fact, climate change scientists have been making predictions for 25 years, and they\xe2\x80\x99ve almost completely turned out to be wrong. The truth is that scientists who claim to know what the earth\xe2\x80\x99s temperature will be in 2100 can\xe2\x80\x99t even predict what the weather will be next month.During my lifetime in artificial Intelligence field and being passionate about, I\xe2\x80\x99ve read number of hysterical environment disaster predictions articles,written by different philosophers across the globe.My favorite is the prediction in 1970 by far left-wing Ramparts Magazine that predicted that the oceans were becoming so polluted that by 1980 the world\xe2\x80\x99s oceans would be covered by a layer of algae. It didn\xe2\x80\x99t happen.One way to know that the climate change activists are wrong is that these climate change activists never mention the Singularity or future technology.And we researchers in Artificial Intelligence,they undermine our visions which will pop out to be true.Proof that the Singularity will occur by 2030 or 2040A proof, based on reasonable assumptions, that any intelligent species on any planet in the universe will develop in a way that\xe2\x80\x99s similar to the development of humans,including following the same Generational Dynamics cycles as humans.A glimpse:lets try to time travel back in 1800s,In the late 1800s, streets in large cities were full of horses (think of a traffic jam in any large city, with horses instead of cars). These horses were producing huge volumes of urine, manure, flies and carcasses \xe2\x80\x94 not to mention cruelty to horses.By 1900, there was 1,200 metric tons of horse manure per day. There were international conferences (like today\xe2\x80\x99s climate change conferences) that accomplished nothing. But within 20 years, the problem took care of itself because of new technology \xe2\x80\x93 the automobile.History shows that new technology, including new AI technologies, will solve the \xe2\x80\x9cclimate change\xe2\x80\x9d problem, and that politicians will have absolutely nothing to do with it, except to take the credit when something works, and to blame someone else otherwise.Now lets come back from 1800s;Early in 2005, the Pentagon announced the Army's Future Combat Systems (FCS). By 2014, just a few years from now, America will be deploying thousands of computerized soldiers that will have the ability to decide on their own to kill people (hopefully, the enemy). In early implementations, kills will be directed wirelessly by human overseers, but as millions of these are deployed, overseeing them will become increasingly impossible. By 2025, super-intelligent computerized robots manufactured in countries around the world will be fighting major battles. By 2030, super-intelligent computers will be running the world without our help.Fiction or Facts from Science Fiction Movies:Robot from I, RobotThis is quite a different view of intelligent robots than the one in the movie I, Robot. It came out in summer, 2004, and it portrays a world in 2035 when super-intelligent robots are being manufactured for domestic home use. These robots are designed to be unable to harm human beings, but the story line is about a rogue robot who may be violating that rule. In the end, Will Smith conquers the malevolent robots and gets the girl, and everyone lives happily ever after. Software algorithms that will bring singularity;Intelligence isn\xe2\x80\x99t some magical, mystical force. It\xe2\x80\x99s actually the ability to find new ways to combine previous experiences in new ways. A new discovery is made by combining old discoveries in new ways, in the same way that jigsaw puzzle pieces can be put together.A computer can do the same thing by combining \xe2\x80\x9cknowledge bits\xe2\x80\x9d (KBs) in new ways, to learn new things, in the same way that jigsaw puzzle pieces can be combined. Computers can do this much faster than humans can.Decisions can be made by using the same \xe2\x80\x9cminimax algorithm\xe2\x80\x9d that\xe2\x80\x99s used to implement games like chess.This algorithm would work today, except that computers aren\xe2\x80\x99t yet fast enough. The speed of computers doubles every 18 months, and by 2030 computers will be fast enough to implement this IC algorithm.Another proof based on reasonable assumptions;Every intelligent species in the universe must follow the same Generational Dynamics cycles as humans.. is outlined as follows:For any species (including humans) to survive, the population growth rate must be greater than the food supply growth rate. This is what I call \xe2\x80\x9cThe Malthus Effect,\xe2\x80\x9d based on the 1798 book by Thomas Roberts Malthus, Essay on Population.Therefore, for any species, there must be cyclical periods of extermination. This can be accomplished in several ways, such as war, predator, famine or disease. But one way or another, it has to happen.Non-intelligent species will simply starve and die quietly when there\xe2\x80\x99s insufficient food. But intelligent species will form identity groups and hold riots and protests, and eventually go to war. These will be the cyclic crisis wars of extermination specified by Generational Dynamics and every intelligent species in the universe will have them.If this can give you a glimpse,then just know Artificial Intelligence ain't no joke.....Lets study it hard...so that we see such a future.Special Thanks to StackExchange"	1251	0	1	0
2889	2886	1	35834	4	b"A MLP only does pattern recognition, it will not learn search. Tictactoe, (Oughts and Crosses), is such a simple game that your network should learn the moves from the training data by heart, no generalisation required. If it still loses games, maybe your training data doesn't consist of particularly good moves. "	51	0	0	0
2890	-1	0	0	5	b'Today we have neural network based AI players that are comparable or better than humans in games that require extensive pattern matching and "intuition". AlphaGo is a prime example. But these AI players usually have both neural networks and search algorithms in place. Humans, on the other hand, rely just on the pattern matching and "intuition" (even the best chess players can see just a handful of moves ahead). So, why do AI players still require extensive search while humans don\'t? How would AIs like AlphaGo perform if we take the search part out?'	93	0	0	0
2891	-1	0	0	5	b'I am going to develop an open-domain Natural Language Question Answering (NL QA) system, and will use the Support Vector Machine (SVM) as the machine-learning (ML) algorithm for question classification.The data on hand,is from a cube, containing multiple dimensions, of which some contain hierarchies.I do not understand how to work/combine the taxonomy and SVM for question classification. If I understand correctly, the taxonomy still needs to be developed by hand, unless an existing one is being used. And the SVM sorts the queried NL question based on this taxonomy?Is this correct, or am I mixing the whole concept?'	97	0	0	0
2894	2890	0	13338	5	b"It is not very accurate to say that AI players requires extensive search while humans don't. Rather, it is a question of degree. AIs do a lot more calculating, because that's what computers are good at. Human intuition is much more powerful than a neural network can currently hope to match, because it is much more integrated into a world of knowledge about the game, it uses orders of magnitudes more neurons and it is not a static thing that just provides a move.But if a human player stops calculating ahead his playing strength will drop very significantly. This can be seen by assessing the performance in games with short time controls: The less time you have the less calculation is happening, your intuition however is fast.If you take out the search component of AlphaGo it would still play quite strongly, probably at a low dan level. Of course that is also far below its strength. So, search is always an important component of playing strength, just more so for machines."	170	0	0	0
2895	2871	0	29277	0	b'It could be said that "maintaining a thought" is a basic requirement of computing, and can be represented as a string of binary digits in the context of a Turing Machine."Basically, it occurred to me, that when I\'m thinking or speaking, there is a constant feedback loop, in which I am formulating which words to use next, which sentences to form, and which concepts to explore, based on my most recent statements and the flow of the dialogue or monologue. I\'m not just responding to outside stimulus but also to myself. In other words, I am usually maintaining a train of thought."This sounds an awful lot like a recursive function.My analysis of the chatbot problem is that it reveals a poor quality reasoning on the part of the bots, as opposed to lack of reasoning. It\'s not so much a question on the raw ability of an algorithm to maintain a train of thought, because the "train of though" is the function itself, but the quality of the algorithm and, by some measures, the "humanness" of the output.'	177	0	0	0
2896	2868	0	51229	1	b"About 15 years ago, John Laird's group at Michigan used the Soar rule-based architecture to play several FPS games effectively (Quake II, Descent III):Here's Laird's overview article from 'Computer':https://www.researchgate.net/profile/John_Laird/publication/2955463_Laird_JE_Using_a_computer_game_to_develop_advanced_AI_Computer_34_70-75/links/54d0f59a0cf20323c21a1bd7/Laird-JE-Using-a-computer-game-to-develop-advanced-AI-Computer-34-70-75.pdf"	28	0	1	0
2897	-1	0	0	1	b'A lot of experts have expressed concerns about evil super intelligence. While their concerns are valid, is it necessary, what are the chances or how the artificial super-intelligence will evolve to have selfishness and self protecting desires inherent in biological systems? Is there any work which comments on this line of inquiry?'	51	0	0	0
2898	2897	0	6567	3	b'AI will only "evolve" selfishness if it "evolves" in a competitive environment and has certain human-like faculties.Self-protecting desires on the other hand are logical consequences of having any goal at all. After all, you can\'t reach your goal if you are destroyed. The concern of "evil" super intelligences isn\'t that they literally turn evil and selfish and cruel. Those are human qualities.Instead a superintelligence that has a certain goal, will logically pursue subgoals that help it to reach the ultimate goal. Such subgoals will be resources, power, safety. So it will amass power and resources to reach its goal and exterminate any threat to its existence as long as its goal hasn\'t been reached, without being selfish at all.'	118	0	0	0
2899	2897	0	25342	1	b'I have some comments on this subject here: This is some deep game theoretic stuff, and it partly depends on how you define "selfishness". There is such a thing, for instance, as a "greedy algorithm". Sometimes a greedy algorithm is the most convenient way to achieve an acceptable result, but the optimality is only local. On a mathematical level, the constructiveness or destructiveness of "self interest" in a system may be a function of whether a Nash Equilibrium is perceived. In this case, self-interest is defined as maintaining the current strategy, because, unless the competitor changes their strategy, there is no gain for changing one\'s own strategy.As the BlindKungFuMaster importantly notes, competitiveness will evolve as a trait if the AI operates in a partisan context. There, the problem comes from whether the "game" is zero sum (Pareto Optimal) or non zero sum (Pareto Improvable), or both. Here, "destructive" may defined agents made worse off per the gains made by another agent. Altruism seems to have a rational basis, and occurs in evolution because it is presumably useful. [See Biological Altruism.]Although this tends to be confined to single species, the co-evolution of dogs and humans is a case for inter-species altruism, based on self interest.Humans and machines have also, and will continue to, co-evolve.'	212	0	1	0
2900	-1	0	0	2	b'Post singularity AI will surpass human intelligence. The evolution of AI can take any direction, some of which may not be preferable for humans. Is it possible to manage the evolution of super-intelligent AI? If yes, how? One way I can think of is following. Instead of having a mobile AI like humanoid, we can keep it immobile, like a box, like current super computers. It can be used to solve problems of maths, theoretical science etc.'	76	0	0	0
2901	2900	0	13139	2	b'Assuming super-intelligence is possible, the answer is probably yes and no.Yes in Kurzweil-like scenarios, where super-intelligence is an extension of human beings by technology (we are already in to some extent). Then control follows, as super-intelligence depends on us. It would extend our capabilities, such as speed of processing, extent of processing, etc. Even then control is debatable, as a remote-controlled killing machine would be part of a super-intelligent organism, partially human "controlled", partially autonomous.No in "Future of Life Institute"-like scenarios, where super-intelligence is independent from humans. The thinking is simple: What can we hope to do facing someone way more intelligent? The usual parallel is to compare this scenario with the arrival of the "developed" conquistadors in early America. Gunpowder vs. mere raw strength and arrows.'	126	0	0	0
2902	-1	0	0	4	b'One of the argument against possibility of super-intelligent AI is that intelligence of a product will be limited by intelligence of its creator. How reasonable is this argument? '	28	0	0	0
2903	2902	0	4165	3	b'Of course the intelligence of a product is limited by the intelligence of its creator. Just not to the intelligence of its creator. That would be about as reasonable as the idea that the speed of a car is limited to the speed of its creator. Or the playing strength of a chess program to the Elo of its creator.Or the ability of a neural network to differentiate between dozens of dog breeds to the dog expertise of its creator.So, not very reasonable. '	83	0	0	0
2904	2891	0	22017	3	b"This is not an answer (I don't have enough reputation to comment). I did something close to this in my master's thesis and think it is close to what you are interested in.In it, I had developed a framework for extracting metadata from web-based educational content. This metadata was used for classifying the the educaitonal content for many different attributes, which could then be used for faster and more efficient search and discovery of educational content. The educational resources (containing the content) could be anything like text or PDFs of assignments, homework, assignments, online books, exam questions, courses etc (which many colleges host online). To identify what kind of educational resource it is, I would parse the text and look for keywords and formatting styles (preprocessing included constructing 2-grams and 3-grams, POS tagging, using small specific parsers for NER, dates and other text entities one encounters in educational content). For some part I used Wordnet (also available under python-nltk) to obtain relationships between different entities and also to find closeness between them. DBpedia was also used. However, for the most part I had to identify the most commonly occuring terms and build a taxonomy by hand. (It took a lot of time!). I obtained a lot of candidates for keywords by looking at openly available taxonomies.For extracting domain specific taxonomy/ontology, one needs manually to build it. Ontology generation from text is an active area of research and building domain-specific ontology has been tried for many years. One example of such taxonomy (here thesaurus) is agrovoc where domain experts have contributed to the knowledge by identifying agricultural entities manually. There are a lot of places where domain specific vocabulary is available; maybe you can use that. In some aspects it is close to supervised machine learning, where one has some nice data and correspondingly nice output. However, on my part, there wasn't much learning in it - more like template matching. Hope this helps."	322	0	1	0
2905	2902	0	26240	1	b'AI is frequently used to discover things that would take laborious amounts of time for humans to do. For example, AI can be used to find the optimal configuration for a mother-board layout, or identify best fit parameters for a financial model. Frequently, the AI can do a better job at a task and do it more qucikly than a human. Therefore, in many applications, the AI is already more intelligent than the creator at specific tasks. Here are just a few things that AI can already do better than humans:Playing ChessPlaying Jeopardy Detecting CancerTo argue against the possibility of a super-intelligent AI is somewhat of a moot point since it has already been proven.'	114	0	0	0
2906	-1	0	0	1	b'When trying to run tensorboard locally to show my logs with tensorboard --logdir logs/ it always shows nothing but the regular tensorboard menu options, such as orange bar at the top, and different section buttons at the top like graphics, etc. however never shows any data regarding my agents. I am using tensorflow 0.11'	53	0	0	0
2907	2900	0	12557	0	b'Without going into more detail at the moment (b/c I\'m time strapped), I strongly urge you to research the Control Problem.My own personal view is that humans are more problematic than machines. Machines are at least rational.To be more specific, I believe human "management" (read as "mis-management") of powerful AI is potentially more of a problem than super-intelligent AI left to it\'s own devices. Humans are known to abuse power, and history is filled with such examples. Machines, at least, have a clean slate in this regard.'	86	0	0	0
2908	-1	0	0	3	b'Is it possible to classify data using a genetic algorithm?For example, would it be possible to sort this database?( https://archive.ics.uci.edu/ml/datasets/Spambase )Examples in matlab?'	22	0	0	0
2909	2900	0	41550	1	b'Competition always gives better result. If machines will try to improve themselves, we as human beings will definitely try to improve ourself.'	21	0	0	0
2910	-1	0	0	1	b'I have not studied machine learning or AI really, but my job sometimes requires me to automate stuff. Right now the requirement I have, seems to be under AI domain, but I am not sure about terminologies or how to go about it. I will really appreciate if someone can guide me about the direction I need to start from.(PS: This question might not belong on this SE, in that case please direct me to suitable SE)What I\'m required to do is find references on web about a certain situation. As an example I\'ll use "Music", so I have to make a system which will search around the web (Google and Twitter mainly) to see if there is any news/mention/event related to Music that occurred today, if so how many references (i.e. how big of a deal it is making).It is not the generic term music which is expected in the output, but the names of Musicians, i.e. in Music this and this Artist appeared this many times. I have to give the number of references, and also provide the references in output so that one can read them in detail.The challenges are - One event can be covered by many websites, and there can be one main website that published the original story with full details, while others just spread the word around in summarized way. How do you filter references to pick the most suitable one, to show in results to the system\'s user, because I can not give user ~50 references to manually read through, I have to give like 1-2 suitable reference- I need to give the name of the artist. One site will have many words, how do I know which word is actually the artist\'s name? One option can be to have a pre compiled list of specific artists and just search for them individually. But this way, I can be missing new artists. The challenges I have, must have been addressed by some existing algorithm or mechanism, I\'ll appreciate if someone can let me know what kind of algo etc I need to refer to or study to get the task done.'	358	0	0	0
2911	-1	0	0	6	b'This has been niggling me a while, so I decided to ask. Sorry if it\'s wordy, I\'m not sure how to express it!It seems fairly uncontroversial to say that NN based approaches are becoming quite powerful tools in many AI areas - whether recognising and decomposing images (faces at a border, street scenes in automobiles, decision making in uncertain/complex situations or with partial data)..... almost inevitably some of those uses will develop into situations where NN based AI takes on part or all of the human burden and generally does it better than people generally do.Examples might include NN hypothetically used as steps in self driving cars, medical diagnosis, human/identity verification, circuit/design verification, dubious transaction alerting ... probably many fields in the next decade or so. Suppose this happens, and is generally seen as successful (eg it gets diagnoses right 80% to human doctors\' 65% or something, or cars with AI that includes an NN component crash 8% less than human driven cars or alternatives, or whatever...)Now - suppose one of these aberrantly and seriously does something very wrong in one case. How can one approach it? With formal logic steps one can trace a formal decision process, but with NN there may be no formal logic, especially if it gets complex enough (in a couple of decades say), there are just 20 billion neural processors and their I/O weightings and connections, it may not be possible to determine what caused some incident even if lives were lost. It also may not be possible to say more than the systems continually learn and such incidents are rare. I also haven\'t heard of any meaningful way to do a "black box" or flight recorder equivalent for NNs, (even if not used i a life critical case), that would allow us to understand and avoid a bad decision. Unlike other responses to product defects, if a NN could be trained after the event to fix one such case, it doesn\'t clearly provide the certainty we would want, that the new NN setup has fixed the problem, or hasn\'t reduced the risk and balance of other problems in so doing. It\'s just very opaque. And yet, clearly, it is mostly very valuable as an AI approach.So what\'s the answer? Is there one? In 20 years if NN is an (acknowledged as safe and successful) component in a plane flight or aircraft design, or built into a hospital system to watch for emergencies, or to spot fraud at a bank, and has as usual passed whatever regulatory and market requirements might exist and performed with a good record for years in the general marketplace, and then in one case such a system some time later plainly mis-acts on one occasion - it damgerously misreads the road, recommends life-damaging medications or blatantly missdiagnoses, or clears a blatant \xc2\xa3200m fraudulent transaction at a clearing bank that\'s only caught by chance before the money is sent - what can the manufacturer do to address public or market concerns, or to explain the incident; what do the tech team do when told by the board "how did this happen and make damn sure it\'s fixed"; what kind of meaningful logs can be kept, etc? Would society have to just accept that uncertainty and occasional wacky behaviour could be inherent (good luck with convincing society of that!)? Or is there some better way to approach logging/debugging/decision activity more suited to NNs?'	571	0	0	0
2912	-1	0	0	1	b'I\'ve been experimenting with a simple tic-tac-toe game to learn neural network programming (MLP and CNNs) with good results. I train the networks on a board positions and the best moves and the network is able to learn and correctly predict the best moves to make when it encounters those board positions.But the network is unable to "discover" newer patterns/features from existing ones. For example -Let\'s say that the board position is below and move is for the X player (AI)The recommended move would be 8 (0 based indices) so that the opponent doesn\'t win, the resulting board would be - If I train the network on the above enough times, the AI (MLP or CNN based) learns to play 8 when it encounters the above situation. But it doesn\'t recognize the below as variations (rotated and shifted, respectively but slanted straight lines in general) of the same pattern and is not able to correctly pick 6 and 0, respectively -My question is - Should I expect CNNs to be able to discover new previously untrained on patterns/features such as above? '	181	15	0	0
2913	2908	0	43218	0	b'It is possible, but is a pretty terrible idea.There are a few options. One is to not use the GA as a direct classifier, but instead use a GA to learn the parameters of another classification model like a neural network. The basic idea of a GA is that it (very roughly speaking) forms a black-box method for searching an arbitrary space for solutions that minimize or maximize some function.Here, you would be searching the space of possible neural network topologies and/or weights to find one that minimizes the misclassification rate.Another approach is that taken by what are sometimes called Learning Classifier Systems (LCS) or Genetics Based Machine Learning (GBML). This approach is to use evolutionary mechanics to evolve rule sets of the form "if X condition is true, then do/classify Y". That\'s a more direct method of solving this sort of problem. You define some features on your dataset, and the algorithm tries to learn rules based on those features.The problem with any of these approaches is just that there are so many better ways to solve the problem. Remember, a GA is basically a black-box that\'s supposed to work acceptably well for a huge range of unknown problems. But I\'m not solving a huge range of unknown problems. I\'m trying to separate ham from spam on one dataset. I can come up with methods that simply do that job better and more quickly than a GA has any real hope of doing.'	243	0	0	0
2914	2910	0	31078	0	b'A parallel situation might be that of spam/not spam. The detection of spam by AI has been pretty successful, so there is an existing algorithm - classification. However while you have a possible approach you are still missing the key ingredient which is sufficient data to train the model on.AI depends on a large amount of data to train the model. Ideally you will have a team of researchers available to read a (large?) number of sources and classify by hand whether the source is the original reference or just a repeater, and whether the topic is relevant. The more labelled and balanced data you have, the better the model and the better the results. Then, like the spam/not spam situation, you just apply your model and the best references pop out. You already have the key word of music, so you just use as candidates those sources that reference "music" and any other defining keywords.'	155	0	0	0
2915	-1	0	0	0	b"Here's a theoretical AI program. It's an API that takes percentages from various observations to draw conclusions based on probability. So, for language, it has percentages based on how often each word appears each word away from each other. To associate word meanings, it would take percentages based on every item in the room in relation to where your eyes, finger, and body were pointing. If it could read emotions, it could associate those with words as well. Then, when thinking of words to say, it would look at its emotions and environment. "	93	0	0	0
2916	2911	1	42292	4	b'If the observation that the neural network saw was recorded, then yes the prediction can be explained. There was a paper written fairly recently on this topic called why should I trust you explaining the predictions of any classifier in this paper the author described an algorithm called LIME which is able to explain any machine learning models predictions. It can be used to establish why a machine learning model made a prediction, help a data scientist debug a model, and help a data scientist improve the accuracy of a specific model. LIME can be used to explain the predictions of any neural network including CNNs, RNNs, and DNNs. '	109	0	0	0
2917	-1	0	0	1	b' Does it exist a human-like or overintelligent AI? Human-like I define as something that can act as a human in most aspects.For example, is it "common knowledge" that there actually exists an overintelligent or human-like AI? Or could you say that there do not exist an overintelligent or human-like AI?'	50	0	0	0
2918	2908	0	33415	0	b"I agree with @deong. But you must understand that a genetic algorithm is an optimization algorithm; you can't feed it e-mails and make it classify spam. A genetic algorithm is used to train 'something' to classify spam. That something could be neural networks.What you need is a genetic algorithm that optimizes neural networks Neuroevolution. But just like @deong says, there are better ways for classifying e-mails (e.g. an algorithm that looks for certain 'spam-words'). But it is definitely possible. I have a javascript library set up for Neuroevolution, if you're interested."	90	5	0	0
2919	-1	0	0	7	b'From Artificial Intelligence: A Modern Approach, Third Edition, Chapter 26: Note that the concept of ultraintelligent machines assumes that intelligence is an especially important attribute, and if you have enough of it, all problems can be solved. But we know there are limits on computability and computational complexity. If the problem of defining ultraintelligent machines (or even approximations to them) happens to fall in the class of, say, NEXPTIME-complete problems, and if there are no heuristic shortcuts, then even exponential progress in technology won\'t help\xe2\x80\x94the speed of light puts a strict upper bound on how much computing can be done; problems beyond that limit will not be solved. We still don\'t know where those upper bounds are.If the textbook\'s argument is correct, then there may be a strict upper bound to "intelligence", meaning that the potential/damage of ultra-intelligent machines is limited. However, it is contingent on there actually being a theoretical maximum for "intelligence".Is there any literature that suggest that we know for sure whether such a maximum exist? Is the existence of that maximum dependent on our definition of "intelligence" (so adopting a vague and hand-wavey definition would imply no theoretical maximum, while adopting a strict and formalized definition would imply a theoretical maximum)?Note: Question was previously posted during the definition phase of this site on Area51 by pkhlop.'	219	0	2	0
2920	-1	0	0	5	b"I am looking for a solution that I can use with identifying cars.So I have a database with images of cars. About 3-4 per car. What I want to do is upload a picture to the web of car(Picture taken with camera/phone) and then let my pc recognize the car. Example: Lets say I have these 2 pictures in my database(Mazda cx5)(I can only upload 2 links at max. atm. but you get the idea).Now I am going to upload this picture of a mazda cs5 to my web app:Now I want an AI to recognize that this picture is of an Mazda CX5 with greyish color. I have looked on the net and found 2 interesting AI's I can use:Tensorflow and Clarifai, but I don't know if these are going to work so my question to you what would be my best bet to go with here?"	147	0	0	0
2921	2920	0	2504	2	b'There are several ways you can do this. One way would involve several steps and would probably work best:Use a trained Gaussian detector to filter out the car from the rest of the imageUse a convolutional neural network to classify the carUse a neural network or a simple most common color algorithm to find the color of the carYou would be able to implement this method most easily in MATLAB but you would also be able to do it in python with tensorflow or torch. You would probably be able to implement the trained Gaussian detector in tensorflow.Method 2:Use a spatial transformer network to "transform" the image of the car for easy classificationUse the output of the spatial transformer network for classification via a convolutional neural network.Use another neural network or a most common color algorithm to find the color of the car.This method would also work pretty well but using a spatial transformer network with a convolutional neural network may be hard to code because it is an area of developing research where there are many problems because the spatial transformer network and the convolutional neural network have to work well together and this is usually hard to get right.Method 3:Use a convolutional neural network straight up on the input image maybe with down sampling to classify the carUse another neural network to find the color of the carI would personally go with method #1 because it would be fairly simple to implement with existing libraries such as tensorflow and it would most likely provide a high accuracy. As always I would also recommend that you use LIME during the development process to debug your model and determine what features you could add in or remove to help your model perform better.**Edit*Since you need to detect certain patterns on the cars for color classification I would recommend that you use a convolutional neural network to classify these patterns. So your method would now look like this:1. Use a spatial transformer network or a filtered Gaussian detector to filter out the car2. Use a convolutional neural network to classify the make and model of the car.3. Use another neural network that has either a convolutional or deep architecture to classify patterns and solid colors. So the outputs would contain all of the colors that you want and all of the patterns that you want to detect.'	395	0	0	0
2922	-1	0	0	3	b'Nowadays Artificial Intelligence seems almost equal to machine learning(especially deep learning). Some said deep learning will replace experts who were once very important in the old days(for feature engineering) in this field. It is said that two breakthroughs underlie the raise of deep learning: on the one hand, in neuralscience neuroplasticity tells us that like human brain which is highly plastic, artificial networks can be utilized to model almost all functions; on the other hand, the hardware is upgraded very quickly, in particular the introduction of GPU boosts the machine in a magnificent way, resulting in the models created decades ago becoming immensely powerful and versatile. Such developments bring computer vision into a new era, but in natural language processing and expert system the situation seems don\'t change very much. Neural networks can be harnessed in building knowledge base but it seems that hardly can the built knowledge base be used by neural network models. My questions are: 1) is knowledge base(for instance knowledge graph coined by Google) a promising branch in AI? If so, in what ways KB can empower machine learning? And how can it help in natural language generation? 2) for survival in an age dominated by DL where is the direction for the knowledge base? Is Wolfram-like dynamic knowledge base the new direction? Or any new directions?Hoping that I am asking an appropriate question here(I even can not tag my question as "knowledge base" so far). Am I horribly missing something happening?'	245	0	1	0
2923	2920	0	1179	2	b'Model of the carWhat you want to do is close to one-shot image recognition. You have not 1, but 3-4 examples of each car, but that is still a small amount, especially considering the car looks different from different angles (are you supposed to recognize them from any point of view, including sideways, rear, front, and 45 degrees etc.? maybe you also want to recognize them photographed from the top?).One interesting article I found is: Siamese Neural Networks for One-shot Image Recognition by Koch, Zemel, and Salakhutdinov.I also found that Caffee supports Siamese networks.You may want to read other literature about the One-shot learning.One trick you can do is to utilize the fact that cars are symmetric, so you can double the number of learning examples by reflecting each image.ColorDetermining the color is not as simple as it seems. Your algorithm need to determine where is the car at your picture, and then determine the color, taking into account the lighting conditions, as well as light effects, most notably reflection. For example, consider the following image: .We see strawberries as red, but there are no red pixels on this picture. The images of strawberries consist on grey pixels.Maybe you also need a convolutional neural network, or just a neural network, for this task.'	212	0	2	0
2924	-1	0	0	0	b"I know I've seen this somewhere before, but can't find it now. Say we have a neural network with a handful of layers, and we're applying dropout to each layer. As we move closer to the output, should dropout decrease, increase, or stay the same?"	44	0	0	0
2925	-1	0	0	2	b'I want to write a program that looks at abbreviated words, then figures out what the words are. For example, the abbreviation is "blk comp", and the translation is "black computer". In order to give it context for more ambiguous terms, I will be inputting sets of words with each request. So, if I input the set "keyboard, software, mouse, monitor", I would expect to get "black computer". On the other hand, if I input "Honda, transmission, mileage, Ford", I then would expect to get "black compact", or at least something that has anything to do with cars. Basing on the above case scenario, what kind of an algorithm should be applied in this case?'	114	0	0	0
2926	-1	0	0	1	b'My situation : my CSV file contains data like "ReadHIT,ReadMiss,WriteHit,WriteMiss,CacheUsage and few more attributes " all based on performance of a workload on a cache and source for every particular interval of times .I want any algorithm(predictive machine learning mostly) to identify pattern in my CSV file without user specifying any conditions and any prior patterns.MY input to algorithm will be only my CSV file.That pattern the algorithm identifies should be helpful in these ways:help user to increase the performance of the workload next time .help to identify any problems based any particular attributes in CSV data .help user to predict future data values or future events that may occur based on pattern.Even if algorithm is able to do any one of the above , i would be glad to know for now.thank you'	133	0	0	0
2927	-1	0	0	3	b"I've been reading a lot about hardware development and implementation for AI/ML, mainly about Deep Learning, and I have a question about its usage.From what I understand, there are 2 stages for DL: first is training and second is inference. The first is often done on GPUs because of their massive parallelism capabilities among other things, and inference, while can be done on GPUs, it's not used that much, because of power usage, and because the data presented while inferring are much less so the full capabilities of GPUs won't be much needed. Instead FPGAs and CPUs are often used for that.My understanding also is that a complete DL system will have both, a training system and an inferring system.My question is that: are both systems required on the same application? Let's assume an autonomous car or an application where visual and image recognition is done, will it have both training system to be trained and an inference system to execute? Or it has only the inference system and will communicate with a distant system which is already trained and has built a database?Also, if the application has both systems, will it have a big enough memory to store the training data? Given that it can be a small system and memory is ultimately limited."	214	0	0	0
2928	-1	0	0	0	b"I'm trying to create simple keras NN which will learn to make addition on numbers between 0 and 10. But I am getting the error: here is my code:how to fix that?"	31	33	0	0
2929	2928	1	2294	0	b"Try to use the model like this, for example:This means that first layer will have 50 neurons and can receive data in form of matrix with 2 columns and an unspecified number of rows.So you can prepare your data in this form \xe2\x80\x93 2 numbers for adding in each row.At the end, you need a layer with 1 neuron and the 'linear' activation, because you expect one simple number as a result.And finally, use 'mse' loss function or something similar. 'categorical_crossentropy' is needed for classification tasks, not regression as needed for you.See: https://keras.io/objectives/"	92	8	0	0
2930	2928	0	10236	0	b"You shouldn't use Softmax as an activation function in intermediate layers. Softmax is used to represent a categorical distribution, and should be applied at the point where one makes a categorical prediction (usually the final layer of the network).Consider replacing you activation function in all layers except the last one with 'relu' or 'sigmoid'."	53	0	0	0
2931	2919	1	49	3	b'Note that the statement says nothing directly about the limit of intelligence, nor even about the limit of computational intelligence - but about the limit of computing power.Perhaps the sentence "the speed of light puts a strict upper bound on how much computing can be done" needs a better explanation: The authors are probably referring to Bremermann\'s limit, which defines an upper bound in bits per second per kilogram. It is an upper bound on the processing power per unit of time of a computer with a given weight.There is also Margolus\xe2\x80\x93Levitin theorem which defines an upper limit in operations per second per joule. It is an upper bound on the processing power per unit of energy.These principles do not define a theoretical limit on computing power, but a practical one. If you\'ll limit your computer and your energy source to the size and capacity of the earth (or to the those of the universe) - you\'ll get a very practical limit.Check the reference section in Wikipedia article Limits to computation'	170	0	0	0
2932	-1	0	0	0	b"I am trying to do an inception layer, but it only works if the convolution strides, pool strides and pool size are the same, otherwise I get an error in  tf.concatthat Dimesion 1 is not the same. So If I change something in the last three tuples, I get the error.For example, this is the error I get if I change the 5x5 filter to have strides 3: Dimension 1 in both shapes must be equal, but are 6 and 16 for 'concat' (op: 'ConcatV2') with input shapes: [?,6,6,64], [?,16,16,64], [?,16,16,32], [?,16,16,32], [].This is the conv2d_maxpool function:How can I combine convolution filters with different strides and/or different pooling to create an inception layer?"	113	31	0	0
2933	2927	0	28018	0	b'Deep learning seems mostly to be a buzzword for what is essentially a neural network. You train with a data set to recognize a pattern, then input new data which is then classified by the trained network.So you train a neural network with 10 different kinds of animals using thousands of pictures. Then you show the network say 100 new images and have the network "guess" what each animal is. The point here is that training a neural network would required code for feedback that an application using the trained network would not need. So an application just using the trained network would be a bit more streamlined than an application which could allow additional training data. What is missing is the ability for machine learning to form hierarchical rules from the trained network. So there is no way for the the trained network to really "explain" why the classification works. So going back to the 10 animal trained network, there is no way for the network to tell you why the animal was classified the way it was. '	179	0	0	0
2934	2927	1	57480	2	b'To answer your question: Training and inference are usually completed on two separate systems you are right in knowing that training of deep neural networks is usually done on GPUs and that inference is usually done on CPUs. However, training and inference are almost always done on two separate systems. The main workflow for many data scientists today is as follows:1. Create and establish all hyper-parameters for a model such as a deep neural network 2. Train the deep neural network using a GPU3. Save the weights that training on the GPU established so that the model can be deployed. 4. Code the model in a production application with the optimal weights found in training. So as you can see from this workflow training and inference are done in two completely separate phases.However, in some specific cases training and inference are done on the same system. For example, if you are using a Deep Neural Network to play video games than you may have the neural network train and infer on the same system. This would lead to more efficiently because it would allow the model to continuously learn.To answer your question on memory, the only applications where inference and training are done in the same application have a lot of memory available(think dual GPU dual CPU 128gb of RAM workstations) whereas applications that have a limited amount of memory only use inference such as embedded applications.'	236	0	0	0
2935	2925	1	6663	3	b'For your first question,take a look at using a skip grab model to find what the abbreviated text is. The skip gram model turns a word into a vector which allows it to be processed by other machine learning algorithms. Or , alternatively you can do some really cool addition and subtraction problems with the resulting vectors. With the skip gram model in your case you could generate a vector for your input and then compare it with other skip gram vectors and once you have found a near perfect match then that is the unabbreviated word. You could also look at using sparse distributed representations of words to do this. This approach is similar to the skip gram model except that instead of a vector with maybe 500 values a sparse representation may contain thousands of binary digits of which only a couple are positive or 1. If you would like to look at this approach take a look at cortical.io which has free API that you can use. As for your second question I reccomend that you use a deep neural network in combination with the skip gram model to produce your output.'	194	0	0	0
2936	-1	0	0	0	b"I'm a newbie in machine learning, so excuse me in advance). I have an idea to make NN that can estimate visual pleasantness of arbitrary image. Like you have a bunch of images that you like, you train NN on them, then you show some random picture to NN and it estimates whether you'll like it or not. I wonder if there is any pervious effort made in this direction. "	70	0	0	0
2937	-1	0	0	-1	b'On what basis predictive algorithms(eg random forest and neural networks) find patterns ?need some info on :Predictive algorithms should take CSV file as input and it should return any useful information to the user,like suggesting any changes to be made to improve performance. Can this be possible ?IF not at least it should return some pattern based on data in CSV by which user can find it himself.Please share some useful source so that i can implement it.thank you'	78	0	0	0
2939	2936	0	24236	0	b"That sounds like a pretty straightforward application of a NN classifier to me. I don't know if anybody has done that specific thing or not, but I don't see any particular reason to think it wouldn't work. My advice to you is to just jump in and do it. "	49	0	0	0
2940	-1	0	0	0	b'By "neural network", I mean the typical, multilayered neural network with inputs, weights, hidden nodes and outputs, as shown in the image below:Such neural networks, in the context of evolving neural networks, can be characterized by the fact that all weighted connections between nodes are all present at the beginning, and can each be represented as a continuous real number. Also, if such a network is used as an agent\'s brain, the agent\'s response will be calculated immediately after receiving a set of stimuli.I want to know if there is any other systems of information processing that do not rely this structure. For example, is there any system in which the topology of a neural network is variable? Or a system in which links between nodes are not real numbers?'	129	0	0	0
2941	-1	0	0	1	b'I would like to detect street and sidewalk surface in a very detailed (0.075m/pix) USGS High Resolution Orthoimagery which basically means image segmentation with two classes. Places in question are residential areas similar to this one. I will download uncompressed raw imagery in GeoTIFF from USGS for the detection.I read that neural networks can perform very good in image segmentation and I would like to try them. I am a developer by day so I can code but am a beginner to neural networks only knowing the basic principles about architecture, weighting and backpropagation etc. Is it possible to jump right in into my task or do I need to start with something simpler? I would prefer jumping right in if it can save time.I skimmed though few papers dealing with similar thing and they seem quite complicated. Is there some simple way I can get started? I mean maybe an open source project in neural networks that deals with image segmentation that is similar to my task and I could make use of it?I see neural networks need to be trained first and I am prepared to do manual segmentation first to have data for training. However, I have no idea about neural network design/architecture, how to design the layers, how many layers do I need etc. I also would like to use the fact that the network would learn some basics on how streets and sidewalks are built - that they are (not sure if my term is correct) "linear structures" which usually run many meters in length and may not even end in the image, also that sidewalks usually run alongside streets, streets have intersections etc.'	278	0	0	0
2942	-1	0	0	5	b'I know this question might have been asked and answered before, but I just couldn\'t find the answer I\'m looking for.I\'ve been reading a lot about DL, and I can understand to an extent how it works, in theory at least, and how it\'s different -technically- from conventional ML.But what I\'m looking for is more of a "conceptual" meaning. Why DL? What it offers better?Let\'s say you\'re designing a self-learning system, why to choose DL? What are the main performance parameters that DL offers? Is it more accuracy? More speed? More power efficiency? Mix of all of them?What is the main parameter to optimize the network for and what can be sacrificed?I need to understand why DL from this point of view.Thanks!'	121	0	0	0
2943	2936	0	73236	0	b"I don't think anyone has done it yet,but you could try.A way you could implement it is having a quite efficient CNN trained on the things you like,then your program should ask the user if he does like some images and on the answers he will give, your program will finetune the original network and then with the fresh-trained one you should obtain good results."	64	0	0	0
2944	2279	0	77682	1	b"You could use another type of CNN that instead of classification is performing regression so it will also give you as output the position(it's not really like that but this is the core idea) .Some algorithms are SSD or YOLO."	39	0	0	0
2945	2926	0	2056	1	b'You could try 3 different approaches :First you must classify each chunk of the CSV file and label it in base of what situation is in that case (like 1 optimal situation .2 critical...), and then you can use the machine learning algorithm you like most.Cluster your data with an algorithm like SOM or K-Means and then you simply classify the classes you will get.Use some unserpervised learning approach.'	68	0	0	0
2946	2942	0	11560	4	b'Deep learning allows you to solve complex problems without necessarily being able to specify the important "features" or key input variables for the model in advance.To give an example, a problem that may be easily tackled without deep learning could be predicting the frequency and claim amounts of insurance vehicle claims, given historical claim data that may include various attributes of the policy holder and their vehicle. In this example, the "features" to be specified in the model are the known attributes of policy holder and vehicle. The model will then attempt to utilise these features to make predictions.On the other hand, facial recognition is a problem more suited to deep learning algorithms. This is because it is difficult to manually identify what combinations of pixels may be important features to include in a conventional machine learning model. A multi-layered neural network however has the potential to identify/create the important features itself, which may include for example eyes, nose and mouth, and then utilise these features to recognise faces and other objects.'	171	0	0	0
2947	1955	0	18374	0	b'There is an interview (link see below) with David E. Smith, a senior researcher in the Intelligent Systems Division at NASA Ames Research Center. In this interview, he talks about the application of AI and AI planning in particular in his work at NASA. He also (just shortly) mentions the Mars Exploration Rover and cites related scientific papers (just search for "Mars").Link to the official publication at Springer:'	67	0	1	0
2948	2236	0	27862	0	b"AI is a wide field that goes beyond machine learning, deep learning, neural networks, etc. In some of these fields, the programming language does not matter at all (except for speed issues), so LISP would certainly not be a topic there.In search or AI planning, for instance, standard languages like C++ and Java are often the first choice, because they are fast (in particular C++) and because many software projects like planning systems are open source, so using a standard language is important (or at least wise in case one appreciates feedback or extensions). I am only aware of one single planner that is written in LISP. Just to give some impression about the role of the choice of the programming language in this field of AI, I'll give a list of some of the best-known and therefore most-important planners:Fast-Downward:description: the probably best-known classical planning systemURL: language: C++, parts (preprocessing) are in Python FF:description: together with Fast-Downward the classical planning system everyone knowsURL: https://fai.cs.uni-saarland.de/hoffmann/ff.htmllanguage: CVHPOP:description: one of the best-known partial-order causal link (POCL) planning systemsURL: language: C++ SHOP and SHOP2:description: the best-known HTN (hierarchical) planning systemURL: https://www.cs.umd.edu/projects/shop/language: there are two versions of SHOP and SHOP2. The original versions have been written in LISP. Newer versions (called JSHOP and JSHOP2) have been written in Java. Pyshop is a further SHOP variant written in Python. PANDA:description: another well-known HTN (and hybrid) planning systemURL: language: there are different versions of the planner: PANDA1 and PANDA2 are written in Java, PANDA3 is written in ScalaThese were just some of the best-known planning systems that came to my mind. More recent ones can be retrieved from the International Planning Competition (IPC, ), which takes place every two years. The competing planners' code is published open source (for a few years). "	295	0	4	0
2949	2941	0	63302	2	b"Yes, in fact neural networks (NNs) are very efficient at segmentation and it seems to me that your problem matches the capabilities of neural networks very well. I think it best for you to truly understand what a NN is before using it. First, let's start with the architecture. A NN has 3 regions, the input layer, the hidden layers and the output layer. The input layer depends on the number of features in your dataset. The hidden layers, you can have multiple layers all of different breadth (number of nodes per layer). The output layer depends on the number of classes in your dataset. An easy example is applying a NN to the MNIST datatset. This is a dataset which contains handwritten digits between 0-9. Let's assume each of these images is 16*16=256 pixels. Thus, you will need 256 input nodes. And you will need 10 output nodes, one for each possible output. The hidden layers can be set in any way you can creatively imagine. There are however ways to optimize your hidden layer to get the best performance possible while not spending too much computational power. This is always how a NN works. The beauty of a NN is that you only need to code it once and it can learn any function. All you need to do is change your architecture, but the underlining principles will always be the same. In your case, you want to do segmentation. This is often done using a window around the pixel you want to classify. Popular choices are 3*3 or 5*5 pixels. The choice of your considered window will determine the number of nodes in your input layer. Then you want to classify them as one of two classes, thus you need 2 output nodes. You can also use just 1, but I don't recommend it, I can expand on this if you care.One caveat of NN is that you will need quite a bit of training data. So get ready to classify a lot of pixels manually. How to know how many layers in hidden layer? How to know how many nodes per layer in the hidden layer?In general, for simple operations like the one you are trying to learn you do not need to have multiple layers. One hidden layer should be enough, at most 2. But, how do you determine the number of nodes you should use? You need to use some model validation techniques to do this. One way is through grid search and cross-validation. Train, and re-train your model with multiple number of nodes and then compare their performances to identify the optimal number of nodes. To get good results this does require a large dataset.Rule of thumb: 1 hidden layer for NN! Don't get dragged into deep models if you don't need them. "	470	0	0	0
2950	-1	0	0	2	b'I want to train text classifier (using https://www.uclassify.com) with 12 classes/categories. I will be training it to classify news/articles (I know that there are existing classifier but I want to train my own).uclassify uses following algorithm (directly copied from their site): The core is a multinominal Naive Bayesian classifier with a couple of steps that improves the classification further (hybrid complementary NB, class normalization and special smoothing). The result of classifications are probabilities [0-1] of a document belonging to each class. This is very useful if you want to set a threshold for classifications. E.g. all classifications over 90% is considered spam. Using this model also makes it very scalable in terms of CPU time for classification/training.I was wondering how many examples I will need to train such classifier? It is possible to estimate the number? Let\'s assume that one article will "fit" 2 categories by average.'	146	0	0	0
2951	154	0	69862	3	b"1) It is possible! In fact it's an example in the popular deep learning framework Keras. Check out this link to see the source code.2) This particular example uses a recurrent neural network (RNN) to process the problem as a sequence of characters, producing a sequence of characters which form the answer. Note that this approach is obviously different from how humans tend to think about solving simple addition problems, and probably isn't how you would ever want a computer to solve such a problem. Mostly this is an example of sequence to sequence learning using Keras. When handling sequential or time-series inputs, RNNs are a popular choice."	107	0	0	0
2952	2950	1	4760	4	b'As a general rule of thumb I typically use 10*(# of features) for shallow machine learning models such as Naive Bayes with only 2 classes. So it all depends on the number of features you will be using. However, the more output classes the more data you will need for proper discrimination. The addition of more classes is not linear but I think you can get away with: 10*(# of features)*(# of output classes)'	73	0	0	0
2953	-1	0	0	0	b'How would one go about building an AI that is capable to look at any kind of input and then identify what is the nature of this data? For example, an AI that is able to do image classification, NLP and react to some other sensors. Is it possible to build an AI that will be able to identify what kind of data it is seeing such that it can send the data to the correct model for it to be treated. Similarly, to the how the human brain knows to send visual information to the visual cortex and auditory information elsewhere. In a simple scenario, I think we can get very good performance by having a cascaded image classifier. For example 2 layers, the first identifies if the image contains a dog and a cat. The next layer, has two different CNNS, one trained to identify the breed of dog and the other one for cats. That way once we identify that we have a dog, the image can be sent to the correct CNN. A CNN that is trained specifically to detect the breed, thus being much more robust that a more generalized CNN. Kind of like a professional in the field. First the human identifies that he is looking at a dog then he consults a professional to ass him the breed. I would like to extend this idea to being able to identify various kinds of data sources that do not resemble each other at all. Various input. Are there any models that can do this?'	259	0	0	0
2954	2942	0	43960	2	b'Deep Learning these days mean a lot of things to a lot of people, its quickly becoming a buzz-word. But so far it still retains two very important conceptual properties:Does away with most feature engineering work. This was mentioned in the answer above, but this is very important. It really saves a lot of work.Allows you to make maximal use of unlabelled data. This is strictly speaking available to other approaches, not just Deep Learning, but its in DL that this really took off. And typically labelled data is very hard to get while unlabelled is all over the place. Things like denoising autoencoders and Restricted Boltzmann machines are just wonderful.'	110	0	0	0
2955	-1	0	0	3	b'What\'s the term (if such exists) for merging with AI (e.g. via neural lace) and becoming so diluted (e.g. 1:10000) that it effectively results in a death of the original self?It\'s not quite "digital ascension", because that way it would still be you. What I\'m thinking is, that the resulting AI with 1 part in 10000 being you, is not you anymore. The AI might have some of your values or memories or whatever, but it\'s not you, and you don\'t exist separately from it to be called you. Basically - you as you are dead; you died by dissolving in AI.I would like to read up on this subject, but can\'t find anything.'	113	0	0	0
2956	2942	0	63601	1	b"Deep learning allows you to not know the answer in order to ask the program a question. Their main benefit is their finite ability and flexible nature.The problem with procedural programing to solve problems is you have to know what the computer needs to do in order to solve the problem.What deep learning does is remove the requirement of the programmer to know how to solve the problem by having them only need to know what the computer needs to know.This is the entire premise of neural networks. The programmer writes the program for data points required to be known in order to solve a particular problem.The computer is given an input it comes up with an answer.If it's answer is wrong it needs to make the answer it gave less likely and the right answer more likely.The goal is to get the computer to always get the right answer. If the computer always gets the wrong answer then the neural network it too small.What deep learning is, is a neural network that is deep.To answer this you need to know how a neural network is.A neural network is based on a neuron:Finite number of `boolean' inputs (More then one)A weight is attached on each input to define how importantoften though as a float between -1 and 1, but it's just a percentage of how likely each input changes the answer.One boolean outputA neuron can be a class or function the implementation really doesn't matter. The weight of each input changes as more answers are asked and responses verified.The depth of a neural network is has one layer when there is one row of neurons between the input and output.two layers when a few neurons make decisions on inputs and a final neuron or multiple neurons make decisions biased on those neurons.A neural network is called deep when there are at least four layers of neurons? (do some research don't take my word for it ^_^)The disadvantage of deep learning is that it's ability is finite.There is no way a deep neural network by it's self to get smarter then it's programed to be.It has a intelligence curve similar to root time if it isn't improved somehow.This leads to the other problem in neural networks. While the programmer has no need to know how decisions are made by the computer they still need to know what questions or nodes need to be added.The reason this is a problem is if the nodes responsible aren't there the program will be wrong in strange cases and have no way of correcting this on it's own. The larger the network the harder it is to solve these kinds of problems.This will lead to an inevitable solution to have the computer self improve by some type of generative algorithm.This has it's own breadth of problems as if not built properly could grow into something unintended which wastes time and money if it fails quickly, and could be potentially dangerous if it appears to work and doesn't.The answer to AI will be a combination of deep neural networks some generative type programing and some new ideas and innovations."	522	0	0	0
2957	-1	0	0	2	b"Human beings are more productive in groups than individually, possibly due to the fact that there is a limit to how much one human brain can improve itself in terms of speed of computation and areas of expertise.By contrast, if a machine with general-purpose artificial intelligence is created and then assigned a task, would it be possible that the machine will be able to better accomplish its task by continuously improving its own computational power and mastery of various skills, as opposed to collaborating with other agents (whether copies of itself, other AI's, or even humans)?In other words, would an AGI ever need to collaborate, or would it always be able to achieve its goals alone?"	115	0	0	0
2958	2846	0	51805	0	b'This is one of the main research areas of my lab which researches intelligent prosthetics which also give sensory feedback such as touch and kinaesthesia (the feeling of a limb moving in space) to the user. We use reinforcement learning to bridge the gap in control and have preliminary work of in communicating to the user predictions made by the artificial agent. '	62	0	0	0
2959	-1	0	0	7	b'According to Wikipedia, citations omitted: In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. The term was coined by analogy to the idea of a nuclear winter. The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or decades later.The wikipedia page discusses a bit about the causes of AI Winters. I\'m curious however whether it is possible to stop an AI Winter from occurring. I don\'t really like the misallocation of resources that are caused by over-investment followed by under-investment.One of the causes of the AI Winter listed on that Wikipedia page is "hype": The AI winters can be partly understood as a sequence of over-inflated expectations and subsequent crash seen in stock-markets and exemplified by the railway mania and dotcom bubble. In a common pattern in development of new technology (known as hype cycle), an event, typically a technological breakthrough, creates publicity which feeds on itself to create a "peak of inflated expectations" followed by a "trough of disillusionment". Since scientific and technological progress can\'t keep pace with the publicity-fueled increase in expectations among investors and other stakeholders, a crash must follow. AI technology seems to be no exception to this rule.And it seems that this paragraph indicates that any new technology will be stuck in this pattern of "inflated expectations" followed by disillusionment. So are AI Winters inevitable? That AI technologies will always be overhyped in the future and that severe "corrections" will always will always occur? Or can there a way to manage this Hype Cycle to stop severe increases/decreases in funding?'	278	0	0	0
2960	2959	1	19061	2	b'I think that by strict definition of the word inevitable, no, future AI Winter events are not inevitable. However likely or unlikely it may be, it is possible to control research spending and to create a more stable plan of funding research in Artificial Intelligence. Because it is possible to avoid an AI Winter event, an event is not inevitable.'	59	0	0	0
2961	2900	0	26696	0	b"Yes, it is possible.When humans were working on the first nuclear bomb, some field experts of the time thought that when the reaction went super-critical, it would not stop, and would devour the earth. It was a plausible possibility given our understand of nuclear energy at the time, and we didn't know for sure until we did it.Some scientists synthesize black-hole like environments in laboratories. Some experts think that if a certain point is accidentally crossed due to ignorance or negligence, we may devour our planet with a self made black hole.The situation is the same with AI. Until we actually create a super-intelligent AI, we cannot say with certainty whether it will be controlled or controllable until it happens. Until that time comes the answer to your question is yes, it's possible, but that does not mean it will or will not happen that way."	145	0	0	0
2962	2917	0	59089	3	b'This depends on your definition of human-like.If you mean a robot that looks and acts like a human, arguably, yes. Here\'s one of many examples: If you are looking for something that performs work and tasks, or works and thinks and talks like-or better than a human, the answer is mostly no, not yet.I recommend you look at \'ANI, AGI, ASI"ANI: artificial narrow intelligence. This is what you see around you right now.AGI: artificial general intelligence. A theoretic AI that can "think" like a human. It does not yet exist. Estimates are between 20-60 years before we will successfully create AGI.ASI: artificial super intelligence. In a nutshell, it is theorized to be everything we wish we could be or hope never to be. It does not exist yet. It is generally believed that, IF we create an AGI, ASI will evolve seconds or less than a decade after AGI is created.'	150	0	1	0
2963	2771	0	75542	0	b'Imho, it is life. Example: consider the possibility that we synthesized from completethe DNA of a human being, with zero atoms from another human, and grew said human in a lab. Most (and myself) would agree that creature is alive. Although there are many opinions that differ, my own is that there is no absolute line to draw between something that is alive, and something that is not alive. A human is alive. But is a single bacterial cell, or a single cell from your own body? They reproduce, they eat, etc. so yes they are alive. They are not like a dog or a cat however. In fact, a bacterial colony in a pool of water, giving off a yellow or brown color can be mistaken as a mineral or mud. It is only when you look closer that you see it is actually life. What about a biological virus? It is not made of cells. It does not have DNA. Most would agree it has life. But is does not really seem to be as alive as say, a shark or giraffe. Many people do not think a car is alive. Yet cars evolve. They move, they "eat." Life is simply a way we define things around us. A much more useful and definitive way to categorize life I think, would be to utilize a continuum instead of an all or nothing approach. Rocks would go on the end of "nonliving." Intelligent, multicellular; self-aware entities could perhaps be on the other end as "fully alive."Other entities can go in between. As for something such as AI, I would propose adding a z-axis, to make a 3 dimensional continuum, allowing for a self aware, intelligence entity not made of cells to fit comfortably with humans without causing an all-or-nothing.PS: thought I came to this conclusion myself, I have a suspicion someone smarter than me has already written about such an idea. If anyone feels like educating me, I\'d love to hear it.'	332	0	0	0
2964	-1	0	0	1	b'Does Artificial Intelligence write its own code and then execute it?If so does it create separate functions(for each purpose) for its code?How does learning get implemented in artificial intelligence?Is there a specific flowchart to describe artificial intelligence'	36	0	0	0
2965	2959	0	78518	0	b'The hype cycles are the rule these days, and AI is always a wonderful topic for unbelievable and crazy hype. I mean simple thing like speech recognition is still not working properly, but everybody is discussing how to survive the revolt of the terminator machines. So unless we can tune the hype down, the next AI winter is inevitable.'	58	0	0	0
2966	2964	0	18087	0	b'Computers are able to write their own code without needing any intelligence -- see the Wikipedia entries for self-modifying code and metaprogramming. You do have to write the instructions for how the computer should program itself, and there\'s a stigma against doing this because (a) it makes it hard to reason about what your program is doing when it\'s changing its source code, and (b) the solution is usually slower than just hardcoding in what you want the program to do in the first place. But it is possible, and programmers have done it (usually for maintainability or aesthetic reasons). Some AI researchers are interested in Genetic Programming though. Genetic Programming is a subset of evolutionary algorithms and Wikipedia provides a good summary of how they usually work: Step One: Generate the initial population of individuals randomly. (First generation)  Step Two: Evaluate the fitness of each individual in that population  Step Three: Repeat the following regenerational steps until termination (time limit, sufficient fitness achieved, etc.):   Select the best-fit individuals for reproduction. (Parents) Breed new individuals through crossover and mutation operations to give birth to offspring. Evaluate the individual fitness of new individuals. Replace least-fit population with new individuals. The "individuals" in this case are randomly-generated computer programs, which are then tested against a fitness function.The Wikipedia page for Genetic Programming claimed that these programs are usually represented by tree structures, though there has been some experiments in using non-tree structures as well.'	245	0	0	0
2967	-1	0	0	0	b'As far as I know MDP are independent from the past. But the definition says that the same policy should always take the same action depending on the state.What if I define my state as the current "main" state + previous decisions?For Example in Poker the "main" state would be my cards and the pot + all previous information about the game.Would this still be a MDP or not? '	69	0	0	0
2969	2967	0	13304	0	b"It's not totally clear from your description, but it sounds like you may be onto something like an Additive Markov Chain."	20	0	0	0
2970	2964	0	49009	0	b' Does Artificial Intelligence write its own code and then execute it? If so does it create separate functions(for each purpose) for its code?First of all, "Artificial Intelligence" isn\'t a singular "thing" where it really makes sense to ask questions like "Does artificial intelligence xxx?" The answer to questions phrased like this can generally be any of "Yes", "no", "maybe", "we don\'t know" or "all of the above". How does learning get implemented in artificial intelligence?The comment above aside, as others have already mentioned, self-modifying code is one of the (many) techniques used in some applications of what can be called "AI". So in that sense, the answer to your question in a very general sense is "yes".But these days, the stuff that is state-of-the-art in machine learning / AI is usually more about finding sets of weights for functions that match a pattern or whatever. In Neural Networks, for example, the NN isn\'t writing any code, it\'s just running through an algorithm that incrementally changes some weights (or coefficients) such that when you enter a certain input, you get an output that\'s close to the desired output. And then you have approaches like using a Genetic Algorithm to evolve the weights in the NN, as opposed to using back-propagation. To the extent that GA\'s are a little closer to the idea of "an AI coding itself" (although not exactly), you could kinda sorta consider that an example of what you\'re asking about. Is there a specific flowchart to describe artificial intelligenceNot even close. If you\'re interested in exploring all of this further, I\'d suggest reading the book The Master Algorithm by Pedro Domingos, and take Andrew Ng\'s MOOC on Machine Learning on Coursera. Then pick up a copy of Russell &amp; Norvig\'s Artificial Intelligence: A Modern Approach and dig in.'	300	0	0	0
2971	2917	0	47333	0	b'I would say that we\'re not even close to a "real" human-like AI. For all the wonderful things that applications like Siri, Cortana and the like can do, they\'re actually really dumb compared to even a child. Of course part of that, IMO, is that most AI applications are not embodied and don\'t experience the world the way humans do. So if you show an AI a video of a dog walking behind a table and briefly disappearing from the frame, it has pretty much no ability to apply "common sense" and know that the dog will reappear in a few seconds, and that if it does\'t reappear, it\'s probably because it found its favorite toy on the floor behind the table. For some examples of the kinds of "easy" questions that computers still don\'t do well at, check out the Winograd Schema Challenge. You might also find this page interesting: '	151	0	1	0
2972	2957	0	81379	0	b"Communications is expensive. It requires a communication channel, a protocol and of course time. Communications is also limited to the expressivity defined by the protocol. Note also that agents may compete over resources, or may have contradictory goals, so in some situations they may try to mislead each other.On the other hand - computational power and memory are limited, so multiple agents may solve computation-intensive or memory-intensive problems together better/faster than each single agent can. Different agents may have different sensors, and mobile agents may have information about different parts of their realm, so by sharing knowledge they may have more complete information and make better decisions. Goals may also be time-bounded, and rewards may be time-dependent. Sometimes working together means greater rewards.In summary, there may be situations where collaboration is beneficial, and there may be some situations where collaboration is essential to achieve one's goal or to achieve a common goal."	151	0	0	0
2973	-1	0	0	4	b"I'm looking for AI systems or natural language processors, that use in the classification and interrelation of notions/objects some philosophical system, like basic laws of logic, Kantian, empiricism etc. Also i have read about goal-seeking procedures. Are these based on psychology fields and some particular psychology theory or these are ad hoc experiments, with only general terms applied?"	57	0	0	0
2974	-1	0	0	0	b'I\'m using a NN created with CNTK\'s SimpleNetworkBuilder to make choices (specifically in board games). I specified ReLU as the layer type, so outputs can be arbitrary numbers.When evaluating a custom set of features, getting the "choice" of the function/model is simple: Look for the output signal with the highest value. However, there are times when I wish to introduce some randomness and assign probabilities to each output signal, then select the choice based on each output\'s probability.Currently, what I\'m doing is manually normalizing all the output using a sigmoid function specified here: https://en.wikipedia.org/wiki/Logistic_functionThen, I multiply them all by a scalar such that the sum total of all outputs is 1.At this point, I pick a random number 0..1, and see where along the map it falls; that is my selected choice.What I\'d like to know is, is there a better way?'	141	0	0	0
2975	2973	0	19153	2	b'Excellent question! I\'m currently working on the pre-Socratics as the most basic philosophies for first principles (These philosophers are intriguing for their simplicity and universality, and the "dawn of consciousness", in some conceptions, may be ascribed to the Classical Era. Linguistically, ancient Greek is fundamental to meaning in the West.)However, I was pointed at this very interesting blog post "Algorithms of the Mind" which has some useful links, and discusses Kant, which you may find useful. '	76	0	0	0
2976	-1	0	0	0	b"I have come across this domain via this Wikipedia article: General game playingSo, where are we when it comes to general game playing AI? (The wiki article doesn't mention the recent advances and the achievements of this domain of research, except the annual games results.)PS: I understand that this is a General project of the Stanford Logic Group of Stanford University, California. But since then, it has become an area of research in the domain of AI."	76	0	0	0
2977	2974	0	79319	0	b'I would just skip the sigmoid function step.Consider these two scenarios with three choices with the associated values:0.1,10,100 or 1,100,1000Given that these scenarios are equivalent in their relativ values, your probabilities should be assigned in the same way. But if you throw in a sigmoid function, the difference between 10 and 100 will be bigger than between 100 and 1000. Just normalise the values, so that they sum to 1. In that case the choice with a value ten times higher than the value of another choice will be picked ten times as often. Makes sense to me. '	98	0	0	0
