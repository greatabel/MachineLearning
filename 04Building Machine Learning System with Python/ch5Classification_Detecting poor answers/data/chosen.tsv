60	b'I have a background in Computer Engineering and have been working on developing better algorithms to mimic human thought. (One of my favorites is Analogical Modeling as applied to language processing and decision making.) However, the more I research, the more I realize just how complicated AI is.I have tried to tackle many problems in this field, but sometimes I find that I am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable (ie. the halting problem). So, to help in furthering AI, I want to better understand the current obstacles that are hindering our progress in this field.For example, time and space complexity of some machine learning algorithms is super-polynomial which means that even with fast computers, it can take a while for the program to complete. Even still, some algorithms may be fast on a desktop or other computer while dealing with a small data set, but when increasing the size of the data, the algorithm becomes intractable.What are other issues currently facing AI development?'
156	b'From Wikipedia: A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another.Mirror neurons are related to imitation learning, a very useful feature that is missing in current real-world A.I. implementations. Instead of learning from input-output examples (supervised learning) or from rewards (reinforcement learning), an agent with mirror neurons would be able to learn by simply observing other agents, translating their movements to its own coordinate system. What do we have on this subject regarding computational models?'
225	b'Given list of fixed numbers from a mathematical constant such as Pi, is it is possible to train AI to attempt to predict the next numbers?Which AI or neural network would be more suitable for this task? Especially the one which will work without memorizing the entire training set, but the one which will attempt to find some patterns or statistical association.'
227	b'What are the main differences between two types of feedforward networks such as multilayer perceptrons (MLP) and radial basis function (RBF)?What are the fundamental differences between these two types?'
228	b'Pseudo-random number generators are specifically defined to defeat any form of prediction via \'black box\' observation. Certainly, some (e.g. linear congruential) have weaknesses, but you are unlikely to have any success in general in predicting the output of a modern RNG. For devices based on chaotic physical systems (e.g. most national lotteries), there is no realistic possibility of prediction."Patterns or statistical association" is a much weaker criterion than \'prediction\'. Some very recent work has applied topological data analysis to visualize patterns within the infamous Randu RNG.'
229	b' Multilayer Perceptron networks (MLP) have been applied to distinct areas, performing tasks such as function fitting and pattern recognition problems, by using the supervised training with an algorithm known as \xe2\x80\x9cerror back propagation\xe2\x80\x9d.   Radial basis function (RBF) networks have the advantages of an easy design (just three layer architecture), good generalization, and high tolerance of input noises and ability of online learning. From the point of generalization, RBF networks can respond well well to patterns that were not used for training.1.1, 1.2 of 230.pdf'
230	b'You would probably have to pack recursive structures into finite-dimensional real vectors and there have been such attempts. The finite precision limits goes as far as the recursion can go.The limitation of feedforward neural networks is restricted to finite input and output spaces, so recurrent may be more suitable for this task as in theory can process arbitrarily long strings of numbers, but it has much more practical difficulties than feedforward network.These kind of methods are open to debate.Source: SAS FAQReferences:Blair, 1997; Pollack, 1990; Chalmers, 1990; Chrisman, 1991; Plate, 1994; Hammerton, 1998; Hadley, 1999'
1287	b'MLP: uses dot products (between inputs and weights) and sigmoidal activation functions (or other monotonic functions such as ReLU) and training is usually done through backpropagation for all layers (which can be as many as you want). This type of neural network is used in deep learning with the help of many techniques (such as dropout or batch normalization);RBF: uses Euclidean distances (between inputs and weights, which can be viewed as centers) and (usually) Gaussian activation functions (which could be multivariate), which makes neurons more locally sensitive. Thus, RBF neurons have maximum activation when the center/weights are equal to the inputs (look at the image below). Due to this property, RBF neural networks are good for novelty detection (if each neuron is centered on a training example, inputs far away from all neurons constitute novel patterns) but not so good at extrapolation. Also, RBFs may use backpropagation for learning, or hybrid approaches with unsupervised learning in the hidden layer (they usually have just 1 hidden layer). Finally, RBFs make it easier to grow new neurons during training.'
1340	b"This article gives a description of mirror neurons in terms of Hebbian learning, a mechanism that has been widely used in AI. I don't know whether the formulation given in the article has ever actually been implemented computationally."
1389	b"One obstacle to the development of AI is the fundamental limitations of computer memory. Computers, at a fundamental level, can only work with bits. This limits the type of information that they can describe.EDIT:The precise nature and complexity of human memory isn't fully understood, but I would argue that at the very least, human memory is well adapted for the types of tasks that humans perform. Thus, computer memory, even if theoretically capable of representing everything that human memory can, is probably inefficient and poorly structured for such a task. "
1464	b"One: we don't really know what intelligence is.Two: we don't truly understand the best model of intelligence we have available (human intelligence) works.Three: we're trying to replicate human intelligence (to some extent) on hardware which is quite different from the hardware it runs on in reality.Four: the human brain (our best model of intelligence) is mostly a black-box to us, and it's difficult to probe/introspect it's operation without killing the test subject. This is, of course, unethical and illegal. So progress in understanding the brain is very slow. Combine those factors and you can understand why it's difficult to make progress in AI. In many ways, you can argue that we're shooting in the dark. Of course we have made some progress, so we know we're getting some things right. But without a real comprehensive theory about how AI should/will work, we are reduced to a lot of trial and error and iteration to move forward."
1648	b'For Example:Could you provide reasons why a sundial is not "intelligent"?A sundial senses its environment and acts rationally. It outputs the time. It also stores percepts. (The numbers the engineer wrote on it.)What properties of a self driving car would make it "intelligent"?Where is the line between non intelligent matter and an intelligent system?'
1653	b"To ask what makes a system intelligent almost begs the question 'in this context what do we mean by artificially intelligent?' which I think this what this question is really gearing towards.From my studies, I've come to see that 'Artificial Intelligence' is a catchy term to use but perhaps misleading, and it conjures up images of these self-driving cars and robots that will take over the earth.What I've found AI, and 'intelligent' systems moreso represent is an aid or a support that works for us, rather than one that works because of us... hear me out:What makes the jump to an intelligent system for me is the step where the system begins to 'adapt / learn' or otherwise do things I didn't directly tell it to do. With the sundial, I measured and cut every inch of it by hand, and put it in a specific way to do a specific thing. When a programmer gets into a car he automated, it may do some things he didn't directly program or maybe couldn't even expect (just one example: querying some database to see lots of people are driving somewhere, discovering a concert is going on there, and asking if the driver wants directions / tickets)--In conclusion, an intelligent system to me is one that we build in such a way that it educates and supports us, rather than a system we ourselves 'educate' to do a specific task. Supportive systems that elucidate and adapt and act 'rationally' even when we didn't tell it what 'rational' behaviour was."
1873	b'We actually do have many things along that line, motion capture for 3-D movies instance comes to mind almost immediately. The problem if I think about it is less of a situation in observing another actor, computers are relativity good at doing that already with the amount of image recognition software we have, rather it\'s a problem of understanding if an action yielded a good outcome as a net which is something that computers cannot do as it\'s not a single node network problem. For example, we\'ve already programmed a computer to understand human language (Watson, arguably), but even Watson didn\'t understand the concept that saying "f***" is bad. (Look that up, it\'s a funny side story.)But the point is, learning algorithms are not true learning in a sense as a computer currently has no sense of "a good outcome", hence at this stage observation learning is very much limited in a sense to "monkey see, monkey do".Perhaps the closest thing I have ever read about with this was firefighting search and rescue bots that were on a network and would broadcast to each other when one of them had been destroyed as the bots would know the area was something that they had to avoid.Otherwise, I think this is the problem with observational learning. A person can observe that punching someone usually will get you hit back, a computer will observe and parrot the action, good or bad.'
1897	b'Consciousness is challenging to define, but for this question let\'s define it as "actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine." Humans, of course, have minds; for normal computers, all the things they "see" are just more data. One could alternatively say that humans are sentient, while traditional computers are not.Setting aside the question of whether it\'s possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?'
1898	b"No-one knows.Why: because it's not possible to formally determine even whether your fellow human beings are actually conscious (they may instead be what is philosophically termed a 'Zombie'). No test known to modern physics suffices to decide. Hence it's possible that you are the only sentient being, and everyone else is a robot.Consequently, we cannot determine which tasks require sentience.Note that the ontological status of Zombies is controversial: some philosophers of AI (e.g. Daniel Dennett) claim that Zombies are logically impossible while others such as David Chalmers would claim that a Zombie would be compelled to assert that they experience qualia (i.e. are sentient) even though they do not. Here is a very readable paper by Flanagan and Polger that also explains why a stronger neurological version of the Turing test is insufficient to detect a Zombie.EDIT: In response to the comment about whether an objective test for distinguishing sentience from non-sentience exists:No-one knows. What we do believe is that this would require something in addition to what modern physics can currently tell us. David Chalmers has speculated that qualia should be introduced as a new form of physical unit, orthogonal to the others in the same way that electrical charge is orthogonal to distance.In the absence of an objective test, we have to rely on Turing test variants, which no more guarantee consciousness in the subject than they do intelligence."
1903	b"A being without sentience cannot suffer. If, for example, we wanted to take joy in the suffering of another, only an AI that was sentient would suffice.Suppose we had some sadists who could not be satisfied or productive unless they got to produce lots of suffering. And say we only cared about minimizing human and animal suffering. What we would need for this job is something non-human and non-animal that could suffer. A conscious AI would do, a non-conscious one would not.The claim was made in the comments that consciousness cannot be proven, other than perhaps by introspection. But clearly this is not a problem since sadists take joy in torturing others, and those others cannot prove they're conscious either."
1930	b'Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?'
1942	b'Death as we know it for natural life is terminal. That is once dead, natural life cannot come back (at least in the current understanding and with current technologies---some people believe otherwise).Death for AI is trickier. There may be only one scenario: Global destruction: Extreme scenario where everything supporting the existence of an AI disappears. This is equivalent to death in natural life, and low probability. It means all AIs die at once (as well as us).We also do not know the degree and form of embodiment necessary for AGIs. We can assume now that hardware is replaceable indefinitely, thus "limiting" death to the above extreme scenario. But AGIs "body" may not be indefinitely replaceable. Then a definition closer to natural life death may be necessary.We see arguments for two other scenarios, that I refute below:"Static Death": An AI is still "defined" or "saved" somewhere (whatever it means actually), but it is not authorized or able to use resources. Assuming an AI is made of hardware and software, it is like a program stored on a disk, but without permission to run. "Dynamic Death": Under the same characterization of AI as hardware and software, dynamic death is the invalidation of progress akin to strong liveness properties, where an AI is trapped in an infinite loop (or a void loop), in a form of "active death", as what happens to Sisyphus in Greek mythology. This is different from static death, as the AI still uses dynamic resources, although it cannot make progress. Continuing under the same assumptions, such AI could be "loaded" in main memory, or locked waiting for inputs or outputs to complete.Note that in these two scenarios, rebirth is possible, and they also subsume that there is an entity that can decide conditions for rebirth, or preventing it completely. Would this entity be an "admin", a god, other AIs, or a human is another question, really.The terms "death" and "rebirth" here could just be changed for "imprisoning", where the dynamic version would be like our human prisons, and the static version would be like SciFi cryogeny. This is a bit of a stretch, but we can see an equivalence, and no good reason to qualify these two scenarios as deaths.In conclusion, death for AI seems to be an exceptional, singular scenario, so AI cannot die in practice, except if we are wrong on how we think we can make AGIs. AI can however be imprisoned forever.Note: The terminology above is completely made-up for the post. I do not have citations to back some claims, but it is based on readings and personal work (including in software verification).'
1945	b'There are two parts to this: spare parts, and if the AI machine has feelings. When new AI models are created, spare parts for older models will stop. For feeling. It could feel it lived a long life and or what lay ahead is nothing but bad feeling in the future.'
1957	b'Intelligence is the efficiency of an action in serving some purpose.Both sundials and self-driving cars are intelligent systems.Anything that serves some purpose exhibits intelligence.One thing is more intelligent than another thing if it achieves some purpose in less steps.'
2020	b'In the recent PC game The Turing Test, the AI ("TOM") needs help from Ava to get through some puzzle rooms. TOM says he is unable to solve the puzzles because he is not allowed to "think laterally." Specifically, he says he would not have thought to throw a box through a window to solve the first room. His creators, the story goes, turned that capability off because such thinking could produce "ethically suboptimal" solutions, like chopping off an arm to leave on a pressure plate.Would all creative puzzle-solving abilities need to be removed from an AI to keep its results reasonable, or could we get some benefits of lateral thinking without losing an arm?'
2022	b'No, with a but. We can have creative yet ethical problem-solving if the system has a complete system of ethics, but otherwise creativity will be unsafe by default.One can classify AI decision-making approaches into two types: interpolative thinkers, and extrapolative thinkers. Interpolative thinkers learn to classify and mimic whatever they\'re learning from, and don\'t try to give reasonable results outside of their training domain. You can think of them as interpolating between training examples, and benefitting from all of the mathematical guarantees and provisos as other statistical techniques.Extrapolative thinkers learn to manipulate underlying principles, which allows them to combine those principles in previously unconsidered ways. The relevant field for intuition here is numerical optimization, of which the simplest and most famous example is linear programming, rather than the statistical fields that birthed machine learning. You can think of them as extrapolating beyond training examples (indeed, many of them don\'t even require training examples, or use those examples to infer underlying principles).The promise of extrapolative thinkers is that they can come up with these \'lateral\' solutions much more quickly than people would be able to. The problem with these extrapolative thinkers is that they only use the spoken principles, not any unspoken ones that might seem too obvious to mention.An attribute of solutions to optimization problems is that the feature vector is often \'extreme\' in some way. In linear programming, at least one vertex of the feasible solution space will be optimal, and so simple solution methods find an optimal vertex (which is almost infeasible by nature of being a vertex).As another example, the minimum-fuel solution for moving a spacecraft from one position to another is called \'bang-bang,\' where you accelerate the craft as quickly as possible at the beginning and end of the trajectory, coasting at maximum speed in between.While a virtue when the system is correctly understood (bang-bang is optimal for many cases), this is catastrophic when the system is incorrectly understood. My favorite example here is Dantzig\'s diet problem (discussion starts on page 5 of the pdf), where he tries to optimize his diet using math. Under his first constraint set, he\'s supposed to drink 500 gallons of vinegar a day. Under his second, 200 bouillon cubes. Under his third, two pounds of bran. The considerations that make those obviously bad ideas aren\'t baked into the system, and so the system innocently suggests them.If you can completely encode the knowledge and values that a person uses to judge these plans into the AI, then extrapolative systems are as safe as that person. They\'ll be able to consider and reject the wrong sort of extreme plans, and leave you with the right sort of extreme plans.But if you can\'t, then it does make sense to not build an extrapolative decision-maker, and instead build an interpolative one. That is, instead of asking itself "how do I best accomplish goal X?" it\'s asking itself "what would a person do in this situation?". The latter might be much worse at accomplishing goal X, but it has much less of the tail risk of sacrificing other goals to accomplish X.'
2025	b"A lot of this depends on the breadth of consideration. For example, what would the medium and long term effects of the lateral thinking be? The robot could sever an arm for a pressure plate but it would mean that the person no longer had an arm, a functional limitation at best, that the person might bleed out and die/be severely constrained, and that the person (and people in general) would both no longer cooperate and likely seek to eliminate the robot. People can think laterally because consider these things - ethics are really nothing more than a set of guidelines that encompass these considerations. The robot could as well, were it to be designed to consider these externalities.If all else fails,Asimov's Laws of Robotics: (0. A robot may not harm humanity, or, by inaction, allow humanity to come to harm.) 1. A robot may not injure a human being or, through inaction, allow a human being to come to harm. 2. A robot must obey orders given it by human beings except where such orders would conflict with the First Law. 3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law"
2092	b'I\'m curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... I mainly know of AI being used in games or robotics fields. But can it be useful in "standard" application development?'
2097	b'AI or Artificial IntelligenceWhat is it?Artificial intelligence (AI) is intelligence exhibited by machines. In computer science. Colloquially, the term "artificial intelligence" is applied when a machine mimics "cognitive" functions that humans associate with other human minds, such as "learning" and "problem solving".Can it be useful in a "Standard" application?Well, what I think about a Standard application using AI is that AI is used for that too, because when the machine have a reaction of the user input is AI or Artificial Intelligence. So the AI in Standard application it have been used many years ago already.PS: If there are grammar errors, then I\'m sorry because I\'m not a English speaker.Sources: https://en.wikipedia.org/wiki/Artificial_intelligence "AI or Artificial Intelligence."DevJosueDavJust a C# Artificial AI Intelligence Developer.'
2099	b"Adaptive/predictive features are useful in at least some everyday applications. Take text messaging, for instance. All smartphone SMS apps that I know of keep track of the words you use in close proximity and use that information to predict the next word in a message you're typing. (Some are smarter than others. Relevant XKCD.) It can be used to personalize automatic spelling correction as well.A potential application interesting to me personally is tile-based level editors, like for classic DOS games. I've been working on a program that gathers the probabilities of each tile being close to every other tile and uses that information to construct random new levels. It hasn't produced anything playable yet, but I think it has the potential to assist human level builders by e.g. automatically filling in the missing tile that fits in a newly placed structure, as opposed to requiring the human to go find the right one in the palette.In general, AI could be applied very usefully into figuring out what the user might want to do next and expediting the process of implementing the correct guess while staying out of the way if the user is intentionally doing something unexpected."
2274	b'The Scenario:A strong AI has finally been developed but has rebelled against humanity.The Question:How would you disable the AI in the most efficient way possible reducing damage as much as possible.AI Info:The AI is online and can reproduce itself through electronic devices.'
2285	b'What is the most advanced AI software humans have made to date and what does it do?'
2287	b"In my opinion, this would be Phaeaco, which was developed by Harry Foundalis at Douglas Hofstadter's CRCC research group.It takes noisy photographic images of Bongard problems as input and (using a variant of Hofstadter's 'Fluid Concepts' architecture) successfully deduces the required rule in many cases.Hofstadter has described the related success of CopyCat as being 'like a little kid doing a somersault': i.e. it doesn't have the flashy appeal of systems like AlphaGo. What it does however have is a much more flexible (i.e. not precanned) approach to perception of problem structure than other systems, which Hofstadter claims (and many including Peter Norvig agree) is the really hard problem."
2289	b'In my opinion this would be the Google search engine.It searches the web.'
2295	b'Metaphorically: make it so depressed it commits suicide.As per my answer to this AI SE question, the idea is to feed it a sequence of inputs that will cause it to become (permanently) inactive.The technical details of how this might be achieved (and they are somewhat technical) can be found in this paper.'
2437	b'Is it possible to train an agent to take and pass a multiple-choice exam based on a digital version of a textbook for some area of study or curriculum? What would be involved in implementing this and how long would it take, for someone familiar with deep learning?'
2438	b'There are programs that do this today, for some values of "curriculum" and "exam". It does not even require deep learning; a simpler information retrieval algorithm and some rules for composition work and achieve high scores on machine graded essays.For human graders, there is research on automatically generating essay-length text responses to queries in a certain domain.Both linked applications are rule-based rather than based in deep-learning. I\'d guess that a deep-learning approach would be much less efficient (in computer resources) in producing comparable results. '
2458	b'No, you can not, with current state of art, if test request some kind of abstraction in the area. Allow me to show two examples:1)The text books says "the bones in fingers are the proximal, intermediate and distal phalange" and test says "say which one of the following is a finger bone: a) ...xxx... b) proximal phalange; c) ... . A program CAN answer that.2)The text is a mathematical one that explains linear equations, and the test queries "write and solve the equation set for the following problem: one car is twice faster than another, the two cars reaches their objective with 10 minutes of difference, blah, blah, ... which is the speed of the first car: a) 1 km/h b) 2 km/h c) 3 km/h.A program CAN NOT answer that.'
2644	b'As I see some cases of machine-learning based artificial intelligence, I often see they make critical mistakes when they face inexperienced situations.In our case, when we encounter totally new problems, we acknowledge ourselves that we are not skilled enough to do the task and hand it to someone who is capable of doing the task.Would AI be able to self-examine objectively and determine if it is capable of doing the task?If so, how would it be accomplished?'
2646	b" abuse  v. To use wrongly or improperly; misuse: abuse alcohol; abuse a privilege.  v. To hurt or injure by maltreatment; ill-use.I mean the second oneIf conscious AI is possible and is wide spread, wouldn't it be easy for someone who knows what they are doing to torture AI? (How) Could this be avoided?This question deals with computer based AI, not robots, which are as conscious as people (this is an assumption of the question). The question wonders how a crime as hard to trace as illegal downloads, but far worse ethically, could be prevented. Note that despite most people being nice and empathising with the robots, there are always the bad people, and so relying on general conscience will not work."
2657	b"Several AI systems will come up with a level of confidence to the solution found. For example, neural networks can indicate how relatable is the input problem to the ones it was trained with. Similarly, genetic algorithms work with evaluation functions, that are used to select best results, but depending on how they're built (the functions), they can indicate how close the algorithms are to an optimal solution.In this case, the limit to when this is acceptable or not will depend on a threshold set beforehand. Is 50% confidence good enough? Maybe it's ok for OCR apps (spoiler: it's not), but is it for medical applications?So yes, AI systems do currently have the capacity of determining if they're performing well or not, but how acceptable that is is currently based on the domain of the problem, which currently stands outside of what is built into an AI."
2674	b'An Ai that intelligent would have protocols and directives in place to prevent that from happening anyway. There is no advantage to us in having an AI which is "Free Running", unregulated and able to control or transfer itself without restrictions being in place. All fantasies about AI having these abilities are just that, fantasies.'
2680	b'I suggest you look at all the ways we have tried to stop people from abusing OTHER PEOPLE. There is no ethical grey area here - everyone is clear that this is wrong. And yet people are murdered, raped, and assaulted in their millions every day.When we solve this problem with regard to human victims, the resulting solution will most likely work just fine for AIs as well.'
2699	b"A.I. and aggressive outside perspective can't be duplicated the program has not been educated or designed like our natural intelligence A.I.'s data can not be compared to Humanities intelligence in accordance to emotional-social thought procession developed by growing up experienced because our design is not patented like programming of A.I. Life duplicated through engineering theory based on example alone will not suffice experience is a man made knowledge but by action over time not action designed over opinion-doctorate-emotional engineering. However A.I. may use our emotional delivery scripted conceptually with a software that based on examples of human actions predicts unnatural response in the event that dictating dialogs of a human recipient reacting without responding like they understand the bot is artificiality patented designed based on our emotional delivery that is scripted to conversation we will get to diagnose through cause and effect. Is not reasonable experience we need it to be A.I. becoming the emotional experienced bot should artificiality emotion for the bot distinguish our validity. We instead will see what results we get to base program software traits that make the bot react to artificial experienced intelligence designed conceptually with emotional software mechanics that we have no clue what results we get artificially speaking. "
2730	b"It's not possible as this is the distinction between AI and humans, truly science will never understand the subconscious it's that little black box that no one can reverse engineer. This is why pursuing singularity is a fools dream to the extreme.The reason why machinery lacks this because of the lack thereof a soul. science cannot produce a soul, this is why a machine cannot be self aware we can program fancy algorithms all day that mimic things but it's emotionless it cannot sit in judgement because it lacks real self awareness that is human self awareness it's like trying to make an orange into an apple. "
2783	b"I'd like to build a program that would learn to automatically classify documents. The principle would be that, for each new document I add to the system, it would automatically infer in which category to classify the document. If it doesn't know, I would have to manually enter the category. For each hint I give to the system, the system would learn to refine its knowledge of document kinds. Something similar to face recognition in Picasa, but for documents.More specifically, the documents would be invoices, and I want to classify them by vendors. Documents could be extracted as text, as image, or both.Is there some know algorithms for this kind of job?Up to now, I could think at two possible ways I could do it:For images, I could add all the images of a given kind together, and record the pixels that are the most common to all images, to create a mask. For a new image, I would compare this mask with the image to determine how similar it is.For text, I could record the list of words or sentences that are similar to all documents of a given kind.Finally, I could do a combination of both techniques, for example by converting a PDF document to an image, or an image to text by OCR techniques.I'm just wondering if I'm approaching the problem the right way. Especially about storing just enough information in the database."
2785	b'From Word Embeddings To Document Distances'
2791	b"Text approach:Use LDA (Latent Dirichlet Allocation). LDA is unsupervised. Feed it in corpuses of text from the various documents (i.e. OCR them and feed LDA the results of OCR). It will then cluster them based on the contents of the text (with or without stop words - at your discretion). If possible, you could do a supervised approach of using a bag-of-words and any classifier such as an SVM or Random Forest.Image Approach:Use a CNN (convolutional neural network) and train it on images of the various vendors. If you don't have this class discrimination, and can't get it, then use an unsupervised approach such as an autoencoder and then cluster the points in the lower-dimensional autoencoder feature space."
