"""
特立独行的猫 在我们的生命里 最好的总在不经意时候降临 或许 上帝希望我们在遇见一些错的人之后 才遇见那个对的 
因此我们会心怀感恩 微博：一直特立独行的猫 今日头条：一直特立独行的猫 商业合作，请发豆油。请注明合作内容和需求，谢谢 ##########
0 6 0.0 6 6 103 0.3037 103 108
1 2 0.9167 2 8 97 0.3735 97 108
2 7 0.0 7 7 104 0.2442 104 107
3 6 0.0 6 6 105 0.3643 105 108
4 5 0.0 5 6 105 0.3639 105 108
5 6 0.0 6 6 104 0.374 104 108
6 6 0.0 6 6 105 0.3643 105 108
7 6 0.0 6 6 104 0.2901 104 108
8 6 0.0 6 6 104 0.374 104 108
9 6 0.0 6 6 105 0.3639 105 108
10 6 0.0 6 6 105 0.2907 105 108
11 6 0.0 6 6 105 0.3643 105 108
12 6 0.0 6 6 104 0.2901 104 108
13 5 0.5 5 6 104 0.374 104 108
14 6 0.0 6 6 105 0.3643 105 108
15 6 0.0 6 6 105 0.3643 105 108
16 6 0.0 6 6 103 0.3168 103 108
17 6 0.0 6 6 105 0.3643 105 108
18 6 0.0 6 6 104 0.2907 104 108
19 6 0.0 6 6 105 0.3643 105 108
20 6 0.0 6 6 105 0.3643 105 108
21 6 0.0 6 6 104 0.3735 104 108
22 6 0.0 6 6 104 0.3735 104 108

图片距离

真实的数据 豆瓣3 和 微博真实的1，2，3的比较
3 1 Distances: (0.735)
3 2 Distances: (0.604)
3 3 Distances: (0.775)
因为噪音数据，我没有生成伪造图像，可以当作接近于完全无穷距离
3 4 Distances: (0.999)
3 5 Distances: (0.999)
3 6 Distances: (0.999)
...
3 22 Distances: (0.999)

文字相似度和图片相似度 合并为：
0 6 0.0 6 6 103 0.3037 103 108 0.735
1 2 0.9167 2 8 97 0.3735 97 108 0.604
2 7 0.0 7 7 104 0.2442 104 107 0.775
3 6 0.0 6 6 105 0.3643 105 108 0.999
4 5 0.0 5 6 105 0.3639 105 108 0.999
5 6 0.0 6 6 104 0.374 104 108 0.999
6 6 0.0 6 6 105 0.3643 105 108 0.999
7 6 0.0 6 6 104 0.2901 104 108 0.999
8 6 0.0 6 6 104 0.374 104 108 0.999
9 6 0.0 6 6 105 0.3639 105 108 0.999
10 6 0.0 6 6 105 0.2907 105 108 0.999
11 6 0.0 6 6 105 0.3643 105 108 0.999
12 6 0.0 6 6 104 0.2901 104 108 0.999
13 5 0.5 5 6 104 0.374 104 108 0.999
14 6 0.0 6 6 105 0.3643 105 108 0.999
15 6 0.0 6 6 105 0.3643 105 108 0.999
16 6 0.0 6 6 103 0.3168 103 108 0.999
17 6 0.0 6 6 105 0.3643 105 108 0.999
18 6 0.0 6 6 104 0.2907 104 108 0.999
19 6 0.0 6 6 105 0.3643 105 108 0.999
20 6 0.0 6 6 105 0.3643 105 108 0.999
21 6 0.0 6 6 104 0.3735 104 108 0.999
22 6 0.0 6 6 104 0.3735 104 108 0.999



[ 6 0.0 6 6 103 0.3037 103 108  0.999 ]
[ 2 0.9167 2 8 97 0.3735 97 108  0.999 ]
[ 7 0.0 7 7 104 0.2442 104 107  0.999 ]
[ 6 0.0 6 6 105 0.3643 105 108  0.999 ]
[ 5 0.0 5 6 105 0.3639 105 108  0.999 ]
[ 6 0.0 6 6 104 0.374 104 108  0.999 ]
[ 6 0.0 6 6 105 0.3643 105 108  0.999 ]
[ 6 0.0 6 6 104 0.2901 104 108  0.999 ]
[ 6 0.0 6 6 104 0.374 104 108  0.999 ]
[ 6 0.0 6 6 105 0.3639 105 108  0.999 ]
[ 6 0.0 6 6 105 0.2907 105 108  0.999 ]
[ 6 0.0 6 6 105 0.3643 105 108  0.999 ]
[ 6 0.0 6 6 104 0.2901 104 108  0.999 ]
[ 5 0.5 5 6 104 0.374 104 108  0.999 ]
[ 6 0.0 6 6 105 0.3643 105 108  0.999 ]
[ 6 0.0 6 6 105 0.3643 105 108  0.999 ]
[ 6 0.0 6 6 103 0.3168 103 108  0.999 ]
[ 6 0.0 6 6 105 0.3643 105 108  0.999 ]
[ 6 0.0 6 6 104 0.2907 104 108  0.999 ]
[ 6 0.0 6 6 105 0.3643 105 108  0.999 ]
[ 6 0.0 6 6 105 0.3643 105 108  0.999 ]
[ 6 0.0 6 6 104 0.3735 104 108  0.999 ]
[ 6 0.0 6 6 104 0.3735 104 108  0.999 ]
"""
import numpy as np

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
import numpy as np
from termcolor import colored
import matplotlib.pyplot as plt

def logistic_regression(x_row):
    x = np.array(x_row)
    print(x, 'in logistic_regression')
    x = x.reshape(-1, 1) # 需要转化成列数组
    y = list(range(1, 10))
    print('y=', y)
    model = LogisticRegression().fit(x, y)
    print('coef_=',model.coef_) # 打印出斜率
    print(model.intercept_) # 打印出截距

    print(model.predict(np.array([2]).reshape(-1,1))) 


# 因为levenshtein_distance等各种距离的趋势方向不统一，先要进行划归统一
# 
def unify_indicator_direction(row):
    print('source:', row)
    result = []
    for i in range(0, len(row)):
        # print(i, row[i])
        if i not in (1, 5, 8):
            unified = round(1 / row[i], 4)
            print(i, unified)
            result.append(unified)
        else:
            if i == 8:
                m = 1
                if row[i] < 0.7:
                    # 特别高的置信度，需要加权
                    m = 70

                row[i] = 1 - row[i]
                row[i] *= m

            if i == 1:
                if row[i] > 0.9:
                    # 特别高的置信度，需要加权
                    row[i] *= 70

            row[i] = round(row[i], 4)
            row[i] *= 5
            print(i, row[i] )
            result.append(row[i])
    print('\n')
    #标准差
    # mystd = np.std(result, ddof=1)
    mystd = np.mean(result)
    print(row, 'mean=', mystd)
    mystd = int(round(mystd, 4)*100)
    # logistic_regression(result)
    return mystd



def main():
    x = np.empty((0))
    x = np.array(
        [
            [6, 0.0, 6, 6, 103, 0.3037, 103, 108, 0.735],
            [2, 0.9167, 2, 8, 97, 0.3735, 97, 108, 0.604],
            [7, 0.0, 7, 7, 104, 0.2442, 104, 107, 0.775],
            [6, 0.0, 6, 6, 105, 0.3643, 105, 108, 0.999],
            [5, 0.0, 5, 6, 105, 0.3639, 105, 108, 0.999],
            [6, 0.0, 6, 6, 104, 0.374, 104, 108, 0.999],
            [6, 0.0, 6, 6, 105, 0.3643, 105, 108, 0.999],
            [6, 0.0, 6, 6, 104, 0.2901, 104, 108, 0.999],
            [6, 0.0, 6, 6, 104, 0.374, 104, 108, 0.999],
            [6, 0.0, 6, 6, 105, 0.3639, 105, 108, 0.999],
            [6, 0.0, 6, 6, 105, 0.2907, 105, 108, 0.999],
            [6, 0.0, 6, 6, 105, 0.3643, 105, 108, 0.999],
            [6, 0.0, 6, 6, 104, 0.2901, 104, 108, 0.999],
            [5, 0.5, 5, 6, 104, 0.374, 104, 108, 0.999],
            [6, 0.0, 6, 6, 105, 0.3643, 105, 108, 0.999],
            [6, 0.0, 6, 6, 105, 0.3643, 105, 108, 0.999],
            [6, 0.0, 6, 6, 103, 0.3168, 103, 108, 0.999],
            [6, 0.0, 6, 6, 105, 0.3643, 105, 108, 0.999],
            [6, 0.0, 6, 6, 104, 0.2907, 104, 108, 0.999],
            [6, 0.0, 6, 6, 105, 0.3643, 105, 108, 0.999],
            [6, 0.0, 6, 6, 105, 0.3643, 105, 108, 0.999],
            [6, 0.0, 6, 6, 104, 0.3735, 104, 108, 0.999],
            [6, 0.0, 6, 6, 104, 0.3735, 104, 108, 0.999],
        ]
    )
    # print(len(x), x[0], x[0][0])
    score_list_dalianmao = []
    y_true = []
    for i in range(0, len(x)):
        y_true.append(0)
        score = unify_indicator_direction(x[i])
        print('score=', score)
        score_list_dalianmao.append(score)
    print(score_list_dalianmao)
    print(sum(score_list_dalianmao))
    total = sum(score_list_dalianmao)
    right = score_list_dalianmao[1]
    print(total, right)
    y_pred, y_true = [], []
    for i in range(total):
        if i < right:
            y_pred.append(1)
        else:
            y_pred.append(0)
        y_true.append(1)
    # y_pred = score_list_dalianmao
    # print('#'*20, y_true, y_pred)
    print(colored('#'*30, 'blue'))
    print('''
        真正 (true positive)、
        假正 (false positive)、
        真负 (true negative) 和
        假负 (false negative) 四种情形，
        分别由TP、FP、TN、FN 表示（T代表预测正确，F代表预测错误）
        ''')
    print('target false/true confusion_matrix= ')
    cm = confusion_matrix(y_true, y_pred)
    print(colored('#'*30, 'red'))
    print(cm)
    tn, fp, fn, tp = cm.ravel()
    print('tn, fp, fn, tp=', tn, fp, fn, tp)

    fig, ax = plt.subplots(figsize=(2.5, 2.5))
    ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(x=j, y=i, s=cm[i,j], va='center', ha='center')
    plt.xlabel('predicted label')        
    plt.ylabel('true label')
    plt.show()

if __name__ == "__main__":
    # logistic_regression()
    main()
