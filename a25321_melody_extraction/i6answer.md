1.我想问一下spleeter和vocano模型的具体结构和参数

Spleeter是一个基于深度学习的音频分离工具，使用的模型结构是堆叠式U-Net，
包含多个卷积层和反卷积层，并使用随机梯度下降算法进行训练。

参数上：Spleeter使用的堆叠式神经网络有5个卷积层和5个反卷积层。
每个卷积层包括一个3x3的卷积核和一个ReLU激活函数，
每个反卷积层包括一个2x2的反卷积核和一个ReLU激活函数

vocano 主要食用深度卷积神经网络（CNN）结构来学习音频信号的特征表示，
并使用多通道注意力机制来聚焦于声乐和伴奏信号的频谱特征

参数上vocano 中使用的主要超参数：
学习率、
批量大小、
权重衰减、
梯度裁剪和噪声添加



2. 把musdb18的结构
MusDB18是一个包含18首歌曲的音乐数据库，其中每首歌曲都有多个音轨，
里面有各个音轨分离出来，如鼓、吉他、贝斯、和声等


为了将MUSDB18数据集输入到Spleeter网络中，
需要将MUSDB18数据集输每个多轨音频混合文件加载到内存中，
并将其转换为适当的格式以供Spleeter网络处理。

在Spleeter中，输入数据的是多轨音频混合文件被转换为一个时间-频率图矩阵，
该矩阵的每一行表示音频的一个时间步长，
每一列表示音频的一个频率范围。
时间-频率图矩阵的大小取决于所选的音频特征提取器，
我们试验的是时间-频率图矩阵的大小通常为512x时间步长。



3. musdb18训练的细节也说一下

我们在代码中首先使用了librosa库将音频文件转换为数字信号，
并对其进行采样率转换和归一化处理，
然后使用变换将其转换为时间-频率图矩阵，
只有时间-频率图矩阵才是spleeter能接受的格式

真实因为spleeter的网络结构与时间-频率图矩阵的使用是密切相关的。
spleeter的网络结构是基于深度卷积神经网络（CNN）或者全卷积神经网络（FCN）的，
可以将时间-频率图矩阵作为输入，通过卷积和池化等操作提取特征，并最终输出分离后的音频信号

我感觉因为是CNN/FCN，所以输入必须转成：时间-频率图矩阵将音频信号在时域和频域上进行划分，
并将每个时刻的频率信息编码为一个向量，使得不同的音频源在时频域上表现出差异

数据准备：首先需要从MusDB18中选择一组训练数据，通常会将数据按比例分成测试集验证集。
对于每个音频文件，需要将其分解成多个音轨，并将它们存储为单独的音频文件。
可以使用Spython调用pleeter提供的库接口来自动化这个过程。

模型选择：Spleeter使用深度学习模型来进行音频分离，支持多种模型结构，
我们选择的是2stems模型等，因为我只需要分离人声。

模型训练：在选择好模型之后，需要使用训练数据对模型进行训练。在训练过程中
，需要设置训练的参数，在gpu上训练（本地或云端）。

模型评估：在模型训练完成后，需要使用验证集和测试集对模型进行评估。
在评估过程中，通常会使用一些常见的评估指标，如信噪比（SNR）、SIR,SAR等指标进行度量

时间看数据量，我们选择5-6首歌，一般也要小时级别的时间训练在服务器端（rtx2080ti）


4. 哪2个数据库
除去 musdb18 数据集
后来我还下载了DSD100
https://www.loria.fr/~aliutkus/DSD100subset.zip 数据集

